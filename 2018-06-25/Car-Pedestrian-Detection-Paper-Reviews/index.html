<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="_en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.0.5" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.5">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.5">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.5">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.5" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '6.0.5',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  
  <meta name="keywords" content="Hexo, my world." />


<meta name="description" content="Car/Pedestrian detections and datasets review[TOC] 1. Car / Pedstrains detection reviews1.1 How Far are We from Solving Pedestrian Detection1.1.0 Abstract Encou">
<meta property="og:type" content="article">
<meta property="og:title" content="Car&#x2F;Pedestrian Detection Paper Reviews">
<meta property="og:url" content="https://www.luodian.ink/2018-06-25/Car-Pedestrian-Detection-Paper-Reviews/index.html">
<meta property="og:site_name" content="Luodian.ink">
<meta property="og:description" content="Car/Pedestrian detections and datasets review[TOC] 1. Car / Pedstrains detection reviews1.1 How Far are We from Solving Pedestrian Detection1.1.0 Abstract Encouraged by the recent progress in pedestri">
<meta property="og:locale" content="_en">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tKfTcly1fs49ifnw80j318g0awaa6.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tKfTcly1fs49nl8l92j30v40igwex.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tKfTcly1fs49s3pueij30iq0b6q2y.jpg">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tKfTcly1fs49u6m3k2j30me0ag0ub.jpg">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tKfTcly1fs4a9zg6mwj30j8080t90.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcly1fs5tkiy3d0j30tu0ggaem.jpg">
<meta property="og:image" content="https://img-blog.csdn.net/20160901094513882">
<meta property="og:image" content="https://img-blog.csdn.net/20160901104135594">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tKfTcly1fs5u07w226j30vi04yq37.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tKfTcly1fs5u41myazj30y60fet9y.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tKfTcly1fs5x6kq7alj30t60dwt9i.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tKfTcly1fs5u5jc1ngj30y60f8aav.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcly1fs5u74hs55j30va08ut94.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcly1fs5u9bw7mrj30ny07s74j.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tKfTcly1fs5uauzhe1j30p20bmaaj.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcly1fs5ubu3r21j30n00qc75z.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tNc79ly1fsdzg9wp0vj31bc0h6n17.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tNc79ly1fsdzxmrnmoj31bg0jw3za.jpg">
<meta property="og:image" content="https://img-blog.csdn.net/20160906220243318">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tNc79ly1fse39kl8ycj30xs05g3yi.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tNc79ly1fse3dhjqhqj31be04cjre.jpg">
<meta property="og:image" content="http://img.xzhewei.com/note/MS-CNN/1508383544192.png">
<meta property="og:image" content="http://img.xzhewei.com/note/MS-CNN/1508399094430.png">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tNc79ly1fse3jeyvfgj31c60l20ty.jpg">
<meta property="og:image" content="http://img.xzhewei.com/note/MS-CNN/1508403584903.png">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tNc79ly1fsllr6ag7lj30zs0g2aaw.jpg">
<meta property="og:image" content="http://img.xzhewei.com/note/MS-CNN/1508421600322.png">
<meta property="og:image" content="http://img.xzhewei.com/note/MS-CNN/1508423079688.png">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcly1fsm5xwwgg2j31480jsdi7.jpg">
<meta property="og:image" content="http://img.xzhewei.com/note/MS-CNN/1508425810979.png">
<meta property="og:image" content="http://img.xzhewei.com/note/MS-CNN/1508425896306.png">
<meta property="og:image" content="http://static.zybuluo.com/303389737/hdp86bzkmazpwp62qbsr5t4i/image.png">
<meta property="og:image" content="http://static.zybuluo.com/303389737/9d4ebkpocrcsuri19371vr0c/image.png">
<meta property="og:image" content="http://static.zybuluo.com/303389737/hwmisocf2sltx8kiaz94adxm/image.png">
<meta property="og:image" content="http://static.zybuluo.com/303389737/vkly55v2dkq9aik0nmcuir5l/image.png">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tKfTcly1fsmqow0fy1j30z80kc77n.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tNc79ly1fsmr91i37uj31220guai3.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tKfTcly1fs5xsvbbu7j310o03edfs.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tKfTcly1fs5xtie6wjj31h80g6td4.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcly1fs5xuof6odj316e0qmk2e.jpg">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/534700/201701/534700-20170106150356316-732250786.jpg">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tKfTcly1fsmr1vp0g1j30tk0us0v9.jpg">
<meta property="og:image" content="http://www.vision.caltech.edu/graphics/cub_200_collage.jpg">
<meta property="og:updated_time" content="2018-06-24T16:57:33.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Car&#x2F;Pedestrian Detection Paper Reviews">
<meta name="twitter:description" content="Car/Pedestrian detections and datasets review[TOC] 1. Car / Pedstrains detection reviews1.1 How Far are We from Solving Pedestrian Detection1.1.0 Abstract Encouraged by the recent progress in pedestri">
<meta name="twitter:image" content="https://ws2.sinaimg.cn/large/006tKfTcly1fs49ifnw80j318g0awaa6.jpg">






  <link rel="canonical" href="https://www.luodian.ink/2018-06-25/Car-Pedestrian-Detection-Paper-Reviews/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>
  <title>Car/Pedestrian Detection Paper Reviews | Luodian.ink</title>
  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110144268-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-110144268-1');
</script>






  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="_en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Luodian.ink</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-user"></i> <br />About</a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />Tags</a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-th"></i> <br />Categories</a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />Commonweal 404</a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />Search</a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.luodian.ink/2018-06-25/Car-Pedestrian-Detection-Paper-Reviews/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Luodian">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/my_face.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Luodian.ink">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Car/Pedestrian Detection Paper Reviews</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-25T00:57:19+08:00">2018-06-25</time>
            

            
            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Computer-Vision/" itemprop="url" rel="index"><span itemprop="name">Computer Vision</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2018-06-25/Car-Pedestrian-Detection-Paper-Reviews/" class="leancloud_visitors" data-flag-title="Car/Pedestrian Detection Paper Reviews">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Views&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Car-Pedestrian-detections-and-datasets-review"><a href="#Car-Pedestrian-detections-and-datasets-review" class="headerlink" title="Car/Pedestrian detections and datasets review"></a>Car/Pedestrian detections and datasets review</h1><p>[TOC]</p>
<h2 id="1-Car-Pedstrains-detection-reviews"><a href="#1-Car-Pedstrains-detection-reviews" class="headerlink" title="1. Car / Pedstrains detection reviews"></a>1. Car / Pedstrains detection reviews</h2><h3 id="1-1-How-Far-are-We-from-Solving-Pedestrian-Detection"><a href="#1-1-How-Far-are-We-from-Solving-Pedestrian-Detection" class="headerlink" title="1.1 How Far are We from Solving Pedestrian Detection"></a>1.1 How Far are We from Solving Pedestrian Detection</h3><h4 id="1-1-0-Abstract"><a href="#1-1-0-Abstract" class="headerlink" title="1.1.0 Abstract"></a>1.1.0 Abstract</h4><blockquote>
<p>Encouraged by the recent progress in pedestrian detection, we investigate the gap between current state-of-the-art methods and the “perfect single frame detector”. We enable our analysis by creating a human baseline for pedestrian detection (over the Caltech dataset), and by manually clustering the recurrent errors of a top detector. Our results characterise both localisation and background-versus-foreground errors. To address localisation errors we study the impact of training annotation noise on the detector performance, and show that we can improve even with a small portion of sanitised training data. To address background/foreground discrimination, we study convnets for pedestrian detection, and discuss which factors affect their performance. Other than our in-depth analysis, we report top performance on the Caltech dataset, and provide a new sanitised set of training and test annotations.</p>
</blockquote>
<h4 id="2-1-1-Analyzing-the-state-of-the-art"><a href="#2-1-1-Analyzing-the-state-of-the-art" class="headerlink" title="2.1.1 Analyzing the state of the art."></a>2.1.1 Analyzing the state of the art.</h4><p>这篇论文是一个分析性质的论文，主要针对于Caltech的数据集进行错误分析，文章让domain experts在Caltech的数据集上进行人工识别，并提出机器检测算法应该达到至少人类水平，甚至应该超过人类水平。</p>
<p>文中的人工检测采用人工基准线，即从人的头到脚画一条线。为了公平比较，关注于单帧单目检测，注释器需要根据行人外表和单帧环境来注释。</p>
<p>但目前主流的基于ICF的方法都被人完爆。</p>
<blockquote>
<p>We find that the human baseline widely outperforms state-of-the-art detectors in all settings, indicating that there is still room for improvement for automatic methods.</p>
<p>Since most top methods of figure 1 are of the ICF family, we expect a similar behaviour for them too.Methods using convnets with proposals based on ICF detectors will also be affected.</p>
</blockquote>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fs49ifnw80j318g0awaa6.jpg" alt=""></p>
<h4 id="2-1-2-Failure-analysis"><a href="#2-1-2-Failure-analysis" class="headerlink" title="2.1.2 Failure analysis"></a>2.1.2 Failure analysis</h4><p>我们需要知道为什么，何时会出错。</p>
<p>检测器主要会出现两种错误：</p>
<ul>
<li><p>假阳性（检测到了背景，或者很弱的定位检测），FP聚类成11个分类：</p>
<p>These categories fall into three groups: localisation, background, and annotation errors.</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fs49nl8l92j30v40igwex.jpg" alt=""></p>
</li>
<li><p>假阴性（低得分率或者错过某些行人检测，检测不全）</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fs49s3pueij30iq0b6q2y.jpg" alt=""></p>
</li>
</ul>
<h4 id="2-1-3-Improving-Annotation"><a href="#2-1-3-Improving-Annotation" class="headerlink" title="2.1.3 Improving Annotation"></a>2.1.3 Improving Annotation</h4><p>论文指出Caltech的数据集有错误，如下：</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fs49u6m3k2j30me0ag0ub.jpg" alt=""></p>
<p>这里提出了新的标注方式，即可以观看完整视频来判断是不是有人，同时在人群中时，也要注意不能重复标记一个人。</p>
<h4 id="2-1-4-Analysis"><a href="#2-1-4-Analysis" class="headerlink" title="2.1.4 Analysis"></a>2.1.4 Analysis</h4><ul>
<li><p>Pruning benefits</p>
<p>从原始到修剪注释的主要变化是删除注释错误，从修剪到新的，主要的变化是更好的对齐。更强的检测器更好地受益于更好的数据，并且检测质量的最大增益来自移除注释错误。</p>
</li>
<li><p>Alignment benefits</p>
<p>为了利用新的10%注释来利用90%剩余数据，我们在新的注释上训练模型，并使用该模型在90%部分上重新对准原始注释。 </p>
</li>
</ul>
<p>因为新的注释更好地对齐，所以我们期望该模型能够修复原始注释中的轻微位置和缩放错误。结果表明，使用检测器模型来提高整体数据对准确实是有效的，并且更好地对准训练数据导致更好的检测质量（在MRO和MRN中）</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fs4a9zg6mwj30j8080t90.jpg" alt=""></p>
<p>总的定位和前景背景错误都很重要，在原dataset 上，去除一些错误的标注，和校准一些不太准确的标注，都有助于提高检测质量。</p>
<p>convnet 在图像分类和目标检测上性能很好，但对于小物体的定位还有一定的局限性，可能与pooling有关系，这时候Bounding box regression和NMS就显得比较重要。而前景背景的区分度上也有待提升，说明convnet对于分类还有提升空间。</p>
<h4 id="2-1-5-论文信息"><a href="#2-1-5-论文信息" class="headerlink" title="2.1.5 论文信息"></a>2.1.5 论文信息</h4><p><strong>会议</strong>：CVPR 2016</p>
<p><strong>团队</strong>：Max Planck Institute for Informatics（德国的研究机构）</p>
<p><strong>数据集</strong>：在Caltech上进行分析，并且自己提出了一个更干净的CityPersons数据集</p>
<h3 id="1-2-Is-Faster-R-CNN-Doing-Well-for-Pedestrian-Detection"><a href="#1-2-Is-Faster-R-CNN-Doing-Well-for-Pedestrian-Detection" class="headerlink" title="1.2 Is Faster R-CNN Doing Well for Pedestrian Detection"></a>1.2 Is Faster R-CNN Doing Well for Pedestrian Detection</h3><h4 id="1-2-1-Abstract"><a href="#1-2-1-Abstract" class="headerlink" title="1.2.1 Abstract"></a>1.2.1 Abstract</h4><p>本文主要是分析了一下Faster R-CNN用于行人检测效果不好的原因，并对比提出了解决方案，本文提出的RPN+BF算法似乎从时间还是准确度上，提升都非常的大。</p>
<p>Faster R-CNN用于行人检测效果不好的原因有两个： </p>
<ol>
<li><p>行人在图像中的尺寸较小（比如 28×70 for Caltech）。对于小物体， Region-of-Interest (RoI) pooling layer 在 low-resolution feature map（特征层的尺寸又减小了很多啊） 提出的特征没有什么区分能力。</p>
<p>针对该情况，论文在更大尺寸的浅层特征上提取特征，以此提高提出特征的区分能力。</p>
</li>
<li><p>行人检测中的误检主要是背景的干扰，广义物体检测主要受多种类影响。</p>
</li>
</ol>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fs5tkiy3d0j30tu0ggaem.jpg" alt=""></p>
<h4 id="2-2-2-Approach"><a href="#2-2-2-Approach" class="headerlink" title="2.2.2 Approach"></a>2.2.2 Approach</h4><p>Our approach consists of two components (illustrated in Fig. 2): an RPN that generates candidate boxes as well as convolutional feature maps, and a Boosted Forest that classifies these proposals using these convolutional features.</p>
<p>本论文提出了一种方法，使用Boosted Forest直接训练 RPN (Region Proposal Network) 提出的深度卷积特征。</p>
<p><img src="https://img-blog.csdn.net/20160901094513882" alt=""></p>
<h5 id="2-2-2-1-Region-Proposal-Network-for-Pedestrian-Detection"><a href="#2-2-2-1-Region-Proposal-Network-for-Pedestrian-Detection" class="headerlink" title="2.2.2.1 Region Proposal Network for Pedestrian Detection"></a>2.2.2.1 Region Proposal Network for Pedestrian Detection</h5><p>这里针对行人检测进行单个物体检测，对 RPN 进行了一些修改。adopt anchors of a single aspect ratio of 0.41 (width to height)。主要人的长宽比是相对固定的。</p>
<p>针对多尺度问题，论文使用了9个尺度作为检测标定，这样我们就不用建立特征金字塔来解决多尺度行人检测。</p>
<h5 id="2-2-2-2-Feature-Extraction"><a href="#2-2-2-2-Feature-Extraction" class="headerlink" title="2.2.2.2 Feature Extraction"></a>2.2.2.2 Feature Extraction</h5><p>RPN 产生候选区域，然后使用 RoI pooling 得到固定长度的特征，使用这些特征训练 Boosted Forest，下面是不同候选区域数目的影响。</p>
<p><img src="https://img-blog.csdn.net/20160901104135594" alt=""></p>
<h5 id="2-2-2-3-Boosted-Forest"><a href="#2-2-2-3-Boosted-Forest" class="headerlink" title="2.2.2.3 Boosted Forest"></a>2.2.2.3 Boosted Forest</h5><p>当RPN已经产生了region proposals, confidence scores, and features. 接下来论文采用BF的方式对这些特征进行训练。</p>
<p>论文采用了RealBoost algorithm[4]进行训练.</p>
<p>Train阶段采用的数据集：∼50k on the Caltech set </p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fs5u07w226j30vi04yq37.jpg" alt=""></p>
<p>BF过程中一共使用了6个stage，每个stage的树分别是 {64, 128, 256, 512, 1024, 1536}，其实没有必要同等的处理初始的 proposals，因为我们的 proposals 在 RPN 之后得到了初始的 score，因此可以把RPN过程看成是 $f_0$.</p>
<h4 id="2-2-3-Comparasion"><a href="#2-2-3-Comparasion" class="headerlink" title="2.2.3 Comparasion"></a>2.2.3 Comparasion</h4><p><strong>不同候选区域数目的影响</strong></p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fs5u41myazj30y60fet9y.jpg" alt=""></p>
<p><strong>不同的Classfiers的组合</strong></p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fs5x6kq7alj30t60dwt9i.jpg" alt=""></p>
<p><strong>How Important is Feature Resolution</strong></p>
<p>上图中使用R-CNN（用VGG-16网络）方法实现了13.1的MR，略好于独立RPN检测器（14.9%的MR），它使用的窗口和上面提到的RPN是一样的。R-CNN从图像上剪切的目标候选区域，并且调整到224x224的尺度，因此它受小目标的影响比较小。这表明如果提取224x224精细的特征，下游的分类器可以提升精度。</p>
<p>同样在RPN提取的候选窗口上训练一个Fast R-CNN分类器，性能掉到了20.2%。尽管R-CNN在这个任务上工作很好，但是Fast R-CNN却产生了更糟糕的结果。</p>
<p>这个问题部分是因为低分辨率的特征。在Conv5上使用a trous trick，把stride从16减少到8个像素，这个问题得到了缓解，实现了16.2%的MR。这说明更高的分辨率是有帮助的。</p>
<p><strong>不同特征的影响</strong></p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fs5u5jc1ngj30y60f8aav.jpg" alt=""></p>
<p><strong>Bootstrap的作用</strong></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fs5u74hs55j30va08ut94.jpg" alt=""></p>
<p><strong>时间效率</strong></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fs5u9bw7mrj30ny07s74j.jpg" alt=""></p>
<p><strong>各大数据集测试结果</strong></p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fs5uauzhe1j30p20bmaaj.jpg" alt=""></p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fs5ubu3r21j30n00qc75z.jpg" alt=""></p>
<h4 id="2-2-5-Conclusion"><a href="#2-2-5-Conclusion" class="headerlink" title="2.2.5 Conclusion"></a>2.2.5 Conclusion</h4><p>总体而言，本文在做实验的基础上，发现了 faster RCNN 在行人检测方面的不足，在实践中发现科研问题，深度挖掘身体背后的原因，并且结合已有的技术，完美的将其解决，本身就为我们提供了一个很好的科研思路和科研案例，值得我们学习！</p>
<p>值得一提的是，本文的大佬是个一线的研究+开发者，论文中提供了训练过程中的一些实现细节，甚至开放了源码：</p>
<p><a href="https://github.com/zhangliliang/RPN_BF" target="_blank" rel="noopener">https://github.com/zhangliliang/RPN_BF</a></p>
<h4 id="2-2-5-论文信息"><a href="#2-2-5-论文信息" class="headerlink" title="2.2.5 论文信息"></a>2.2.5 论文信息</h4><p><strong>会议</strong>：EECV 2016</p>
<p><strong>团队</strong>：Zhang Li Liang from 中山大学林倞教授研究组 &amp; MSRA何凯明</p>
<h3 id="1-3-A-Unified-Multi-scale-Deep-Convolutional-Neural-Network-for-Fast-Object-Detection"><a href="#1-3-A-Unified-Multi-scale-Deep-Convolutional-Neural-Network-for-Fast-Object-Detection" class="headerlink" title="1.3 A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection"></a>1.3 A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection</h3><h4 id="1-3-1-Abstract"><a href="#1-3-1-Abstract" class="headerlink" title="1.3.1 Abstract"></a>1.3.1 Abstract</h4><p>这是一篇UCSD-SVCL实验室和IBM研究院一起研究的结果，主要是关注多尺度快速目标检测的问题。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fsdzg9wp0vj31bc0h6n17.jpg" alt=""></p>
<p>这篇文章主要解决多尺度同时存在时的检索问题，设计了MS-CNN。MS-CNN主要包含proposal sub-network和detection-subnetwork</p>
<p>这两个网络结构面向的任务分别是：</p>
<ol>
<li><p>针对多尺度问题（proposal subnetwork）：</p>
<p>类似于FCNT跟踪方法，该文章也是观察到了卷积网络不同层得到的特征特点的不同，对不同层的特征采用不同的利用方式。比如conv-3的低网络层，有更小的感受野，可以进行小目标的检测；而高层如conv-5，对于大目标的检测更加准确。<strong>对于不同的输出层设计不同尺度的目标检测器</strong>，完成多尺度下的检测问题。</p>
<blockquote>
<p>在卷积神经网络中，感受野的定义是卷积神经网络每一层输出的特征图（feature map）上的像素点 在原始图像上映射的区域大小。</p>
</blockquote>
</li>
<li><p>针对速度问题（detection subnetwork）：</p>
<p>使用特征上采样代替输入图像的上采样步骤。通过设计一个去卷积层，来增加特征图的分辨率，使得小目标依然可以被检测出来。这里使用了特征图的deconvolutional layer（去卷积层）来代替input图像的上采样，可以大大减少内存占用，提高速度。</p>
<p>去卷积层一直用于分隔和边缘检测，该论文第一次用它加速和提高检测率。</p>
</li>
</ol>
<h4 id="1-3-2-Related-Work"><a href="#1-3-2-Related-Work" class="headerlink" title="1.3.2 Related Work"></a>1.3.2 Related Work</h4><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fsdzxmrnmoj31bg0jw3za.jpg" alt=""></p>
<p><strong>(a)</strong> Learn a single classifier and rescale the image multiple times, so that the classifier can match all possible object sizes. 但这种方法非常的耗时间。</p>
<p><strong>(b)</strong> [5]中提出，针对a方法的一个改进：Apply multiple classifiers to a single input image. 避免了重复对一个feature maps的重复计算。However, it requires an individual classifier for each object scale and usually fails to produce good detectors. </p>
<p><strong>(c)</strong> rescale the input a few times and learn a small number of model templates.</p>
<p><strong>(d)</strong> this consists of rescaling the input a small number of times and interpolating the missing feature maps. <strong>（这里不太理解）</strong>. 这种方法能够在极小的准确率损失的情况下，实现较高的速度提升。</p>
<p><strong>(e)</strong> the R-CNN of [6] simply warps object proposal patches to the natural scale of the CNN. features are computed for patches rather than the entire image. </p>
<blockquote>
<p>这是一个类似于a的方法，但是我还不是很理解。</p>
</blockquote>
<p><strong>(f)</strong> RPN的检测机制，multiple sets of templates of the same size are applied to all feature maps. </p>
<blockquote>
<p>因为从输入的图像到输出的model templates都是相同大小的，在对于不同大小物体的检测上会存在着很大的问题。</p>
</blockquote>
<p><strong>(g)</strong> 本文提出的 multi-scale strategy，可以被看成是c的CNN拓展版本。It exploits feature maps of several resolutions to detect objects at different scales. 这可以输出不同大小的感知层，从而实现不同大小的目标检测。</p>
<blockquote>
<p>这个方法是e和f两者的结合产物，也就是做了一个trade-off</p>
</blockquote>
<h4 id="1-3-3-Proposal-Network"><a href="#1-3-3-Proposal-Network" class="headerlink" title="1.3.3 Proposal Network"></a>1.3.3 Proposal Network</h4><p>文章的网络结构分为proposal提取和目标检测，两个部分独立进行。proposal子网络和目标检测子网络结构图分别如下：</p>
<p><img src="https://img-blog.csdn.net/20160906220243318" alt="img"></p>
<p>Object Proposal Network是基于VGG来进行设计的，其像一棵树。主干就是原始的VGG，分支上是另外为了实现多尺度目标检测而设计的网络，构成相同。</p>
<p>注意该网络有多个检测分支，每个检测分支都是最终的proposal检测结果。该网络有个标准的主干CNN，一组单一检测分支。这里在conv4-3之后开始建立检测分支，因为在之前，回传的梯度会对后面的检测结果具有较大影响，造成训练的不稳定。</p>
<p>整个结构算是 faster RCNN 的 multi-scale 版本。因为越浅层的 branch 反馈的梯度，对主干 network 的影响越大。这样会造成训练的不稳定。The Buffer Convolution 层防止 branch 的梯度直接回传到主干上。</p>
<h5 id="1-3-3-1-Multi-Branch-Loss"><a href="#1-3-3-1-Multi-Branch-Loss" class="headerlink" title="1.3.3.1 Multi-Branch Loss"></a>1.3.3.1 Multi-Branch Loss</h5><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fse39kl8ycj30xs05g3yi.jpg" alt=""></p>
<ul>
<li>$W$是网络参数</li>
<li>$Y_i=(y_i,b_i)$ 是 Ground Truth</li>
<li>$X_i$是训练图像块</li>
<li>$a_m$是损失权重</li>
<li>$S=(X_i,Y_i)^N_{i=1}=S^1,S^2,…,S^M$ 是训练样本</li>
<li>$M$ 是检测分支的数量</li>
</ul>
<h5 id="1-3-3-2-Loss-of-each-detection-layer"><a href="#1-3-3-2-Loss-of-each-detection-layer" class="headerlink" title="1.3.3.2 Loss of each detection layer"></a>1.3.3.2 Loss of each detection layer</h5><p>The loss of each detection layer combines these two objectives.</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fse3dhjqhqj31be04cjre.jpg" alt=""></p>
<ul>
<li>一个是多类分类损失</li>
<li>一个是定位损失，背景样本不提供定位损失</li>
</ul>
<h5 id="1-3-3-3-Sampling"><a href="#1-3-3-3-Sampling" class="headerlink" title="1.3.3.3 Sampling"></a>1.3.3.3 Sampling</h5><p>对于每个检测层，$S^m={S^m <em>+,S^m</em> -}$</p>
<p><img src="http://img.xzhewei.com/note/MS-CNN/1508383544192.png" alt="1508383544192"></p>
<p>Anchor尺寸是与filter的尺寸相关的，anchor 与标记 $IoU \geq 0.5$ 认为是正样本，$≤ 0.2$ 认为是负样本。</p>
<p>对于自然图像，目标和非目标的数量具有极大的不平衡。采样是为了补偿这种不平衡。考虑三种采样策略：随机，bootstrapping，混合。</p>
<ul>
<li>随机采样获得的样本大量为随机样本，我们知道困难样本挖掘能提升性能</li>
<li>Bootstrapping策略，用物体性得分对样本排序。然后从高到低收集负样本</li>
<li>混合策略，就是随和和bootstrapping一半一半，在我们的实验中其效果和bootstrapping相似</li>
</ul>
<p>为了保证每个检测层只检测特定尺度范围的目标，训练集按照相应的范围组织。但是，在一张图中，一些尺度可能没有正样本，导致正负样本不平衡，学习不稳定。为了解决这个问题，交叉熵损失修改为：</p>
<p><img src="http://img.xzhewei.com/note/MS-CNN/1508399094430.png" alt="1508399094430"></p>
<h4 id="1-3-4-Object-Detection-Network"><a href="#1-3-4-Object-Detection-Network" class="headerlink" title="1.3.4 Object Detection Network"></a>1.3.4 Object Detection Network</h4><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fse3jeyvfgj31c60l20ty.jpg" alt=""></p>
<p>RPN本身虽然也可以作为检测器，但滑窗不能很好覆盖目标，所以不是很好。为了提高精度，因此增加了检测网络。ROI pooling层在特征图上提取固定维度的特征（7×7×512）然后送入fc层。这里增加了一个反卷积层来提升特征图分辨率。因此，多任务损失扩展为：</p>
<p><img src="http://img.xzhewei.com/note/MS-CNN/1508403584903.png" alt="1508403584903"></p>
<h5 id="1-3-4-1-CNN-Feature-Map-Approximation"><a href="#1-3-4-1-CNN-Feature-Map-Approximation" class="headerlink" title="1.3.4.1 CNN Feature Map Approximation"></a>1.3.4.1 CNN Feature Map Approximation</h5><p>在faster RCNN中采用两次上采样的方式将小的目标放大，但是KITTI数据集，其含有大量的小体积物体需要做unsampling. And input upsampling also has three side effects.</p>
<ul>
<li>large memory requirements</li>
<li>slow training</li>
<li>slow testing</li>
</ul>
<p>本文考虑了一些 increase the resolution of feature maps 的方式。</p>
<p>Unlike input upsampling, feature upsampling does not incur in extra costs for memory and computation. </p>
<p><strong>Our experiments show that the addition of a deconvolution layer significantly boosts detection performance, especially for small objects.</strong> </p>
<h5 id="1-3-4-2-Context-Embedding（创新点）"><a href="#1-3-4-2-Context-Embedding（创新点）" class="headerlink" title="1.3.4.2 Context Embedding（创新点）"></a>1.3.4.2 Context Embedding（创新点）</h5><p>本文专注于从多个区域中提取语义。在一些研究中[7,8]已经证明其有用了。</p>
<p>结合下图来看，其从目标区域（绿色）和语义区域（蓝色）提取特征，然后堆叠在一起。语义区域是目标区域的1.5倍，后面增加一个卷积层减少通道数，从而保证在不增加模型参数的同时不损失精度。</p>
<blockquote>
<p>上下文嵌入的好坏也是一个trade-off的过程，虽然其可以提升性能，但是却几乎是参数数量翻倍了。</p>
</blockquote>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fsllr6ag7lj30zs0g2aaw.jpg" alt=""></p>
<h4 id="1-3-5-Experimental-Evaluation"><a href="#1-3-5-Experimental-Evaluation" class="headerlink" title="1.3.5 Experimental Evaluation"></a>1.3.5 Experimental Evaluation</h4><h5 id="1-3-5-1-Proposal-Evaluation"><a href="#1-3-5-1-Proposal-Evaluation" class="headerlink" title="1.3.5.1 Proposal Evaluation"></a>1.3.5.1 Proposal Evaluation</h5><p>文章在KITTI和Caltech数据集上做的评测，其表示VOC和ImageNet的图片太小？</p>
<blockquote>
<p> KITTI contains three object classes: car, pedestrian and cyclist.</p>
</blockquote>
<p>跟随[7]，他们同样训练了一个model来检测car，一个用来检测pedestrian。使用oracle recall用于评估指标。car的IOU大于等于70%，行人、骑车人IOU大于等于50%。</p>
<p><img src="http://img.xzhewei.com/note/MS-CNN/1508421600322.png" alt=""></p>
<p><strong>输入图像尺寸的作用：</strong>图5显示了proposal网络与输入图像尺寸的关系。行人和车辆的proposal网络对图像尺寸较为鲁棒，这说明，proposal网络不需要增加图像输入尺寸也能得到较好的proposal结果。</p>
<p><strong>独立检测分支的作用：</strong>表2显示了不同检测分支与行人高度的检测精度关系。与期望的一致，尺度匹配的精度越高。</p>
<p><strong>与其他先进方法比较：</strong></p>
<p><img src="http://img.xzhewei.com/note/MS-CNN/1508423079688.png" alt=""></p>
<p>图像第一排是IoU确定时，召回率与候选样本数量的关系。第二排是100个候选样本，召回率与IoU的关系。MS-CNN在只有100个proposal的情况下实现了98%的召回率。</p>
<h5 id="1-3-5-2-Object-Detection-Evaluation"><a href="#1-3-5-2-Object-Detection-Evaluation" class="headerlink" title="1.3.5.2 Object Detection Evaluation"></a>1.3.5.2 Object Detection Evaluation</h5><p><strong>输入图像上采样的影响</strong></p>
<p>Table 3 shows a significant improvement is obtained by up- sampling the inputs by 1.5∼2 times.</p>
<p><strong>采样策略的区别</strong></p>
<p>Table 3 compares sampling strategies: random (“h576- random”), bootstrapping (“h576”) and mixture (“h576-mixture”) </p>
<p><strong>CNN 特征估计</strong></p>
<p>本文尝试了三种方法，分别是</p>
<p>1）双线性差值权重（bilinearly interpolated weights）</p>
<p>2）双线性差值权重初始化，然后通过反向传播学习；</p>
<p>3）高斯噪声初始化，然后反向传播学习；</p>
<p>我们发现第一种方法最好。表3中显示，反卷积能在输入图像尺寸较小时提升。</p>
<p><strong>嵌入语义</strong> </p>
<p>嵌入语义信息后，精度也有提升，但是参数会增加。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fsm5xwwgg2j31480jsdi7.jpg" alt=""></p>
<h5 id="1-3-5-3-在KITTI和其他方法进行比较"><a href="#1-3-5-3-在KITTI和其他方法进行比较" class="headerlink" title="1.3.5.3 在KITTI和其他方法进行比较"></a>1.3.5.3 在KITTI和其他方法进行比较</h5><p>MS-CNN使用 <strong>h768-ctx-c</strong> 模型（没用反卷积，使用的是context encoding和dimensionality reduction）.</p>
<p><img src="http://img.xzhewei.com/note/MS-CNN/1508425810979.png" alt=""></p>
<h5 id="2-3-5-4-MS-CNN使用“h720-ctx”"><a href="#2-3-5-4-MS-CNN使用“h720-ctx”" class="headerlink" title="2.3.5.4 MS-CNN使用“h720-ctx”"></a>2.3.5.4 MS-CNN使用“h720-ctx”</h5><p><img src="http://img.xzhewei.com/note/MS-CNN/1508425896306.png" alt=""></p>
<h4 id="1-3-6-Conclusion"><a href="#1-3-6-Conclusion" class="headerlink" title="1.3.6 Conclusion"></a>1.3.6 Conclusion</h4><ul>
<li>提出了一个统一的深度卷积神经网络MS-CNN，用于快速多尺度目标检测</li>
<li>在多个中间网络层进行检测，使得感知域匹配目标尺寸</li>
<li>探究了CNN特征估计（反卷积），作为输入升采样的另一种选择，能节省计算和内存开销</li>
<li>综上，MS-CNN能实现15fps的检测速度</li>
</ul>
<h3 id="1-4-Mask-CNN-Localizing-Parts-and-Selecting-Descriptors-for-Fine-Grained-Image-Recognition"><a href="#1-4-Mask-CNN-Localizing-Parts-and-Selecting-Descriptors-for-Fine-Grained-Image-Recognition" class="headerlink" title="1.4 Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition"></a>1.4 Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition</h3><h4 id="1-4-1-Abstract"><a href="#1-4-1-Abstract" class="headerlink" title="1.4.1 Abstract"></a>1.4.1 Abstract</h4><p>传统的图像识别一般都是识别花、鸟、汽车等不同类别物体，而细粒度图像识别则是要识别同一类物体下的不同子类。举个例子，识别一张图片是猫、狗、汽车还是飞机就是传统的图像识别，而识别一张图片是贵宾犬、边境牧羊犬、吉娃娃还是斗牛犬，则是细粒度图像识别。</p>
<p>细粒度（fine-grained）图像识别主要有两个难点：</p>
<ol>
<li>类间差异小（都属于同一个物种下的小类）</li>
<li>类内差异大（受姿态、尺度和旋转等因素影响）</li>
</ol>
<p>这篇文章主要证明 <strong>selecting useful deep features</strong> 对细粒度识别有很大的作用。文中提出的Mask-CNN模型是“全卷积网络”，并基于part annotations利用了FCN来。</p>
<p>现在图像识别大都使用卷积神经网络CNN，卷积层会针对整个图像（不论是背景还是物体）提取特征，而细粒度图像识别重点在于物体的一些关键部分，如此一来CNN提取的有很多特征向量都是没用的。 </p>
<p>本文提出了Mask-CNN模型（M-CNN），它在训练时仅需要part annotations和image-level标签这两个信息。其中part annotations分成两个集合：头部和躯干，如此part localization就成了一个三类分割问题。</p>
<p>完整的网络可见下图，M-CNN是一个四线模型（four-stream），四个输入分别为完整图像、检测到的头部、检测到的躯干和检测到的完整物体，每条线程通过卷积最后都得到了deep descriptors（应该是常说的特征图），进而得到1024-d向量，将四个向量拼接在一起，通过 L2 正则化、全连接层和softmax，最后得到类别。 </p>
<p><img src="http://static.zybuluo.com/303389737/hdp86bzkmazpwp62qbsr5t4i/image.png" alt=""></p>
<h4 id="1-4-2-Learning-Object-and-Part-Masks"><a href="#1-4-2-Learning-Object-and-Part-Masks" class="headerlink" title="1.4.2 Learning Object and Part Masks"></a>1.4.2 Learning Object and Part Masks</h4><p>在数据集CUB200-2011中，每个鸟类细粒度图像都有许多part annotations，比如左腿、右腿、喉、喙、眼睛、肚子、前额等等，它们都以key points的形式标注。</p>
<p>本论文将这些key points分成头、躯干两大类，简单地连接这些点来生成头和躯干两个Mask，剩下的都是背景。学习Mask的网络结构如下所示： </p>
<p><img src="http://static.zybuluo.com/303389737/9d4ebkpocrcsuri19371vr0c/image.png" alt=""></p>
<p>以下是部分图像学习到的Mask。红色的称作<strong>head mask</strong>，蓝色的称作<strong>torse mask</strong>，这两个合并在一起就是一个完整的物体，称作<strong>object mask</strong></p>
<p><img src="http://static.zybuluo.com/303389737/hwmisocf2sltx8kiaz94adxm/image.png" alt=""></p>
<h4 id="1-4-3-Implementation-Details"><a href="#1-4-3-Implementation-Details" class="headerlink" title="1.4.3 Implementation Details"></a>1.4.3 Implementation Details</h4><ul>
<li><p>Caltech-UCSD 2011 bird dataset有200种鸟类，其中每类都有30个训练图像，每张图像还有15个标注点，用来标记鸟类的身体部位。</p>
</li>
<li><p>四线程网络中每一条都有一个VGG-16模型，其参数通过在ImageNet分类上预训练得到。</p>
</li>
<li><p><strong>作者还用水平翻转的方法使训练图像翻倍。在测试时将原图和对应的翻转图像的预测求平均，并输出得分最高的那个分类。</strong></p>
<blockquote>
<p>这个还挺有意思的</p>
</blockquote>
</li>
<li><p>直接使用softmax的结果要比使用logistic回归差</p>
</li>
</ul>
<h4 id="1-4-4-Comparision"><a href="#1-4-4-Comparision" class="headerlink" title="1.4.4 Comparision"></a>1.4.4 Comparision</h4><h5 id="1-4-4-1-Classfication-Accuracy"><a href="#1-4-4-1-Classfication-Accuracy" class="headerlink" title="1.4.4.1 Classfication Accuracy"></a>1.4.4.1 Classfication Accuracy</h5><p>这里使用的方法都没有借助于BBOX，直接使用标注的part。</p>
<blockquote>
<p>我有点疑惑Bounding BOX对于训练的帮助有多大。</p>
</blockquote>
<p>这里也提供了准确率优化的历程，从<a href="https://blog.csdn.net/Cyiano/article/details/71440358" target="_blank" rel="noopener">这里</a>看过来的。</p>
<p>M-CNN准确率的提升之路： </p>
<ul>
<li>一开始，输入的图像是224*224，M-CNN的准确率有83.1%；</li>
<li>将输入图像变为448*448后，准确率提升到了85.3%；</li>
<li>提高4-stream M-CNN的输入大小到448*448后，准确率反而有些下降；</li>
<li>如果从<strong>relu5_2</strong>层来提取deep descriptors，并且用Mask过滤一遍，提取出4096-d向量，再和<strong>pool5</strong>提取出来的拼在一起，变成一个8096-d向量，后续操作相同。该模型称作<strong>“4-stream M-CNN+”</strong>，它的准确率提升到了85.4%；</li>
<li>用SVD whitening方法将上述的8096-d向量压缩到4096-d，准确率提升到了85.5%；</li>
<li>如果CNN部分采用和<strong>part-stacked CNN</strong>一样的 Alex-Net模型，准确率只有78.0%，但还是比part-stacked CNN高。关键是替换后的参数只有9.74M了。</li>
</ul>
<p><img src="http://static.zybuluo.com/303389737/vkly55v2dkq9aik0nmcuir5l/image.png" alt=""></p>
<h5 id="1-4-3-2-Part-Localization-Results"><a href="#1-4-3-2-Part-Localization-Results" class="headerlink" title="1.4.3.2 Part Localization Results"></a>1.4.3.2 Part Localization Results</h5><p>本文采用常用的PCP准则（Percentage of Correctly Localized Parts），该准则指的是与ground-truth相比，IOU大于50%的bounding box的比例，下表是和其他方法相比的分割结果。作者的方法和Deep LAC相比躯干的准确率更低，主要是因为Deep LAC在测试时有用到Bounding box，而M-CNN没有。 </p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fsmqow0fy1j30z80kc77n.jpg" alt=""></p>
<h5 id="1-4-4-3-Object-Segmentation-Performance"><a href="#1-4-4-3-Object-Segmentation-Performance" class="headerlink" title="1.4.4.3 Object Segmentation Performance"></a>1.4.4.3 Object Segmentation Performance</h5><p>上图也包含了物体分隔的结果，第二排是ground truth，第三排是M-CNN输出的结果，M-CNN在分割鸟的细微部分（例如爪子）存在一些困难。</p>
<h4 id="1-4-5-Conclusion"><a href="#1-4-5-Conclusion" class="headerlink" title="1.4.5 Conclusion"></a>1.4.5 Conclusion</h4><p>这篇文章主要证明“selecting useful deep features”对细粒度识别有很大的作用。文中提出的Mask-CNN模型是“全卷积网络”，并基于part annotations利用了FCN来：</p>
<ol>
<li>定位关键部位（头部、躯干）</li>
<li>生成带weighted object/part mask。</li>
</ol>
<p>由于丢弃了全连接层，所以Mask-CNN相对于其他算法，速度更快效率更高；在两个鸟类数据集上取得了state of art的结果。</p>
<h3 id="1-5-Mask-R-CNN"><a href="#1-5-Mask-R-CNN" class="headerlink" title="1.5 Mask R-CNN"></a>1.5 Mask R-CNN</h3><blockquote>
<p>其实本来是想看这个的，看起来这个是语义分隔，而上篇则是细粒度物体检测。但是也误打误撞的看了Mask CNN，两者似乎除了mask（掩模）这个概念外没有什么直接的联系，看来应该去了解下mask这个概念。</p>
</blockquote>
<p>还正在阅读中，留图占坑。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fsmr91i37uj31220guai3.jpg" alt=""></p>
<h2 id="2-Dataset"><a href="#2-Dataset" class="headerlink" title="2. Dataset"></a>2. Dataset</h2><h3 id="2-1-CityPersons"><a href="#2-1-CityPersons" class="headerlink" title="2.1 CityPersons"></a>2.1 CityPersons</h3><h4 id="2-1-1-简述"><a href="#2-1-1-简述" class="headerlink" title="2.1.1 简述"></a>2.1.1 简述</h4><p>CityPersons数据集是脱胎于语义分割任务的Cityscapes数据集，对这个数据集中的所有行人提供 bounding box 级别的对齐性好的标签。</p>
<p>其数据是在3个不同国家中的18个不同城市以及3个季节中采集的，其中单独行人的数量明显高于 Caltech 和 KITTI 两个数据集。实验结果也表明，CityPersons 数据集上训练的模型在 Caltech 和 KITTI 数据集上的测试漏检率更低。表明CityPersons数据集的多样性更强，因而提高了模型的泛化能力。</p>
<h4 id="2-1-2-数据格式"><a href="#2-1-2-数据格式" class="headerlink" title="2.1.2 数据格式"></a>2.1.2 数据格式</h4><ul>
<li>数据集将所有humans分为四类：pedestrian（walking，running，standing up），rider（riding bicycles or motorbikes），sitting person，other person（非正常姿势）</li>
<li>一共5000张图像，35k个行人，13k个忽略区域。</li>
</ul>
<h4 id="2-1-3-遮挡处理"><a href="#2-1-3-遮挡处理" class="headerlink" title="2.1.3 遮挡处理"></a>2.1.3 遮挡处理</h4><p>Cityscapes数据集是通过车辆进行采集，其中包含一些著名城市的中心，如法兰克福、汉堡。有些图片中包含100多个行人，每个都有很多的遮挡。如此多的遮挡情况在其他数据集上是很少见的。图4显示了不同遮挡等级的行人的分布。我们注意到Caltech种包含60%的行人是完全可见的，而CityPersons中是30%。这表明，我们有更多的遮挡情况，这也使得我们的数据集对处理遮挡更有兴趣。并且在Resonable子集中，Caltech大部分都是非遮挡行人，而CityPersons得遮挡情况更多。</p>
<p>为了更好的理解那种情况的遮挡更多，我们将所有遮挡模式量化为11种，图5显示了其中的9种。如图5所示。前两种遮挡基本覆盖了resonable，有55.9%。第三、第四个情况是左边或右边遮挡。除了这些之外，还有30%的其他遮挡类型。遮挡类型的分布多样性使得数据集更加具有挑战性。</p>
<p>最后，论文链接在这里：<a href="https://arxiv.org/abs/1702.05693" target="_blank" rel="noopener">https://arxiv.org/abs/1702.05693</a></p>
<h3 id="2-2-CalTech-Pedestrian-Database"><a href="#2-2-CalTech-Pedestrian-Database" class="headerlink" title="2.2 CalTech-Pedestrian Database"></a>2.2 CalTech-Pedestrian Database</h3><h4 id="2-2-1-简述"><a href="#2-2-1-简述" class="headerlink" title="2.2.1 简述"></a>2.2.1 简述</h4><p>这个应该是很早期的一个数据集了。</p>
<p>该数据库是目前规模较大的行人数据库，采用车载摄像头拍摄，约10个小时左右，视频的分辨率为640x480，30帧/秒。标注了约250,000帧（约137分钟），350000个矩形框，2300个行人，另外还对矩形框之间的时间对应关系及其遮挡的情况进行标注。</p>
<p>数据集分为set00 - set10，其中set00 - set05为训练集，set06 - set10为测试集（标注信息尚未公开）。</p>
<blockquote>
<p>这个数据集似乎被上面的一篇论文diss了，加上时间原因我没有做详细，在此标注一下，以后来补。</p>
</blockquote>
<p>下载链接在：<a href="http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/" target="_blank" rel="noopener">http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/</a></p>
<h3 id="2-3-KITTI"><a href="#2-3-KITTI" class="headerlink" title="2.3 KITTI"></a>2.3 KITTI</h3><h4 id="2-3-1-简述"><a href="#2-3-1-简述" class="headerlink" title="2.3.1 简述"></a>2.3.1 简述</h4><p>该数据集用于评测立体图像(stereo)，光流(optical flow)，视觉测距(visual odometry)，3D物体检测(object detection)和3D跟踪(tracking)等计算机视觉技术在车载环境下的性能。</p>
<h4 id="2-3-2-数据形式"><a href="#2-3-2-数据形式" class="headerlink" title="2.3.2 数据形式"></a>2.3.2 数据形式</h4><p>数据首先分为下面几个类别：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fs5xsvbbu7j310o03edfs.jpg" alt=""></p>
<p>每个类别有如下的视频流图像：</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fs5xtie6wjj31h80g6td4.jpg" alt=""></p>
<p>可以在这里看到更详细的信息：<a href="http://www.cvlibs.net/datasets/kitti/raw_data.php" target="_blank" rel="noopener">http://www.cvlibs.net/datasets/kitti/raw_data.php</a></p>
<p>标注如下图的形式：</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fs5xuof6odj316e0qmk2e.jpg" alt=""></p>
<p>各个物体类型以及上述标注数据的格式描述如下：</p>
<p><img src="https://images2015.cnblogs.com/blog/534700/201701/534700-20170106150356316-732250786.jpg" alt="img"></p>
<h3 id="2-4-CalTech-Archive"><a href="#2-4-CalTech-Archive" class="headerlink" title="2.4 CalTech Archive"></a>2.4 CalTech Archive</h3><p>最近才发现CalTech除了有Pedestrian的数据之外，这里还有各种各样的数据集。</p>
<p><a href="http://www.vision.caltech.edu/archive.html" target="_blank" rel="noopener">http://www.vision.caltech.edu/archive.html</a></p>
<p>比如这些车的数据集，在Car detection中应该会有所用处。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fsmr1vp0g1j30tk0us0v9.jpg" alt=""></p>
<p>再比如这些传统物体的数据集（下图为CalTech-CUB200），都还挺有趣的。</p>
<p><img src="http://www.vision.caltech.edu/graphics/cub_200_collage.jpg" alt=""></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li>How Far are We from Solving Pedestrian Detection? / CVPR 2016</li>
<li>Filtered channel features for pedestrian detection. / CVPR 2015</li>
<li>Is Faster R-CNN Doing Well for Pedestrian Detection? / EECV 2016</li>
<li>Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors)</li>
<li>Benenson, R., Mathias, M., Timofte, R., Gool, L.J.V.: Pedestrian detection at 100 frames per second. In: CVPR. (2012) 2903–2910 </li>
<li>Girshick, R.B., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: CVPR. (2014) 580–587 </li>
<li>Object detection via a multi-region and semantic segmentation-aware CNN model. ICCV(2015)</li>
<li>Object proposals for accurate object class detection. NIPS(2015) </li>
<li>Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition</li>
<li>Mask R-CNN / FAIR 何恺明</li>
</ol>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018-06-24/Joseph-Sifakis-如何保障自动驾驶系统的安全性/" rel="next" title="Joseph Sifakis-如何保障自动驾驶系统的安全性">
                <i class="fa fa-chevron-left"></i> Joseph Sifakis-如何保障自动驾驶系统的安全性
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019-03-12/Trident-Network-阅读笔记/" rel="prev" title="Trident Network 阅读笔记">
                Trident Network 阅读笔记 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/my_face.png"
                alt="Luodian" />
            
              <p class="site-author-name" itemprop="name">Luodian</p>
              <p class="site-description motion-element" itemprop="description">Hello, my world.</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">43</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">13</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">tags</span>
                  
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/Luodian" target="_blank" title="GitHub"><i class="fa fa-fw fa-globe"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://twitter.com/naidoul" target="_blank" title="Twitter"><i class="fa fa-fw fa-globe"></i>Twitter</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://weibo.com/2657993174/profile?rightmod=1&wvr=6&mod=personinfo&is_all=1" target="_blank" title="Weibo"><i class="fa fa-fw fa-globe"></i>Weibo</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/luo-dian-zhu-84/activities" target="_blank" title="zhihu"><i class="fa fa-fw fa-globe"></i>zhihu</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Car-Pedestrian-detections-and-datasets-review"><span class="nav-number">1.</span> <span class="nav-text">Car/Pedestrian detections and datasets review</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Car-Pedstrains-detection-reviews"><span class="nav-number">1.1.</span> <span class="nav-text">1. Car / Pedstrains detection reviews</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-How-Far-are-We-from-Solving-Pedestrian-Detection"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1 How Far are We from Solving Pedestrian Detection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-0-Abstract"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">1.1.0 Abstract</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-Analyzing-the-state-of-the-art"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">2.1.1 Analyzing the state of the art.</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-Failure-analysis"><span class="nav-number">1.1.1.3.</span> <span class="nav-text">2.1.2 Failure analysis</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-3-Improving-Annotation"><span class="nav-number">1.1.1.4.</span> <span class="nav-text">2.1.3 Improving Annotation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-4-Analysis"><span class="nav-number">1.1.1.5.</span> <span class="nav-text">2.1.4 Analysis</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-5-论文信息"><span class="nav-number">1.1.1.6.</span> <span class="nav-text">2.1.5 论文信息</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Is-Faster-R-CNN-Doing-Well-for-Pedestrian-Detection"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2 Is Faster R-CNN Doing Well for Pedestrian Detection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-Abstract"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">1.2.1 Abstract</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-Approach"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">2.2.2 Approach</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-2-1-Region-Proposal-Network-for-Pedestrian-Detection"><span class="nav-number">1.1.2.2.1.</span> <span class="nav-text">2.2.2.1 Region Proposal Network for Pedestrian Detection</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-2-2-Feature-Extraction"><span class="nav-number">1.1.2.2.2.</span> <span class="nav-text">2.2.2.2 Feature Extraction</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-2-3-Boosted-Forest"><span class="nav-number">1.1.2.2.3.</span> <span class="nav-text">2.2.2.3 Boosted Forest</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-3-Comparasion"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">2.2.3 Comparasion</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-5-Conclusion"><span class="nav-number">1.1.2.4.</span> <span class="nav-text">2.2.5 Conclusion</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-5-论文信息"><span class="nav-number">1.1.2.5.</span> <span class="nav-text">2.2.5 论文信息</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-A-Unified-Multi-scale-Deep-Convolutional-Neural-Network-for-Fast-Object-Detection"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3 A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-Abstract"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">1.3.1 Abstract</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-2-Related-Work"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">1.3.2 Related Work</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-3-Proposal-Network"><span class="nav-number">1.1.3.3.</span> <span class="nav-text">1.3.3 Proposal Network</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3-3-1-Multi-Branch-Loss"><span class="nav-number">1.1.3.3.1.</span> <span class="nav-text">1.3.3.1 Multi-Branch Loss</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3-3-2-Loss-of-each-detection-layer"><span class="nav-number">1.1.3.3.2.</span> <span class="nav-text">1.3.3.2 Loss of each detection layer</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3-3-3-Sampling"><span class="nav-number">1.1.3.3.3.</span> <span class="nav-text">1.3.3.3 Sampling</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-4-Object-Detection-Network"><span class="nav-number">1.1.3.4.</span> <span class="nav-text">1.3.4 Object Detection Network</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3-4-1-CNN-Feature-Map-Approximation"><span class="nav-number">1.1.3.4.1.</span> <span class="nav-text">1.3.4.1 CNN Feature Map Approximation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3-4-2-Context-Embedding（创新点）"><span class="nav-number">1.1.3.4.2.</span> <span class="nav-text">1.3.4.2 Context Embedding（创新点）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-5-Experimental-Evaluation"><span class="nav-number">1.1.3.5.</span> <span class="nav-text">1.3.5 Experimental Evaluation</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3-5-1-Proposal-Evaluation"><span class="nav-number">1.1.3.5.1.</span> <span class="nav-text">1.3.5.1 Proposal Evaluation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3-5-2-Object-Detection-Evaluation"><span class="nav-number">1.1.3.5.2.</span> <span class="nav-text">1.3.5.2 Object Detection Evaluation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-3-5-3-在KITTI和其他方法进行比较"><span class="nav-number">1.1.3.5.3.</span> <span class="nav-text">1.3.5.3 在KITTI和其他方法进行比较</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-3-5-4-MS-CNN使用“h720-ctx”"><span class="nav-number">1.1.3.5.4.</span> <span class="nav-text">2.3.5.4 MS-CNN使用“h720-ctx”</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-6-Conclusion"><span class="nav-number">1.1.3.6.</span> <span class="nav-text">1.3.6 Conclusion</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-Mask-CNN-Localizing-Parts-and-Selecting-Descriptors-for-Fine-Grained-Image-Recognition"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.4 Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-1-Abstract"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">1.4.1 Abstract</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-2-Learning-Object-and-Part-Masks"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">1.4.2 Learning Object and Part Masks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-3-Implementation-Details"><span class="nav-number">1.1.4.3.</span> <span class="nav-text">1.4.3 Implementation Details</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-4-Comparision"><span class="nav-number">1.1.4.4.</span> <span class="nav-text">1.4.4 Comparision</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-4-4-1-Classfication-Accuracy"><span class="nav-number">1.1.4.4.1.</span> <span class="nav-text">1.4.4.1 Classfication Accuracy</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-4-3-2-Part-Localization-Results"><span class="nav-number">1.1.4.4.2.</span> <span class="nav-text">1.4.3.2 Part Localization Results</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-4-4-3-Object-Segmentation-Performance"><span class="nav-number">1.1.4.4.3.</span> <span class="nav-text">1.4.4.3 Object Segmentation Performance</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-5-Conclusion"><span class="nav-number">1.1.4.5.</span> <span class="nav-text">1.4.5 Conclusion</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-Mask-R-CNN"><span class="nav-number">1.1.5.</span> <span class="nav-text">1.5 Mask R-CNN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Dataset"><span class="nav-number">1.2.</span> <span class="nav-text">2. Dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-CityPersons"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 CityPersons</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-简述"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">2.1.1 简述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-数据格式"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">2.1.2 数据格式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-3-遮挡处理"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">2.1.3 遮挡处理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-CalTech-Pedestrian-Database"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 CalTech-Pedestrian Database</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-简述"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">2.2.1 简述</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-KITTI"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3 KITTI</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-1-简述"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">2.3.1 简述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-2-数据形式"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">2.3.2 数据形式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-CalTech-Archive"><span class="nav-number">1.2.4.</span> <span class="nav-text">2.4 CalTech Archive</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">1.3.</span> <span class="nav-text">References</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Luodian</span>

  

  
</div>











        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  















  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.5"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.5"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.0.5"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.0.5"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.5"></script>



  



	





  





  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("P3KDAQ4HdXF3uAgiKSHuWo12-gzGzoHsz", "zmMRXMn5P4sO2kcK5UBY6F6g");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            
            counter.save(null, {
              success: function(counter) {
                
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(counter.get('time'));
                
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            
              var newcounter = new Counter();
              /* Set ACL */
              var acl = new AV.ACL();
              acl.setPublicReadAccess(true);
              acl.setPublicWriteAccess(true);
              newcounter.setACL(acl);
              /* End Set ACL */
              newcounter.set("title", title);
              newcounter.set("url", url);
              newcounter.set("time", 1);
              newcounter.save(null, {
                success: function(newcounter) {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
                },
                error: function(newcounter, error) {
                  console.log('Failed to create');
                }
              });
            
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  
  

  

  

  

  

</body>
</html>
