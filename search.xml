<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[在Ignite框架下解决Opencv中Mat类型传递的问题]]></title>
    <url>%2F2018-03-08%2F%E5%9C%A8Ignite%E6%A1%86%E6%9E%B6%E4%B8%8B%E8%A7%A3%E5%86%B3Opencv%E4%B8%ADMat%E7%B1%BB%E5%9E%8B%E4%BC%A0%E9%80%92%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[作者：落点 目前在做一个分布式计算的应用，使用当下很新的Apache Ignite框架，因为中文相关博客几乎没有，我们的开发小组计划着写一点文字用于记录下我们遇到的问题，希望能够帮助到后人。 昨天在写前端摄像头获取图像，后端处理图像识别人脸的过程中，在后端从DataGrid的cache中获取opencv的特定图像数据类型Mat时遇到了问题，总是无法获取，提醒为null的情况。 Ignite并没有详细的对异常原因作出提示，我们经过排查之后发现了可能是序列化的问题（因为java内置的许多数据类型如string,int等都可以传输）。 而Mat类型的实现中是没有序列化的，因此我们需要手动的序列化。 该如何做呢？ 显然不是写一个 1class MyMat implements Serialization 这么简单。 我们这里提供一种思路，将其转化为字节数组byte[]进行序列化。 序列化的过程如下，这里要将矩阵Mat的结构信息传输过去，方便对方根据字节数组byte[]还原矩阵。 123456789101112131415IgniteCache&lt;Integer,byte[]&gt; igniteCache = ignite.getOrCreateCache ("stream_data");IgniteCache&lt;Integer,int[]&gt; matcache = ignite.getOrCreateCache ("matcache");Mat m1 = new Mat();byte[] bytes = new byte[(int) (m1.total()*m1.elemSize())];m1.get(0,0,bytes);int[] mat_profile = new int[5];mat_profile[0] = m1.type ();mat_profile[1] = m1.rows();mat_profile[2] = m1.cols();igniteCache.put(1,bytes);matcache.put(1,mat_profile); 反序列化的过程如下： 123456789IgniteCache&lt;Integer,byte[]&gt; igniteCache = ignite.getOrCreateCache("stream_data");IgniteCache&lt;Integer,int[]&gt; matcache = ignite.getOrCreateCache("matcache");int[] matprofile = new int[5];matprofile = matcache.get(1);byte[] bytes = igniteCache.get(1);Mat m2 = new Mat(matprofile[1],matprofile[2],matprofile[0]);m2.put(0,0,bytes); 这里通过两个cache，一个用来传输字节数组，一个用来传输矩阵信息，先压缩，再解压，最终能够成功的实现序列化。 最后附上小广告 在Ignite的道路上，我们都是初学者，但是我们会努力的在开发的过程中，不断的去写技术博客的形式，来传播我们的经验。 想要加入我们？或者是获取更多的知识？ 可以联系我的QQ：1121058986，备注Ignite即可。 也可以加入我们的QQ群：481810803，群内有技术大牛和在分布式系统方面经验丰富的指导老师为大家答疑解惑。 让我们一起，点燃这火，ignite~]]></content>
      <categories>
        <category>Apache Ignite</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Contest-934-a-Compatible Pair]]></title>
    <url>%2F2018-03-06%2FContest-934-A-Compatible-Pair%2F</url>
    <content type="text"><![CDATA[A题： http://codeforces.com/contest/934/problem/A 讲道理这道题是要比 B 更难的，在 case 10 这里 WA 掉了好多次。 这个题 Alice 可以藏一个自己的数，Bob 则想办法从A那里拿一个数和自己这里的某个数相乘，使得最终的结果最大。开始的策略想的是排序，但是发现在负数的时候会出问题。 现在考虑一个场景，当 Alice 和 Bob 都有负数的时候，负负得正，-3,-3,2,4这样的，排序取最大相乘答案就错了。 再考虑了下解决负数时也可以分类讨论，将A和B的序列都分成 A_minus 和 A_plus 来搞，也有另外的解法，可以直接模拟，每次 Alice 拿走一个，然后 Bob 做 m * n的枚举找剩下序列中的最大值，然后最后再求所有的 m * n 的最小值即是最终的解。 code如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#include&lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;iomanip&gt;#include &lt;string&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;cstdlib&gt;#include &lt;vector&gt;#include &lt;queue&gt;#include &lt;stack&gt;#include &lt;cmath&gt;#include &lt;bitset&gt;#include &lt;unordered_set&gt;#include &lt;numeric&gt;#include &lt;set&gt;#include &lt;list&gt;#include &lt;map&gt;using namespace std;#define lower_bound LB#define upper_bound UB#define mem(a, x) memset(a,x,sizeof(a))#define rep(i, a, n) for (int i=a;i&lt;n;i++)#define per(i, a, n) for (int i=n-1;i&gt;=a;i--)#define mp make_pair#define all(x) (x).begin(),(x).end()#define SZ(x) ((int)(x).size())#define IT iterator#define test puts("OK")#define lowbit(x) x &amp; -x#define PRQ priority_queue#define PB push_back#define gcd(a, b) _gcd(a,b)typedef long long LL;typedef unsigned long long uLL;typedef pair&lt;int, int&gt; pii;typedef vector&lt;int&gt; VI;typedef pair&lt;int, int&gt; PII;typedef vector&lt;PII&gt; VPII;const LL mod = 1000000007;const double PI = acos (-1.0);const double eps = 1e-8;const int INF = 0x3f3f3f3f;const LL LINF = 0x3f3f3f3f3f3f3f3f;int n,m;const int maxn = 55;LL ar_n[maxn];LL ar_m[maxn];int main ()&#123;#ifndef ONLINE_JUDGE freopen ("A.txt", "r", stdin);#endif ios::sync_with_stdio (false); cin.tie (nullptr); cout.tie (nullptr); while(cin&gt;&gt;n&gt;&gt;m) &#123; mem(ar_n,0); mem(ar_m,0); for (int i = 0; i &lt; n ; ++i) &#123; cin&gt;&gt;ar_n[i]; &#125; for (int i = 0; i &lt; m; ++i) &#123; cin&gt;&gt;ar_m[i]; &#125; LL suit1 = LINF; LL suit2 = -LINF; for (int i = 0; i &lt; n; ++i) &#123; suit2 = -LINF; for (int j = 0; j &lt; n; ++j) &#123; if (j != i) &#123; for (int k = 0; k &lt; m; ++k) &#123; suit2 = max (suit2,ar_n[j] * ar_m[k]); &#125; &#125; &#125; suit1 = min(suit2,suit1); &#125; cout&lt;&lt;suit1&lt;&lt;"\n"; &#125; return 0;&#125;]]></content>
      <categories>
        <category>Codeforces</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Next主题下MathJax与Markdown不兼容问题]]></title>
    <url>%2F2018-03-04%2FNext%E4%B8%BB%E9%A2%98%E4%B8%8BMathJax%E4%B8%8EMarkdown%E4%B8%8D%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题描述由于mathjax和markdown的冲突，在某些mathjax语句中，会出现如下这种渲染冲突的情况。 这是因为在Markdown语法中，两个下划线之间的文本会被转换为斜体，所以这个错误是由于Markdown本身没有支持Latex，Markdown文本先交由marked.js（Hexo默认渲染器）对文本进行渲染时，将_替换成了&lt;em&gt;标签，然后才被Mathjax交由mathjax.js进行渲染，导致无法正确识别公式。同样的问题也发生在\\经过转义后变成\，MathJax渲染时不能正确识别换行符。 解决方案 Pandoc 理解了这个问题的本质是Markdown与LaTeX语法冲突后，我们来理一理解决问题的思路。最根本的解决方法当然是从Markdown语法本身入手，换用有着更strong的语法的标记语言来避免冲突，比如pandoc。pandoc大法固然好，但是为了保持博客的轻量级（当初就是为了这个从Wordpress转到了Markdown+Hexo），暂时还不打算动用pandoc这个核武器。感兴趣的小伙伴可以去了解一下pandoc，与之对应的Hexo插件hexo-renderer-pandoc。 保护公式块 第二个思路就是利用Markdown特有的rawblock标签保护LaTeX代码块，如使用下图的双美元符号，将公式整体括起来。 ​ 替换渲染引擎 修改如下： 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 小结我个人推荐使用3+2的解决方法，这样可以完美无误的书写任何公式，在使用kramed渲染+使用双美元符号时不用换行，直接在行内书写即可，最终的效果如下。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[异常检测-面向大数据的异常检测算法设计分析]]></title>
    <url>%2F2018-03-04%2F%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E9%9D%A2%E5%90%91%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[15-李博 时间序列 时间序列是指按照时间先后顺序排列的各个观测记录的有序集合，广泛存在于商业、经济、工程、社会科学和医学等领域。随着时间的推移 ，时问序列通常包含大量的信息，是建模和预测的主要依据。对时间序列进行分析，可以揭示事物运动、变化和发展的内在规律，对于人们正确认识事物并据此做出科学决策具有重要的现实意义。 时间序列的一个固有特征是相邻的观测值之间存在着相互的依赖性，这种相互的依赖性往往蕴含着被观测事物或现象在特定环境或特定时刻的大量信息，研究与分析这种相互的依赖性具有极大的实用价值。 异常数据异常是数据中的特定模式，这些模式与事先定义的正常模式存在不一致。时间序列异常是时间序列上下文中与正常模式存在不一致的时间子序列。在时间序列中，数据每一时期都受到多种因素的共同作用，通常产生异常点的原因主要包括： 数据受到新机制的作用，如欺诈、入侵、疾病的爆发、不寻常的实验结果等。这些异常点出现是因为有新事物出现或者新情况发生，比如经济领域时间序列研究中，某种经济政策的出台；地质模型中某种可能含有矿藏的地层的发现；由于罢工、广告促销、突发性政治或经济重大事件、物理系统的突变等，这些因素会造成不同于寻常模式的观测结果。这类异常点通常蕴涵着具体的意义，也往往是研究者感兴趣 的，异常点诊断旨在识别出这些现象背后的本质起因。 数据变化固有规律引起，这是自然发生的，反映了数据的分布特征，如气候变化、基因突变等。 数据测量收集误差引起，主要是由于人为差错、测量仪器故障。 时间序列是受监测事物状态的具体表现，而异常数据同样也包含着不可忽略的重要信息，数据的异常表明着数据所表征的物理设备或者是统计信息在局部或者是周期出现了异常的波动，而这些波动是我们需要在生产生活中去检测到并加以避免的。 基本算法针对于异常检测，前人已经做了很多相关的工作，这里我们列举一些异常检测的基本算法思路。 基于假设检验的方法 假设检验是最早用来发现异常样本的基于统计学原理的方法，它基于对小概率事件的判别来实现对数据样本异 常性的鉴别，如 t 检验、Dixon 检验 、Grubs 检验等。 这种方法的难点在于对于分布特征未知的数据，使用先验假设具有很大的不确定性和局限性。 基于线性模型的统计学方法 对原始数据变换弱化原时间序列的相关性，使其满足经典线性回归的各项假设条件。在回归框架下又细分为残差分析方法和影响分析方法，前者是根据线性框架的拟合效果来判别数据是否异常，后者则是根据数据对于统计量的影响来判别。 这种方法的效率很高，但是对于数据选择有一定的局限性，大部分的经典的线性回归模型，如ARIMA等，都需要数据满足一些前提假设。 基于聚类的算法 这类算法将数据集分成若干类 ，不属于任何类的数据点就是异常点，比较典型的算法有 DBSCAN，Isolated Forest等。这类方法看似好用，但是却存在着算法效率较低，而且特定的算法对于数据集以及参数的要求较高，较难真正的普适到更多的应用场景。 基于密度的算法 基于密度的方法主要有（LOF：local outlier factor），LOF即为数据对象邻域的平均可达密度与其自身的可达密 度之比，LOF越大，其离群程度越高。LOF检测的效果不错，但是同样依赖于参数而且算法效率较低。 基于极值理论的算法 大道至简，近年来，很多学者开始将极值理论应用到时间序列异常诊断中，模型异常点诊断的关键就是决定检验统计量在一定的显著水平下是否超越某一临界值。这种方法可行且好用，可以作为 2 类方法设计的补充或者是一部分，且关于如何选取阈值大小，以及阈值种类，都是有着很多的学问以及相关研究。 工业级别大数据的异常检测随着技术的发展，可监测对象的种类越来越多，采集数据的设备数量越来越大。时间序列的多样性与规模化对时间序列异常检测方法提出了新的要求。 上述的算法在面向工业数据应用时，普遍存在着检测效果以及检测性能的问题。 现有异常检测方法在检测效果方面无法适应时间序列多样化的要求，大多方法脱离了论文，应用到实战中，便需要结合对于数据的深刻理解，才能够调整出合适的参数，识别出异常。 另外大部分方法基于机器学习算法设计，算法检测效果虽然较好，但是一旦数据集过大，在内存和时间方面的效率远不能达到我们理想的要求。 在结合大数据方面的相关文献以及我们对于异常检测问题的粗浅理解，在下面我们简单的分析针对于工业级别大数据的异常检测算法的设计思路。 通过降维减小数据维度 通常，我们的时间序列数据是多维的，多维时间序列可用于描述受监测事物的多个方面状态与情况，然而随着维度的增长，多时间序列异常异常检测的计算时间会快速增加。通过观察发现当时间序列存在与异常发生原因无关的维度时，进行一些相关性分析，去除无关维度不会对异常检测的准确性产生决定性影响。 使用简单的线性判别 在时间序列的研究当中，很多时候少即是多，时间序列受随机性，趋势性影响较大，往往越多的参数，越复杂的模型意味着很难去长时间，更泛化的拟合真实的数据。而模型的复杂通常也意味着复杂度的提升，因此我们提倡在大数据算法的设计当中，可以适当的为了泛化能力以及时间效率，选择较为简单的线性方法进行粗略的异常判别，后续再使用更精确的算法对子区间进行判别。 尽量设计在线的算法 目前大多数异常检测方法均为静态方法，即对历史中特定段落的时间序列进行分析并得出结果。静态的时间序列方法不能应用于实时的时间序列异常检测。然而在许多应用场景中时间序列是不断增长的，因此我们对于实时获得的时间序列中的异常的需求同样迫切。而且在线的算法也意味着效率近似于线性，是一件非常让人愉悦的事情。 通过并行的思想改善耗时部分的性能 大数据的异常检测对于检测方法提出了存储能力与计算能力的新要求。单个计算结点的存储能力与计算能力无法满足这些要求，我们需要利用并行化计算的方法改善异常检测方法的检测性能。 如何设计 Master 和 Slaver 结点的算法，以及它们划分，合并的关系，将哪个部分正确的，有效的用于并行计算，是这个方向上的研究重点。 做好时间与检测效果的 Trade Off 针对于大数据所设计出的算法，应该有相应的调整系数，能够平衡效率和检测效果的不同侧重。对于某些不需要高精度检测，但是实时性较强的环境，可以通过调整参数来达到相应的需求。 以框架的方式设计算法 以上所提出的几点，均是这个方向上的算法的设计思路，或者说期望达到的目标，一个算法想要很好的结合这三点是很困难的。因此我们认为或许可以使用框架的方式，将各种手段或者技术进行合理的整合，比如异常序列的粗预警，细预警的分离，多维数据，多指标检测的并行分离，等等手段，都可以通过框架的方式进行有机的整合，最终形成一个良好可用的算法。 总结总的来说，时间序列的异常检测是一个在当下 CV 和 NLP 等异常火热的时代下，看起来似乎不那么耀眼的问题，但是它对于这个世界的生产生活具有同等，似乎还有更重要的意义。 目前对于普通的异常检测可能研究相对较多，但是考虑大数据背景的时序数据的异常检测的相关研究目前还不是很充分，这是一个急需去占领的高地，希望我们能在未来，看到更多的相关问题的解决方案。]]></content>
      <categories>
        <category>异常检测</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Contest-911-D-Inversion Counting]]></title>
    <url>%2F2018-01-24%2FContest-911-D-Inversion-Counting%2F</url>
    <content type="text"><![CDATA[http://codeforces.com/contest/911/problem/D 题目大意为：逆序对问题，事先求出有多少个逆序对，只用知道是奇数个还是偶数个即可，然后现在我们对l,r区间的序列进行 reverse 之后再问序列的逆序对是奇数个还是偶数个。 因为 $l,r$ 区间反转之后，如果原来的全是顺序对，那么现在会造成总共 \frac{(len(l,r) * len(l,r) - 1)}{2} 个逆序对，所以总的逆序对个数为 原有逆序对（区间外逆序对个数）+ 新逆序对，如果新逆序对个数为奇数，则总逆序对的奇偶性发生改变。 如果原来的区间里含有m个逆序对，区间外逆序对个数为 $p$ 个，那么反转之后，逆序对会变成顺序对，原有顺序对会变成逆序对，同样总共会有新的 \frac{(len(l,r) * len(l,r) - 1)}{2} - m 个逆序对产生，总的逆序对数变为\frac{(len(l,r) * len(l,r) - 1)}{2} - m + p 个，原逆序对数为 $m + p$ 个，怎么在只知道 $m + p$ ，不知道 $m$ 的情况下，推出的奇偶 \frac{(len(l,r) * len(l,r) - 1)}{2} - m + p 性呢？ 可以利用 \frac{(len(l,r) * len(l,r) - 1)}{2} - m + p = \frac{(len(l,r) * len(l,r) - 1)}{2} + m + p - 2m ，然后就可以利用 $m + p$ 的奇偶性，以及 \frac{(len(l,r) * len(l,r) - 1)}{2} 的奇偶性，推出 \frac{(len(l,r) * len(l,r) - 1)}{2} - m + p 的奇偶性了。 代码很简单： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;iomanip&gt;#include &lt;string&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;cstdlib&gt;#include &lt;vector&gt;#include &lt;queue&gt;#include &lt;stack&gt;#include &lt;cmath&gt;#include &lt;bitset&gt;#include &lt;unordered_set&gt;#include &lt;numeric&gt;#include &lt;set&gt;#include &lt;list&gt;#include &lt;map&gt;using namespace std;#define lower_bound LB#define upper_bound UB#define mem(a,x) memset(a,x,sizeof(a))#define rep(i,a,n) for (int i=a;i&lt;n;i++)#define per(i,a,n) for (int i=n-1;i&gt;=a;i--)#define mp make_pair#define all(x) (x).begin(),(x).end()#define SZ(x) ((int)(x).size())#define IT iterator#define test puts("OK")#define lowbit(x) x &amp; -x#define PRQ priority_queue#define PB push_back#define gcd(a,b) _gcd(a,b)typedef long long LL;typedef unsigned long long uLL;typedef pair&lt;int,int&gt; pii;typedef vector&lt;int&gt; VI;typedef pair&lt;int,int&gt; PII;typedef vector&lt;PII&gt; VPII;const LL mod=1000000007;const double PI = acos(-1.0);const double eps = 1e-8;const int INF = 0x3f3f3f3f;int main()&#123; #ifndef ONLINE_JUDGE freopen("D.txt","r",stdin); #endif ios::sync_with_stdio(false); cin.tie(nullptr); cout.tie(nullptr); int n,m; int l,r; vector&lt;int&gt; ar; ar.clear(); while(cin&gt;&gt;n) &#123; ar.clear(); for (int i = 0; i &lt; n; ++i) &#123; int temp; cin&gt;&gt;temp; ar.PB(temp); &#125; bool ret = 0; for (int i = 0; i &lt; n; ++i) &#123; for (int j = i; j &lt; n; ++j) &#123; if (ar[j] &lt; ar[i]) &#123; ret ^= 1; &#125; &#125; &#125;// cout&lt;&lt;ret&lt;&lt;endl; cin&gt;&gt;m; for (int i = 0; i &lt; m; ++i) &#123; cin&gt;&gt;l&gt;&gt;r; int permutations = (r - l) * (r - l + 1) / 2; if (permutations &amp; 1) &#123; ret ^= 1; &#125; if (ret &amp; 1) &#123; cout&lt;&lt;"odd\n"; &#125; else &#123; cout&lt;&lt;"even\n"; &#125; &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>Codeforces</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基于统计方法进行时序数据预测的异常检测模型]]></title>
    <url>%2F2017-12-29%2F%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E9%A2%84%E6%B5%8B%E7%9A%84%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[时间序列数据异常检测报告 15级-李博 2017-12-29 基于统计方法进行时序数据预测的异常检测模型简述基于统计的预测，无非是根据收集过去时间的数据，建立一个模型，来计算未来时间的数据，建立的是一种数学或者统计模型，它能表现出已有数据的变化规律，因为大数定理的存在，定义了世间所有的行为都可以通过数字表示，并且存在一定的客观规律。 对于股票市场中存在的量化交易这一概念，即是指以先进的数学模型替代人为的主观判断，利用计算机技术从庞大的历史数据中海选能带来超额收益的多种『大概率』事件以制定策略，极大地减少了投资者情绪波动的影响，避免在市场极度狂热或悲观的情况下作出非理性的投资决策。 在这里我们从这个方向入手，通过传统股票市场的预测分析工具来进行我们的工业传感器数据分析，首先我简要分析其关联性： 股票数据和传感器数据都具有趋势性，以及一定程度的随机性，趋势性保证了两者的数值会按照一定的规律变化，并且从长时间数据的角度看，其是较为连续的，因此我们可以使用ARIMA，EA等模型进行平滑处理，分析异常点。 股票数据根据金融因素，考虑通货膨胀等原因，可能会存在不断起伏，但是大趋势增长的情况。但是对于传感器数据，大多会在一定范围内震荡（如温度，湿度等数据因为物理因素的原因，不会高出一定范围），所以我们可以在EA（适用于渐进上升型数据）或者是ARIMA模型（适用于周期波动性数据）中进行决策。 传感器数据可能存在一定的周期，但是股票数据不一定存在明显周期性，这一点也是需要对模型进行修正调整的考虑因素之一。 模型方法介绍在上一次周报中，我详细介绍了这种方法，现在在这里简单略过。 基于预测的异常检测模型如下图所示，$O_{data}$ 是真实数据，通过预测器得到预测数据，然后 $O_{data}$ 和 $P_{data}$分别作为比较器的输入，比较器输出的是真实数据中被判别为异常值的下标 $Index$。 预测器时间序列分析一般假设我们获得的数据在时域上具有一定的相互依赖关系，通常，如果传感器数值在 $t$ 时刻很高，那么在 $t+1$ 时刻价格也有一定的概率会比较高，而时间序列分析的目的包含以下两个方面： 发现这种隐含的依赖关系，并增加我们对此类时间序列的理解； 对未观测到的或者尚未发生的时间序列进行预测。 在接下来的分析中，我们认为时间序列 $X$ 由两部分组成，即 $X_t=\hat{X}_t+\epsilon_t$. 其中$\hat{X}_t$ 是有规律的序列而$ \epsilon_t$ 则无规律的噪声。有规律的 $X_t$ 包含我们想要发现的依赖关系（pattern），而 $\epsilon_t$ 我们认为在时间域内不存在相互依赖的关系，即 $\epsilon_t$ 和 $\epsilon_{t+1}$ 之间是相互独立的。 一个最简单的模型就是我们假设 $\epsilon_t$ 是一个随机数，服从一定的概率分布 $f_t(\epsilon)$。 可以发现，我们想要找到 $\hat{X}_t$ 而对 $ϵ_t$不怎么感兴趣。为了使有规律的 $\hat{X}_t$ 更加明显，我们通常希望能过滤到噪声，而最简单的过滤噪声的方法就是『取平均』。 而问题来了，对于时间序列信号来说，我们该如何取平均呢？这里便引出了我们的基于历史数据平滑曲线的几种方法，也即我们的预测期。 我们使用的预测器主要有以下几种（还在等待其余方法的补充）。 对于时序数据的 $F_{t+m}$ 的预测方法MA(滑动平均模型)这种方法并不考虑数据的趋势性，单纯根据历史信息来求得当前点的滑动平均数值，参数 T（也即WindowSize） 决定了依赖历史的程度。 我们用 $S$ 表示处理后的序列，那么 $S_t$ 等于 $X_{t−T+1}$ 到 $X_t$ 的平均值，即 $S_t=\frac{1}{T}\Sigma^t_{i=t−T+1}X_i$ 我们使用 $S_t \simeq \hat{X_t}$，下面我们使用滑动平均来预测 windmachine 的第一列的数据，并根据预测值进行异常检测。 EA(指数平均模型)在这里我们使用二阶指数平均，二阶在一阶的基础上增加了对于趋势的考量，更符合我们的数据要求，而三阶需要依赖于周期性，这点和上述的 ARIMA 模型所依赖的周期性也是相同的，下一步我们也需要加入周期性的考量。 我们可以看到，虽然指数平均在产生新的数列的时候考虑了所有的历史数据，但是仅仅考虑其静态值，即没有考虑时间序列当前的变化趋势。如果当前的股票处于上升趋势，那么当我们对明天的股票进行预测的时候，好的预测值不仅仅是对历史数据进行『平均』，而且要考虑到当前数据变化的上升趋势。同时考虑历史平均和变化趋势，这边是二阶指数平均。我们先给出二阶指数平均的两种方法，如下 $initialize$$S_1 = X_1$$b_1 = X_1−X_0$$for t &gt; 1$$S_t = αX_t + (1−α) ( S_t−1+b_t−1)$$b_t = β(S_t−S_t−1)+(1−β)b_t−1$$end for$ 如果我们对 $X_{t+m}$ 之后的数值进行预测，那么我们的预测值为$\hat{X}_{t+m} = S_t + mb_t$ $S′_0=X_0$$S^{′′}_0=X_0$$for t \geq 1$$\quad S^{‘}_t = \alpha X_t+(1-\alpha)S_{t-1}^{‘} \quad S^{‘’}_t = \alpha S^{‘}_t +(1-\alpha)S_{t-1}^{‘’}\quad S_t = 2S^{‘}_t-S^{‘’}_t$ end for 在方法二中，只有一个参数 $α$ 。其中$S^{‘}_t$为最基本的指数平均得到的结果，而 $S^{‘}_t - S^{“}_t$ 为变化的趋势. 如果我们对 $X_{t+m}$ 之后的数值进行预测，那么我们的预测值为$\hat{X}_{t+m} = S_t +(m\frac{\alpha}{1-\alpha})(S^{‘}_t-S^{‘’}_{t})$ 我们利用二阶指数平均对于数据进行处理及异常检测的结果如下： 用于修正的方法布林通道布林通道是一个在股票市场中经常使用的概念，它在应用上结合了移动平均和标准差的概念，其基本的型态是由三条轨道线组成的带状通道（中轨和上、下轨各一条）。上下轨分别由平均值加减二倍标准差（）得到，中轨为股价的平均成本，上轨和下轨可分别视为股价的压力线和支撑线。 下图中红线为上轨，绿线为下轨，蓝线为滑动平均值。 由大数定律以及高斯分布模型，我们可以获知在上下二倍标准差基本涵盖了数据变化的95%左右的情况，如果超出这个分布，那么极有可能是异常点，因此在我们的模型中我们结合布林通道进行了异常点的修正。 RSI指数RSI指数在证券市场中适用于衡量一段周期内增长和下降强弱对比的一个指标，其计算公式如下： RSI = 100×RS / (1+RS) RS = X天的平均上涨点数 / X天的平均下跌点数 通俗的讲，RSI 指数过高代表上涨明显（超买现象），意味着有可能会存在潜在的下跌，RSI指数过低则反之。 下面是我们使用布林通道进行异常检测的测试： 下面是我们使用修正之后的模型结合EA方法进行异常检测的情况： 比较器预测器预测出当前时刻传感器的预测值后，还需要与真实值比较来判断当前时刻数据是否异常。一般的比较器都是通过阈值法，比如实际值超过预测值的一定比例就认为该点出现异常，进行报警。这种方式错误率比较大。在传感器数值模型的报警检测中没有使用这种方式，而是使用了两个串联的 Filter，只有当两个 Fliter 都认为该点异常时，才进行报警，下面简单介绍一下两个 Filter 的实现。 离散度Filter根据预测误差曲线离散程度过滤出可能的异常点。一个序列的方差表示该序列离散的程度，方差越大，表明该序列波动越大。如果一个预测误差序列方差比较大，那么我们认为预测误差的报警阈值相对大一些才比较合理。离散度 filter 利用了这一特性，我们对于这个真实数据点的前 $N$ 个点求方差，下一步阈值 filter 的输入为方差 $\sigma$. 阈值Filter根据误差绝对值是否超过某个阈值过滤出可能的异常点。利用离散度 Filter 进行过滤时，报警阈值随着误差序列波动程度变大而变大，但是在输入数据比较小时，误差序列方差比较小，报警阈值也很小，容易出现误报。所以设计了根据方差 $\sigma$ 进行过滤的阈值 filter。阈值 filter 设计了一个分段阈值函数 $y=f(x)$，对于实际值 $x$ 和预测值 $p$ ，只有当 $|x-p|&gt;f(x)$ 时报警。实际使用中，可以根据数据寻找一个对数函数替换分段阈值函数，更易于参数调优。 总结创新性目前少见有人使用预测器 + 判别器的方式结合金融统计方法进行异常检测，这种方式本身比较新颖。主要优势有如下几点： 相对于我们之前采用的机器学习的方法，这种方法即使在大数据的情况下也能够顺利完成，因为根据选取方法的不同，基本都是在 $O(N)$ 级别的方法。 可以实现在线实时预测，可以将成套的算法写入传感器芯片内作为硬件级别的预测，芯片内只需要存储规定 $N$ 日内的数据即可。 不足 在预测器中其实我们也应该考虑ARIMA或者是三阶指数平滑（即Holt-winters）模型，但是对于传感器数据的先验信息我们并不了解，导致我们不能够手动的设置其周期值，对于周期性不明确的情况，我们暂时使用 fix 方法作为补偿。 下一步我计划使用RNN作为预测器，使用LSTM方法，直接对数据进行预测，但是这种方法可能会比现有的方法更耗时，但是有可能会有更高的准确性，而且目前我已经在尝试使用 LSTM 提取数据的周期性，效果还不错。 困难点分析 对数据的先验知识不够，这近似于一个非监督学习，如果要对效率有更高的要求可能需要更多的标记。 对未来这个系统的使用方法，使用对象和项目雏形不是很了解。 目前尚未完成对于数据的周期性提取的高效算法（LSTM相对来说比较耗时，对于大数据可能不太适用）。 目前尚未完成对于模式异常的识别。 研究方向 正在学习使用 RNN 进行序列的预测，到时候会根据正确性以及效率来和现有的方法进行选择以及比较。 正在考虑是否需要将大段的点异常归类为段异常，从而进行模式异常的识别。 在目前我们的实验中，主要使用了两种预测方法+两种修正方法，但实际上时序数据还有许多可以增加的模型和修正方法（我们倾向于使用多修正方法进行参数投票，最终用拟合的方式找出一个最适合我们训练数据的函数），这些方法的最终目的就是能够更加根据历史信息平滑的预测变化量，从而为我们的异常检测提出建议的值。]]></content>
      <categories>
        <category>时间序列</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[17周周报-使用Holt-Winters模型通过比对预测值进行异常检测]]></title>
    <url>%2F2017-12-24%2F17%E5%91%A8%E5%91%A8%E6%8A%A5-%E4%BD%BF%E7%94%A8Holt-Winters%E6%A8%A1%E5%9E%8B%E9%80%9A%E8%BF%87%E6%AF%94%E5%AF%B9%E9%A2%84%E6%B5%8B%E5%80%BC%E8%BF%9B%E8%A1%8C%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[基于预测比较模型的异常检测 16周时，开始尝试一些新的思路与原有方法的对比，在比较了传统的滑动平均和现有的指数平均方法之后，我们采用Holt Winters方法从预测的角度来做误差分析。 新的方法在序列数据的异常检测过程中，我们既可以直接使用对序列进行异常检测的算法，也可以先对序列数据进行特征提取然后转化为传统的离群点检测。 离群点检测方法 方法描述 方法特点 基于统计 大部分的基于统计的离群点检测方法是构建一个概率分布模型，并计算对象符合该模型的概率，把具有低概率的对象视为离群点 基于统计模型的离群点检测方法的前提是必须知道数据集服从什么分布；而对于高维的数据，可能每一维度服从的分布都不太一致，所以通常对高维数据来讲通常效果较差。 基于邻近度 通常可以在数据对象之间定义邻近性度量，把远离大部分点的对象视为离群点。 算法假定离群点是离散的，低维数据我们可以作图观察，而高维数据我们无法观察，所以难以确定有效的参数和全局阈值，效果较差。 基于聚类 一种利用聚类检测离群点的方法是直接丢弃远离其他簇的小簇；另一种是对数据点属于簇的程度进行评价，去除得分较低的点。 聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大，对数据的可分类性要求较高 之前考虑的算法方向主要是在『基于统计』+『基于聚类』的这个方向来考量。 而如今我发现了一种新的方法可以作为采用与尝试，即上图中『基于临近度』，也是一种使用历史数据判断当前数据的方法。 基于预测的异常检测模型如下图所示，$x_t$ 是真实数据，通过预测器得到预测数据，然后 $x_t$ 和 $p_t$ 分别作为比较器的输入，最终得到输出 $y_t$，$y_t$ 是一个二元值，可以用+1（+1表示输入数据正常），-1（-1表示输入数据异常）表示。 如果说我们设置异常检测的模型如此，那么我们可以从两个以下方面入手，一是预测器的优化，二是比较器的优化。 预测器优化同比环比预测器同比环比是比较常用的异常检测方式，它是将当前时刻数据和前一时刻数据（环比）或者前一天同一时刻数据（同比）比较，超过一定阈值即认为该点异常。如果用图模型来表示，那么预测器就可以表示为用当前时刻前一时刻或者前一天同一时刻数据作为当前时刻的预测数据。 如果将不同日期、时刻的监控数据以矩阵方式存储，每一行表示一天内不同时刻的监控数据，每一列表示同一时刻不同日期的监控数据，那么存储矩阵如下图所示： 假如需要预测图中黄色数据，那么环比使用图中的蓝色数据作为预测黄点的源数据，同比使用图中红色数据作为预测黄点的源数据。 基线预测器（MA方法）同比环比使用历史上的单点数据来预测当前数据，误差比较大。$t$ 时刻的监控数据，与$t-1,t-2,…$ 时刻的监控数据存在相关性。同时，与$t-k,t-2k,…$ 时刻的数据也存在相关性（k为周期），如果能利用上这些相关数据对t时刻进行预测，预测结果的误差将会更小。 比较常用的方式是对历史数据求平均，然后过滤噪声，可以得到一个平滑的曲线（基线），使用基线数据来预测当前时刻的数据。该方法预测 $t$ 时刻数据（图中黄色数据）使用到的历史数据如下图所示（图中红色数据）： Holt-Winters预测器同比环比预测到基线数据预测，使用的相关数据变多，预测的效果也较好。但是基线数据预测器只使用了周期相关的历史数据，没有使用上同周期相邻时刻的历史数据，相邻时刻的历史数据对于当前时刻的预测影响是比较大的。对于 Holt-winters 预测期模型，它建议使用黄色点左上方的所有数据。 Holt-Winters是三次指数滑动平均算法，它将时间序列数据分为三部分：残差数据 $a(t)$，趋势性数据 $b(t)$，周期性数据 $s(t)$。使用Holt-Winters预测 $t$ 时刻数据，需要 $t$ 时刻前包含多个周期的历史数据。 详细信息看这里：https://www.otexts.org/fpp/7/5 在实际的异常检测模型中，我们对Holt-Winters预测器进行了简化。预测器的趋势数据表示的是时间序列的总体变化趋势，经过分析，如果以天/小时为周期看待传感器数据的订单量时间序列，是没有明显的趋势性的，如下的分解图也证明了这一点。因此，我们可以去掉其中的趋势数据部分。 各部分迭代的简化计算公式如（其中 $k$ 为周期）： $a[t] = \alpha(Y[t] - s[t-k]) + (1-\alpha) a[t-1]$ $s[t] = \gamma(Y[t] - a[t]) + (1 - \gamma)(s[t-k])$ 预测值：$Y[t+h] = a[t] + s[t-k+1 + (h-1) mod k]$ 为了将算法应用到线上的实时预测，我们可以将 Holt-Winters 算法拆分为两个独立的计算过程： 定时任务计算序列的周期数 $s(t)$。 $S(t)$ 不需要实时计算，只用按照周期性更新即可，使用 Holt-Winters 公式计算出时间序列的周期性数据。 对残差序列做实时预测。 计算出周期数据后，下一个目标就是对残差数据的预测。使用下面的公式，实际监控数据与周期数据相减得到残差数据，对残差数据做一次滑动平均，预测出下一刻的残差，将该时刻的残差、周期数据相加即可得到该时刻的预测数据。对于分钟数据，则将残差序列的长度设为60，即可以得到比较准确的预测效果。 红线为预测数据，蓝线为真实数据 ​ 比较器优化预测器预测出当前时刻传感器的预测值后，还需要与真实值比较来判断当前时刻数据是否异常。一般的比较器都是通过阈值法，比如实际值超过预测值的一定比例就认为该点出现异常，进行报警。这种方式错误率比较大。在传感器数值模型的报警检测中没有使用这种方式，而是使用了两个串联的Filter，只有当两个Fliter都认为该点异常时，才进行报警，下面简单介绍一下两个Filter的实现。 离散度Filter根据预测误差曲线离散程度过滤出可能的异常点。一个序列的方差表示该序列离散的程度，方差越大，表明该序列波动越大。如果一个预测误差序列方差比较大，那么我们认为预测误差的报警阈值相对大一些才比较合理。离散度 Filter 利用了这一特性，取连续 15 分钟的预测误差序列，分为首尾两个序列（e1,e2），如果两个序列的均值差大于 e1 序列方差的某个倍数，我们就认为该点可能是异常点。 阈值Filter根据误差绝对值是否超过某个阈值过滤出可能的异常点。利用离散度 Filter 进行过滤时，报警阈值随着误差序列波动程度变大而变大，但是在输入数据比较小时，误差序列方差比较小，报警阈值也很小，容易出现误报。所以设计了根据误差绝对值进行过滤的阈值 Filter。阈值 Filter 设计了一个分段阈值函数 $y=f(x)$，对于实际值 $x$ 和预测值 $p$ ，只有当 $|x-p|&gt;f(x)$ 时报警。实际使用中，可以寻找一个对数函数替换分段阈值函数，更易于参数调优。 模型最终架构每天定时抽取历史10天数据，经过预处理模块，去除异常数据，经过周期数据计算模块得到周期性数据。对当前时刻预测时，取60分钟的真实数据和周期性数据，经过实时预测模块，预测出当前传感器数值。将连续15分钟的预测值和真实值通过比较器，判断当前时刻是否异常。 参考来源： https://www.jianshu.com/p/6fb0408b3f54 https://www.otexts.org/fpp/7/5]]></content>
      <categories>
        <category>异常检测</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mecari-Analysis(LB=0.4229~rank 17 / 1090)]]></title>
    <url>%2F2017-12-23%2FMecari-Analysis-LB-0-4229-rank-17-1090%2F</url>
    <content type="text"><![CDATA[Mecari其实从情理上来说这个比赛有一点奇怪，因为全凭给出的 feature 似乎并不能很好的去 fit 结果的 price。 所以如果不能从特征工程的角度去挖掘数据的信息的话，只拿已给出的信息扔进 xgboost 或者是 lgbm，似乎就会和大部分人在同一个水平线。 数据处理NLP-RelatedDocument-term matrixA document-term matrix or term-document matrix is a mathematical matrix) that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. There are various schemes for determining the value that each entry in the matrix should take. D1 = “I like databases” D2 = “I hate databases” then the document-term matrix would be: I like hate databases D1 1 1 0 1 D2 1 0 1 1 也可以使用tf-idf schema对其进行计数。 Bags of words model下列文件可用词袋表示: 以下是两个简单的文件: 1(1) John likes to watch movies. Mary likes movies too. 1(2) John also likes to watch football games. 基于以上两个文件，可以建构出下列清单: 123456789101112[ "John", "likes", "to", "watch", "movies", "also", "football", "games", "Mary", "too"] 此处有10个不同的词，使用清单的索引表示长度为10的向量: 1[1, 2, 1, 1, 2, 0, 0, 0, 1, 1] (2) [1, 1, 1, 1, 0, 1, 1, 1, 0, 0] 每个向量的索引内容对应到清单中词出现的次数。 标签二值化LabelBinarizer 是一个用来从多类别列表创建标签矩阵的工具类: 123456789&gt;&gt;&gt; from sklearn import preprocessing&gt;&gt;&gt; lb = preprocessing.LabelBinarizer()&gt;&gt;&gt; lb.fit([1, 2, 6, 4, 2])LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)&gt;&gt;&gt; lb.classes_array([1, 2, 4, 6])&gt;&gt;&gt; lb.transform([1, 6])array([[1, 0, 0, 0], [0, 0, 0, 1]]) 在 mecari 中我们对 brand_name 进行二值化处理，生成了一个item * brand_count 大小的矩阵。 One-hot 编码和 get_dummies离散特征的编码分为两种情况： 离散特征的取值之间没有大小的意义，比如color：[red,blue],那么就使用one-hot编码 离散特征的取值有大小的意义，比如size:[X,XL,XXL],那么就使用数值的映射{X：1,XL：2,XXL：3} 使用pandas可以很方便的对离散型特征进行one-hot编码。 12345678910111213141516import pandas as pd df = pd.DataFrame([ ['green', 'M', 10.1, 'class1'], ['red', 'L', 13.5, 'class2'], ['blue', 'XL', 15.3, 'class1']]) df.columns = ['color', 'size', 'prize', 'class label'] size_mapping = &#123; 'XL': 3, 'L': 2, 'M': 1&#125; df['size'] = df['size'].map(size_mapping) class_mapping = &#123;label:idx for idx,label in enumerate(set(df['class label']))&#125; df['class label'] = df['class label'].map(class_mapping) 对于有大小意义的离散特征，直接使用映射就可以了，{‘XL’:3,’L’:2,’M’:1} Using the get_dummies will create a new column for every unique string in a certain column. 1pd.get_dummies(df) Tf-idfTF：Term Frequency. 单词在文章中出现的频率。 Idf：Inverse Document Frequency. 逆文档频率：为了衡量单词在该文章中的重要程度（在Mecari中我们是衡量单词在这条评论中的重要程度）。 如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。 TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。所以，自动提取关键词的算法就很清楚了，就是计算出文档的每个词的TF-IDF值，然后按降序排列，取排在最前面的几个词。 Boosting 思想 不断的强化模型 The term Boosting refers to a family of algorithms which converts weak learner to strong learners. Ada BoostWhy often use decision tree? Decision trees are non-linear. Boosting with linear models simply doesn’t work well. The weak learner needs to be consistently better than random guessing. You don’t normal need to do any parameter tuning to a decision tree to get that behavior. Training an SVM really does need a parameter search. Since the data is re-weighted on each iteration, you likely need to do another parameter search on each iteration. So you are increasing the amount of work you have to do by a large margin. Decision trees are reasonably fast to train. Since we are going to be building 100s or 1000s of them, thats a good property. They are also fast to classify, which is again important when you need 100s or 1000s to run before you can output your decision. By changing the depth you have a simple and easy control over the bias/variance trade off, knowing that boosting can reduce bias but also significantly reduces variance. Boosting is known to overfit, so the easy nob to tune is helpful in that regard. GBM回归树总体流程类似于分类树，区别在于，回归树的每一个节点都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化平方误差。也就是被预测出错的人数越多，错的越离谱，平方误差就越大，通过最小化平方误差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 Lgbm(Light Gradient Boosting Model)Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm. Below diagrams explain the implementation of LightGBM and other boosting algorithms. Parameters Tunning it is not advisable to use LGBM on small datasets. Light GBM is sensitive to overfitting and can easily overfit small data. Their is no threshold on the number of rows but my experience suggests me to use it only for data with 10,000+ rows. Control Parameters max_depth: It describes the maximum depth of tree. This parameter is used to handle model overfitting. Any time you feel that your model is overfitted, my first advice will be to lower max_depth. min_data_in_leaf: It is the minimum number of the records a leaf may have. The default value is 20, optimum value. It is also used to deal over fitting feature_fraction: Used when your boosting(discussed later) is random forest. 0.8 feature fraction means LightGBM will select 80% of parameters randomly in each iteration for building trees. bagging_fraction: specifies the fraction of data to be used for each iteration and is generally used to speed up the training and avoid overfitting. early_stopping_round: This parameter can help you speed up your analysis. Model will stop training if one metric of one validation data doesn’t improve in last early_stopping_round rounds. This will reduce excessive iterations. lambda: lambda specifies regularization. Typical value ranges from 0 to 1. min_gain_to_split: This parameter will describe the minimum gain to make a split. It can used to control number of useful splits in tree. max_cat_group: When the number of category is large, finding the split point on it is easily over-fitting. So LightGBM merges them into ‘max_cat_group’ groups, and finds the split points on the group boundaries, default:64 Core Parameters Task: It specifies the task you want to perform on data. It may be either train or predict. application: This is the most important parameter and specifies the application of your model, whether it is a regression problem or classification problem. LightGBM will by default consider model as a regression model. regression: for regression binary: for binary classification multiclass: for multiclass classification problem boosting: defines the type of algorithm you want to run, default=gdbt gbdt: traditional Gradient Boosting Decision Tree rf: random forest dart: Dropouts meet Multiple Additive Regression Trees goss: Gradient-based One-Side Sampling num_boost_round: Number of boosting iterations, typically 100+ learning_rate: This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates. Typical values: 0.1, 0.001, 0.003… num_leaves: number of leaves in full tree, default: 31 device: default: cpu, can also pass gpu Metric parameter metric： again one of the important parameter as it specifies loss for model building. Below are few general losses for regression and classification. mae: mean absolute error mse: mean squared error binary_logloss: loss for binary classification multi_logloss: loss for multi classification IO parameter max_bin： it denotes the maximum number of bin that feature value will bucket in. categorical_feature: It denotes the index of categorical features. If categorical_features=0，1，2 then column 0， column 1 and column 2 are categorical variables. ignore_column： same as categorical_features just instead of considering specific columns as categorical, it will completely ignore them. save_binary： If you are really dealing with the memory size of your data file then specify this parameter as ‘True’. Specifying parameter true will save the dataset to binary file, this binary file will speed your data reading time for the next time. Knowing and using above parameters will definitely help you implement the model. Remember I said that implementation of LightGBM is easy but parameter tuning is difficult. So let’s first start with implementation and then I will give idea about the parameter tuning. Ridge线性最小二乘拟合解析解 当XTX的行列式接近于0时，我们将其主对角元素都加上一个数k，可以使矩阵为奇异的风险大降低。于是： B(k)=(XTX+kI)−1XTYB(k)=(XTX+kI)−1XTY (I是单位矩阵) 随着k的增大，B(k)中各元素bi(k)的绝对值均趋于不断变小，它们相对于正确值bi的偏差也越来越大。k趋于无穷大时，B(k)趋于0。b(k)随k的改变而变化的轨迹，就称为岭迹。实际计算中可选非常多的k值，做出一个岭迹图，看看这个图在取哪个值的时候变稳定了，那就确定k值了。 X不满足列满秩，换句话就是说样本向量之间具有高度的相关性（如果每一列是一个向量的话）。遇到列向量相关的情形，岭回归是一种处理方法，也可以用主成分分析PCA来进行降维。 岭回归的原理较为复杂。根据高斯马尔科夫定力，多重相关性并不影响最小二乘法估计量的无偏性和最小方差性，但是，虽然最小二乘估计量在所有线性估计量中是方差最小的，但是这个方差都不一定小，而实际上可以找到一个有偏估计量，这个估计量虽然有较小的偏差，但它的精度却能够大大高于无偏的估计量。岭回归分析就是根据这个原理，通过在正规方程中引入有偏常熟二求的回归估计量的。 辅助函数preprocessing.MinMaxScaler： The MinMaxScaler is the probably the most famous scaling algorithm, and follows the following formula for each feature: xi–min(x)max(x)–min(x) It essentially shrinks the range such that the range is now between 0 and 1 (or -1 to 1 if there are negative values). This scaler works better for cases in which the standard scaler might not work so well. If the distribution is not Gaussian or the standard deviation is very small, the min-max scaler works better. 使用这种方法的目的包括： 对于方差非常小的属性可以增强其稳定性。 维持稀疏矩阵中为0的条目。 Cross Validation在解决实际问题中，我们可以将所有的数据集 dataset ，划分为 train_set（例如70%）和test_set（30%），然后在 train_set 上做 cross_validation ，最后取平均之后，再使用test_set测试模型的准确度。 K-Fold A model is trained using k-1 of the folds as training data; the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy). The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. Grid Search12345678910111213141516171819import pandas as pdfrom sklearn import svm, datasetsfrom sklearn.model_selection import GridSearchCVfrom sklearn.metrics import classification_reportiris = datasets.load_iris()parameters = &#123;'kernel':('linear', 'rbf'), 'C':[1, 2, 4], 'gamma':[0.125, 0.25, 0.5 ,1, 2, 4]&#125;svr = svm.SVC()clf = GridSearchCV(svr, parameters, n_jobs=-1)clf.fit(iris.data, iris.target)cv_result = pd.DataFrame.from_dict(clf.cv_results_)with open('cv_result.csv','w') as f: cv_result.to_csv(f) print('The parameters of the best model are: ')print(clf.best_params_)y_pred = clf.predict(iris.data)print(classification_report(y_true=iris.target, y_pred=y_pred)) Konstantin的建议Sure, let me give some examples: after looking at explained predictions, I see that “t-“ in word “t-shirt” is not highlighted, then I can check how scikit-learn vectorizer processes such words and see that it discards “t-“, so the model sees “shirt” - which may or may not be the problem, but it’s worth checking after looking at the model features, I see that words like “16gb” and “32gb” are really important - I would check, maybe people also write “16 gb” too, and it’s better to normalize such cases to give the model a better job I see “item_description__regimen” as a positive feature, this looks strange - is it a german word and so any german descriptions make the product more expensive? Or something else?]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ng'Cousera-Week 6-Bias vs Variance]]></title>
    <url>%2F2017-12-17%2FNg-Cousera-Week-6-Bias-VS-Variance%2F</url>
    <content type="text"><![CDATA[Diagnosing Bias vs. VarianceIn this section we examine the relationship between the degree of the polynomial d and the underfitting or overfitting of our hypothesis. We need to distinguish whether bias or variance is the problem contributing to bad predictions. High bias is underfitting and high variance is overfitting. Ideally, we need to find a golden mean between these two. The training error will tend to decrease as we increase the degree d of the polynomial. At the same time, the cross validation error will tend to decrease as we increase d up to a point, and then it will increase as d is increased, forming a convex curve. High bias (underfitting): both Jtrain(Θ) and JCV(Θ) will be high. Also, JCV(Θ)≈Jtrain(Θ). High variance (overfitting): Jtrain(Θ) will be low and JCV(Θ) will be much greater than Jtrain(Θ). The is summarized in the figure below: Regularization In the figure above, we see that as $\lambda$ increases, our fit becomes more rigid. On the other hand, as $\lambda$ approaches 0, we tend to over overfit the data. So how do we choose our parameter $\lambda$ to get it ‘just right’ ? In order to choose the model and the regularization term $\lambda$, we need to: Create a list of lambdas (i.e. $\lambda \in {0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24})$; Create a set of models with different degrees or any other variants. Iterate through the λs and for each λ go through all the models to learn some $\theta$ . Compute the cross validation error using the learned $\theta$ (computed with λ) on the $J_{CV}(\theta) $ without regularization or λ = 0. Select the best combo that produces the lowest error on the cross validation set. Using the best combo $\theta$ and λ, apply it on $J_{CV}(\theta) $ to see if it has a good generalization of the problem. DecisionOur decision process can be broken down as follows: Getting more training examples: Fixes high variance Trying smaller sets of features: Fixes high variance Adding features: Fixes high bias Adding polynomial features: Fixes high bias Decreasing λ: Fixes high bias Increasing λ: Fixes high variance. Diagnosing Neural Networks A neural network with fewer parameters is prone to underfitting. It is also computationally cheaper. A large neural network with more parameters is prone to overfitting. It is also computationally expensive. In this case you can use regularization (increase λ) to address the overfitting. Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using your cross validation set. You can then select the one that performs best. 网络越大越好，不要因噎废食怕过拟合就用小网络。上大网络加正则化肯定比小网络好，因为大网络加正则化后的损失函数更容易优化到一个更小的局部极值点，对随机初始化的依赖更小。 Model Complexity Effects: Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently. Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance. In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习-拉格朗日函数与KKT条件]]></title>
    <url>%2F2017-12-07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%87%BD%E6%95%B0%E4%B8%8EKKT%E6%9D%A1%E4%BB%B6%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[天算计划-为什么使用Docker]]></title>
    <url>%2F2017-12-05%2F%E5%A4%A9%E7%AE%97%E8%AE%A1%E5%88%92-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8Docker%2F</url>
    <content type="text"><![CDATA[介绍Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。 常用命令Docker运行Linux上可以原生无压力，Windows上需要装插件虚化一个Linux环境，感觉很麻烦。 在 Docker 下安装镜像 1docker pull uhopper/hadoop-spark 在 Docker 中移除镜像 12docker rm [container_id]docker rmi [image_id] ​]]></content>
      <categories>
        <category>分布式计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[天算计划-虚拟化与云计算]]></title>
    <url>%2F2017-12-05%2F%E5%A4%A9%E7%AE%97%E8%AE%A1%E5%88%92-%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E4%BA%91%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[摘录于知乎https://www.zhihu.com/question/22793847 组合了一些优秀答案 咱们需要实现的也是虚拟化（Docker）+分布式计算（Hadoop） 虚拟化对于主机上的虚拟化技术，其中一个可行的定义就是可以让IT系统的物理拓扑图与逻辑拓扑图无关，即解耦，我们暂时以商用虚拟化系统vmware举例为了实现拓扑解耦，它做的第一点就是让一台机器可以同时跑多个操作系统，即虚拟机，而且虚拟机还可以在物理机间来回转移，高可用，这样我们的操作系统就从物理机上彻底解放出来了，你可以把同一个虚拟机随时放到其他物理机上，实现了对硬件的高效资源利用，和系统的高度灵活，解除了大量人工劳动，便于实现大规模系统的方便管理，这种就是服务器虚拟化 通过虚拟化技术将一台计算机虚拟为多台逻辑计算机。在一台计算机上同时运行多个逻辑计算机，每个逻辑计算机可运行不同的操作系统，并且应用程序都可以在相互独立的空间内运行而互不影响，从而显著提高计算机的工作效率。 虚拟化使用软件的方法重新定义划分IT资源，可以实现IT资源的动态分配、灵活调度、跨域共享，提高IT资源利用率，使IT资源能够真正成为社会基础设施，服务于各行各业中灵活多变的应用需求。 虚拟化是一个广义的术语，是指计算元件在虚拟的基础上而不是真实的基础上运行，是一个为了简化管理，优化资源的解决方案。如同空旷、通透的写字楼，整个楼层没有固定的墙壁，用户可以用同样的成本构建出更加自主适用的办公空间，进而节省成本，发挥空间最大利用率。这种把有限的固定的资源根据不同需求进行重新规划以达到最大利用率的思路，在IT领域就叫做虚拟化技术。 虚拟化技术可以扩大硬件的容量，简化软件的重新配置过程。CPU的虚拟化技术可以单CPU模拟多CPU并行，允许一个平台同时运行多个操作系统，并且应用程序都可以在相互独立的空间内运行而互不影响，从而显著提高计算机的工作效率。 虚拟化技术与多任务以及超线程技术是完全不同的。多任务是指在一个操作系统中多个程序同时并行运行，而在虚拟化技术中，则可以同时运行多个操作系统，而且每一个操作系统中都有多个程序运行，每一个操作系统都运行在一个虚拟的CPU或者是虚拟主机上；而超线程技术只是单CPU模拟双CPU来平衡程序运行性能，这两个模拟出来的CPU是不能分离的，只能协同工作。 云计算云计算 （Cloud Computing）是基于互联网的相关服务的增加、使用和交付模式，通常涉及通过互联网来提供动态易扩展且经常是虚拟化的资源。云是网络、互联网的一种比喻说法。过去在图中往往用云来表示电信网，后来也用来表示互联网和底层基础设施的抽象。因此，云计算甚至可以让你体验每秒10万亿次的运算能力，拥有这么强大的计算能力可以模拟核爆炸、预测气候变化和市场发展趋势。用户通过电脑、笔记本、手机等方式接入数据中心，按自己的需求进行运算。 超大规模 “云”具有相当的规模，Google云计算已经拥有100多万台服务器， Amazon、IBM、微软、Yahoo等的“云”均拥有几十万台服务器。企业私有云一般拥有数百上千台服务器。“云”能赋予用户前所未有的计算能力。 虚拟化 云计算支持用户在任意位置、使用各种终端获取应用服务。所请求的资源来自“云”，而不是固定的有形的实体。应用在“云”中某处运行，但实际上用户无需了解、也不用担心应用运行的具体位置。只需要一台笔记本或者一个手机，就可以通过网络服务来实现我们需要的一切，甚至包括超级计算这样的任务。 高可靠性 “云”使用了数据多副本容错、计算节点同构可互换等措施来保障服务的高可靠性，使用云计算比使用本地计算机可靠。 通用性 云计算不针对特定的应用，在“云”的支撑下可以构造出千变万化的应用，同一个“云”可以同时支撑不同的应用运行。 高可扩展性 “云”的规模可以动态伸缩，满足应用和用户规模增长的需要。 按需服务 “云”是一个庞大的资源池，你按需购买，云可以像自来水，电，煤气那样计费。其还强调一个动态分配的特点，比如双11前后几天，电商部门需要比平时10倍的访问量，那就可以在那几天增加10倍的资源，过完又释放相关资源，而不是以前按最高性能要求来购买硬件，减少浪费，借助虚拟化（但不是必须）可以更加方便实现此目的； 其它特性，如服务自助，服务计量化，包括不同包装特性（IaaS,PaaS,Saas）只是在此基础上的增强派生而已。 极其廉价 由于“云”的特殊容错措施可以采用极其廉价的节点来构成云，“云”的自动化集中式管理使大量企业无需负担日益高昂的数据中心管理成本，“云”的通用性使资源的利用率较之传统系统大幅提升，因此用户可以充分享受“云”的低成本优势，经常只要花费几百美元、几天时间就能完成以前需要数万美元、数月时间才能完成的任务。 云计算可以彻底改变人们未来的生活，但同时也要重视环境问题，这样才能真正为人类进步做贡献,而不是简单的技术提升。 潜在的危险性 云计算服务除了提供计算服务外，还必然提供了存储服务。但是云计算服务当前垄断在私人机构（企业）手中，而他们仅仅能够提供商业信用。对于政府机构、商业机构（特别像银行这样持有敏感数据的商业机构）对于选择云计算服务应保持足够的警惕。一旦商业用户大规模使用私人机构提供的云计算服务，无论其技术优势有多强，都不可避免地让这些私人机构以“数据（信息）”的重要性挟制整个社会。对于信息社会而言，“信息”是至关重要的。另一方面，云计算中的数据对于数据所有者以外的其他用户云计算用户是保密的，但是对于提供云计算的商业机构而言确实毫无秘密可言。所有这些潜在的危险，是商业机构和政府机构选择云计算服务、特别是国外机构提供的云计算服务时，不得不考虑的一个重要的前提。]]></content>
      <categories>
        <category>分布式计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux：一些常用的Linux命令]]></title>
    <url>%2F2017-12-04%2FLinux%EF%BC%9A%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%20%2F</url>
    <content type="text"><![CDATA[CP操作格式：cp [选项]... 源… 目录 命令作用：将源文件复制至目标文件，或将多个源文件复制至目标目录。 命令参数： 1234567891011121314151617-a, --archive 等于-dR --preserve=all --backup[=CONTROL 为每个已存在的目标文件创建备份-b 类似--backup 但不接受参数 --copy-contents 在递归处理是复制特殊文件内容-d 等于--no-dereference --preserve=links-f, --force 如果目标文件无法打开则将其移除并重试(当 -n 选项 存在时则不需再选此项)-i, --interactive 覆盖前询问(使前面的 -n 选项失效)-H 跟随源文件中的命令行符号链接-l, --link 链接文件而不复制-L, --dereference 总是跟随符号链接-n, --no-clobber 不要覆盖已存在的文件(使前面的 -i 选项失效)-P, --no-dereference 不跟随源文件中的符号链接-p 等于--preserve=模式,所有权,时间戳 --preserve[=属性列表 保持指定的属性(默认：模式,所有权,时间戳)，如果 可能保持附加属性：环境、链接、xattr 等-R, -r, --recursive 复制目录及目录内的所有项目]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[天算计划-为什么Based on Spark?]]></title>
    <url>%2F2017-12-04%2F%E5%A4%A9%E7%AE%97%E8%AE%A1%E5%88%92-%E4%B8%BA%E4%BB%80%E4%B9%88Based-on-Spark%2F</url>
    <content type="text"><![CDATA[Spark简介Spark最初诞生于美国加州大学伯克利分校（UC Berkeley）的AMP实验室，是一个可应用于大规模数据处理的快速、通用引擎。2013年，Spark加入Apache孵化器项目后，开始获得迅猛的发展，如今已成为Apache软件基金会最重要的三大分布式计算系统开源项目之一（即Hadoop、Spark、Storm）。Spark最初的设计目标是使数据分析更快——不仅运行速度快，也要能快速、容易地编写程序。为了使程序运行更快，Spark提供了内存计算，减少了迭代计算时的IO开销；而为了使编写程序更为容易，Spark使用简练、优雅的Scala语言编写，基于Scala提供了交互式的编程体验。虽然，Hadoop已成为大数据的事实标准，但其MapReduce分布式计算模型仍存在诸多缺陷，而Spark不仅具备Hadoop MapReduce所具有的优点，且解决了Hadoop MapReduce的缺陷。Spark正以其结构一体化、功能多元化的优势逐渐成为当今大数据领域最热门的大数据计算平台。 Spark支持使用Scala、Java、Python和R语言进行编程。由于Spark采用Scala语言进行开发，因此，建议采用Scala语言进行Spark应用程序的编写。Scala是一门现代的多范式编程语言，平滑地集成了面向对象和函数式语言的特性，旨在以简练、优雅的方式来表达常用编程模式。Scala语言的名称来自于“可伸展的语言”，从写个小脚本到建立个大系统的编程任务均可胜任。Scala运行于Java平台（JVM，Java 虚拟机）上，并兼容现有的Java程序。 Spark模式区分在Spark中存在着多种运行模式，可使用本地模式运行、可使用伪分布式模式运行、使用分布式模式也存在多种模式如：Spark Mesos模式、Spark YARN模式。 Spark Mesos模式：官方推荐模式，通用集群管理，有两种调度模式：粗粒度模式（Coarse-grained Mode）与细粒度模式（Fine-grained Mode）。 Spark YARN模式：Hadoop YARN资源管理模式。 Standalone模式： 简单模式或称独立模式，可以单独部署到一个集群中，无依赖任何其他资源管理系统。不使用其他调度工具时会存在单点故障，使用Zookeeper等可以解决 （我们之前配置的是standalone模式） Local模式：本地模式，可以启动本地一个线程来运行job，可以启动N个线程或者使用系统所有核运行job。 RDD介绍Spark的核心是建立在统一的抽象RDD之上，使得Spark的各个组件可以无缝进行集成，在同一个应用程序中完成大数据计算任务。RDD的设计理念源自AMP实验室发表的论文《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》。 RDD设计背景在实际应用中，存在许多迭代式算法（比如机器学习、图算法等）和交互式数据挖掘工具，目前的MapReduce框架都是把中间结果写入到HDFS中，带来了大量的数据复制、磁盘IO和序列化开销。虽然，类似Pregel等图计算框架也是将结果保存在内存当中，但是，这些框架只能支持一些特定的计算模式，并没有提供一种通用的数据抽象。RDD就是为了满足这种需求而出现的，它提供了一个抽象的数据架构，我们不必担心底层数据的分布式特性，只需将具体的应用逻辑表达为一系列转换处理，不同RDD之间的转换操作形成依赖关系，可以实现管道化，从而避免了中间结果的存储，大大降低了数据复制、磁盘IO和序列化开销。 迭代式算法：下次计算依赖于上次结果，PageRank，单源最短路径是常常见的迭代式算法之一。 交互式数据挖掘：数据库+可视化+数据挖掘算法 RDD 概念一个RDD就是一个分布式对象集合，本质上是一个只读的分区记录集合，每个RDD可以分成多个分区，每个分区就是一个数据集片段，并且一个RDD的不同分区可以被保存到集群中不同的节点上，从而可以在集群中的不同节点上进行并行计算。RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，不能直接修改，只能基于稳定的物理存储中的数据集来创建RDD，或者通过在其他RDD上执行确定的转换操作（如map、join和groupBy）而创建得到新的RDD。RDD提供了一组丰富的操作以支持常见的数据运算，分为“行动”（Action）和“转换”（Transformation）两种类型，前者用于执行计算并指定输出的形式，后者指定RDD之间的相互依赖关系。两类操作的主要区别是，转换操作（比如map、filter、groupBy、join等）接受RDD并返回RDD，而行动操作（比如count、collect等）接受RDD但是返回非RDD（即输出一个值或结果）。RDD提供的转换接口都非常简单，都是类似map、filter、groupBy、join等粗粒度的数据转换操作，而不是针对某个数据项的细粒度修改。因此，RDD比较适合对于数据集中元素执行相同操作的批处理式应用，而不适合用于需要异步、细粒度状态的应用，比如Web应用系统、增量式的网页爬虫等。正因为这样，这种粗粒度转换接口设计，会使人直觉上认为RDD的功能很受限、不够强大。但是，实际上RDD已经被实践证明可以很好地应用于许多并行计算应用中，可以具备很多现有计算框架（比如MapReduce、SQL、Pregel等）的表达能力，并且可以应用于这些框架处理不了的交互式数据挖掘应用。Spark用Scala语言实现了RDD的API，程序员可以通过调用API实现对RDD的各种操作。RDD典型的执行过程如下： RDD读入外部数据源（或者内存中的集合）进行创建； RDD经过一系列的“转换”操作，每一次都会产生不同的RDD，供给下一个“转换”使用； 最后一个RDD经“行动”操作进行处理，并输出到外部数据源（或者变成Scala集合或标量）。需要说明的是，RDD采用了惰性调用，即在RDD的执行过程中（如图9-8所示），真正的计算发生在RDD的“行动”操作，对于“行动”之前的所有“转换”操作，Spark只是记录下“转换”操作应用的一些基础数据集以及RDD生成的轨迹，即相互之间的依赖关系，而不会触发真正的计算。 从输入中逻辑上生成A和C两个RDD，经过一系列“转换”操作，逻辑上生成了F（也是一个RDD），之所以说是逻辑上，是因为这时候计算并没有发生，Spark只是记录了RDD之间的生成和依赖关系。当F要进行输出时，也就是当F进行“行动”操作的时候，Spark才会根据RDD的依赖关系生成DAG，并从起点开始真正的计算。]]></content>
      <categories>
        <category>分布式计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[天算计划-Spark-计算任务分发流程]]></title>
    <url>%2F2017-12-04%2F%E5%A4%A9%E7%AE%97%E8%AE%A1%E5%88%92-Spark-%E8%AE%A1%E7%AE%97%E4%BB%BB%E5%8A%A1%E5%88%86%E5%8F%91%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Spark-Shell操作加载本地文件 在开始具体词频统计代码之前，需要解决一个问题，就是如何加载文件？ 要注意，文件可能位于本地文件系统中，也有可能存放在分布式文件系统HDFS中，所以，下面我们分别介绍如何加载本地文件，以及如何加载HDFS中的文件。 12val textFile = sc.textFile("file:///usr/local/spark/mycode/wordcount/word123.txt")//如果文件不存在会在执行诸如 textFile.first()指令时拒绝连接 加载HDFS中的文件 为了能够读取 HDFS 中的文件，我们需要先启动 Hadoop 中的 HDFS 组件。 1start-all 请使用下面命令创建用户名 hadoop 登录 Linux 系统。 1./bin/hdfs dfs -mkdir -p /user/hadoop 下面我们使用命令查看一下HDFS文件系统中的目录和文件： 123./bin/hdfs dfs -ls ../bin/hdfs dfs -ls /user/hadoop//这两个命令等价 上面命令中，最后一个点号“.”，表示要查看Linux当前登录用户hadoop在HDFS文件系统中与hadoop对应的目录下的文件，也就是查看HDFS文件系统中“/user/hadoop/”目录下的文件 如果要查看HDFS文件系统根目录下的内容，需要使用下面命令： 1./bin/hdfs dfs -ls / 下面我们使用dfs -put指令将文件上传到hdfs文件系统中 1./bin/hdfs dfs -put &lt;path&gt; . 使用-cat指令查看文件内容 1./bin/hdfs dfs -cat ./&lt;filename&gt; 现在，让我们切换回到spark-shell窗口，编写语句从HDFS中加载word.txt文件，并显示第一行文本内容： 12scala&gt; val textFile = sc.textFile("hdfs://localhost:9000/user/hadoop/&lt;filename&gt;")scala&gt; textFile.first() 如下三条语句也等价 123val textFile = sc.textFile("hdfs://localhost:9000/user/hadoop/word.txt")val textFile = sc.textFile("/user/hadoop/word.txt")val textFile = sc.textFile("word.txt")]]></content>
      <categories>
        <category>分布式计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kaggle实战-Mecari价格预测]]></title>
    <url>%2F2017-12-03%2FKaggle%E5%AE%9E%E6%88%98-Mecari%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[先从分析别人的visualization kernel入手。 OverviewThe training data has 1482535 observations with 7 features. The test data has 693359 rows that we need to predict. According to the competition description, the public leaderboard will be evaluated by ALL of the test data we have at the first stage. We can have a a rough look at the features (test data summary is hidden for simplicity) train_id / test_id: A unique key for each item. name: The item’s name as a string. item_condition_id: A factor with 5 levels. As the plot below shows, the mean prices for different conditions are really close and it’s hard to guess which whether higher / lower condition id is better so far. category_name: The category of the item. brand_name: The brand name of the item. Nearly half of the items do not have a brand. shipping: A binary indicator of the shipping information. (1 if shipping fee is paid by seller and 0 by buyer) item_description: A long string containing the raw text of the item description. ~5% of the items do not have a description. ShippingThe shipping cost burden is decently splitted between sellers and buyers with more than half of the items’ shipping fees are paid by the sellers (55%). In addition, the average price paid by users who have to pay for shipping fees is lower than those that don’t require additional shipping cost. This matches with our perception that the sellers need a lower price to compensate for the additional shipping. 普遍来说：价格低的不怎么需要付邮费，而价格高的需要支付更多的邮费。 1230 0.5527261 0.447274Name: shipping, dtype: float64 对于物品的状况item_condition来说，数值和价格关联不大，目前还不知道具体的作用是什么，处理的时候先扔掉（估计对于非常相似的物品，比较下它的状态或许能够给我们一些启发）。 ID = 5时看起来四分位数和中位数都比较高。 Tokenlize123456789101112131415161718192021stop = set(stopwords.words('english'))def tokenize(text): """ sent_tokenize(): segment text into sentences word_tokenize(): break sentences into words """ try: regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\r\\t\\n]') text = regex.sub(" ", text) # remove punctuation tokens_ = [word_tokenize(s) for s in sent_tokenize(text)] tokens = [] for token_by_sent in tokens_: tokens += token_by_sent tokens = list(filter(lambda t: t.lower() not in stop, tokens)) filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)] filtered_tokens = [w.lower() for w in filtered_tokens if len(w)&gt;=3] return filtered_tokens except TypeError as e: print(text,e) TF-IDFtf-idf（英语：term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。tf-idf加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了tf-idf以外，互联网上的搜索引擎还会使用基于链接分析的评级方法，以确定文件在搜索结果中出现的顺序。 有很多不同的数学公式可以用来计算tf-idf。这边的例子以上述的数学公式来计算。词频（tf）是一词语出现的次数除以该文件的总词语数。假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是3/100=0.03。而计算文件频率（DF）的方法是以文件集的文件总数，除以出现“母牛”一词的文件数。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是log（10,000,000 / 1,000）=4。最后的tf-idf的分数为0.03 * 4=0.12。 tf-idf算法是创建在这样一个假设之上的：对区别文档最有意义的词语应该是那些在文档中出现频率高，而在整个文档集合的其他文档中出现频率少的词语，所以如果特征空间坐标系取tf词频作为测度，就可以体现同类文本的特点。另外考虑到单词区别不同类别的能力，tf-idf法认为一个单词出现的文本频数越小，它区别不同类别文本的能力就越大。因此引入了逆文本频度idf的概念，以tf和idf的乘积作为特征空间坐标系的取值测度，并用它完成对权值tf的调整，调整权值的目的在于突出重要单词，抑制次要单词。但是在本质上idf是一种试图抑制噪声的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用，显然这并不是完全正确的。idf的简单结构并不能有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能，所以tf-idf法的精度并不是很高。 此外，在tf-idf算法中并没有体现出单词的位置信息，对于Web文档而言，权重的计算方法应该体现出HTML的结构特征。特征词在不同的标记符中对文章内容的反映程度不同，其权重的计算方法也应不同。因此应该对于处于网页不同位置的特征词分别赋予不同的系数，然后乘以特征词的词频，以提高文本表示的效果。 t-SNE因为原理不同，导致，tsne 保留下的属性信息，更具代表性，也即最能体现样本间的差异，且TSNE 运行极慢，PCA 则相对较快。 因此更为一般的处理，尤其在展示（可视化）高维数据时，常常先用 PCA 进行降维，再使用 tsne。 12data_pca = PCA(n_components=50).fit_transform(data)data_pca_tsne = TSNE(n_components=2).fit_transform(data_pca)]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Material主题中Mathjax的配置]]></title>
    <url>%2F2017-12-01%2FHexo-Material%E4%B8%BB%E9%A2%98%E4%B8%ADMathjax%E7%9A%84%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[最近Hexo无故始终出渲染的问题，删掉重建的过程中发现是因为操作系统-内存那节因为有些是从PPT里拷出来的，导致了无法识别符号，渲染失败。 在重建的过程中，最头疼的就是 Mathjax 的配置了，实现这个效果的插件似乎有很多个版本，有些也有时效性，而且挺多插件，针对于我当前使用的Material主题并无效果，如果使用最出名的Next主题似乎就要好很多。 下面简单记录下配置Mathjax的过程。 修改默认的renderhexo 默认的渲染引擎是 marked，但是 marked 不支持 mathjax。 kramed 是在 marked的基础上进行修改。我们在工程目录下执行以下命令来安装 kramed。 1234npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --savenpm uninstall hexo-math --savenpm install hexo-renderer-mathjax --save //这个使用来代替hexo-math的 执行了这两个操作这个，不用去_config.yml文件中改什么plugin之类的东西（这些是之前使用hexo-math才需要的操作）。 修改kramed的渲染细节这一步不做其实也可以，但是会出现一些转义的bug. 接下来在 /node_modules/hexo-renderer-kramed/lib/renderer.js，修改 12345678910// Change inline math rulefunction formatText(text) &#123; // Fit kramed's rule: $$ + \1 + $$ return text.replace(/`\$(.*?)\$`/g, '$$$$$1$$$$');&#125;// 将上面的修改为如下的// Change inline math rulefunction formatText(text) &#123; return text;&#125; 到&lt;path-to-your-project/node_modules/kramed/lib/rules/inline.js，将11行的 123escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/修改为escape: /^\\([`*\[\]()# +\-.!_&gt;])/, 将20行的： 123em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,修改为em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, ​ 配置Material主题的Mathjax加载的CDN链接： 在material/_config.yml下添加如下的链接 123456789vendors:# MaterialCDN# You can load theme unique files from your private cdn or oss.# The new src will have the base domain you configured below.# For example# materialcdn: https://cdn.jsdelivr.net/npm/hexo-material@1.4.0/source materialcdn: https://cdn.jsdelivr.net/npm/hexo-material@1.4.0/source # MathJax 2.7.0-2.7.1 mathjax: https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js ​]]></content>
      <categories>
        <category>其余</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[随想-同谋（Complicit）]]></title>
    <url>%2F2017-12-01%2F%E9%9A%8F%E6%83%B3-%E5%90%8C%E8%B0%8B%EF%BC%88Complicit%EF%BC%89%2F</url>
    <content type="text"><![CDATA[2017年度词汇 —— 同谋 “ 2017 年度词汇：同谋（Complicit）” —— dictionary.com 这个词的来源是： 4月5日，美国总统特朗普的女儿伊万卡· 特朗普接受《CBS 今晨秀》采访时，被问到她与丈夫是否是特朗普总统的同谋，她回答说 『我不知道同谋是什么意思』。 10月24日，参议员杰夫·弗莱克宣布辞职，并表示『我不会成为共谋』。 Dictionary.com 在公布年度词汇时表示 我们所选择的年度词汇既指那些看得见的行为，也指看些看不见的行为。这个词汇提醒我们，即使不作为也是一种行动。沉默地接受了错误的行为，是我们落到这般境地的原因。我们不能让这种情况继续变成一种常态。如果我们这样做，我们都是共谋。 用一种更通俗的话来讲，或许它在表达：人们越来越沉浸在社交网络的娱乐氛围当中，缺少对时事的严肃讨论和关切，人们倾向于看到事情更让滑稽，更戏剧性，或者说更愿意让自己相信的那一面，而沉默或者是不理智的发声，有时候也是错误的帮凶。]]></content>
      <categories>
        <category>随想</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[天算计划-Ubuntu-Spark组网计划配置操作]]></title>
    <url>%2F2017-11-30%2F%E5%A4%A9%E7%AE%97%E8%AE%A1%E5%88%92-Ubuntu-Spark%E7%BB%84%E7%BD%91%E8%AE%A1%E5%88%92%E9%85%8D%E7%BD%AE%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[记录下组内环境的配置流程，目前我们需要实现五个人在Ubuntu环境下的Hadoop+Spark配置，配置流程如下。 SSH无密码互联 使用ssh-keygen -t rsa -P &quot;&quot; 命令生成公钥，表示生成的公钥不含登录密码，只要A机器拿到B的公钥就可以实现 A 机器无密码登录 B 机器。 cd $HOME/.ssh目录下，使用cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys 命令或者直接将id_rsa.pub中的最后一行复制到authorized_keys当中。 最终保证五个人的电脑的中都有一份相同authorized_keys文件是最好的，这样就可以实现无密码登陆了（第一次可能需要输密码，这里建议咱们先都设置成12345678，方便实验）。 两个人之间测试能否通过当前的内网IP互相登录，如elrond@master这种格式，使用ssh &#39;inet IP&#39;@master登录 elrond 的机器。 错误收集 如果遇到sign_and_send_pubkey: signing failed: agent refused operation的错误，原因是因为ssh-agent并没有真正的工作，输入下列命令： 12eval `ssh-agent -s` ssh-add ​ 统一化HOSTNAME 对于不同的电脑有可能有不同的用户名，我们这里针对于hadoop统一设置一下我们每个人的用户名。 为了更好的在Shell中区分三台主机，修改其显示的主机名，执行如下命令 1sudo vim /etc/hostname master的/etc/hostname添加如下配置： 1master 同样slave01的/etc/hostname添加如下配置： 1slave01 同样slave02的/etc/hostname添加如下配置： 1slave02 重启三台电脑，重启后在终端Shell中才会看到机器名的变化,如下图： 到/etc/hosts路径下修改ip - 缩写，如当前阿臻是master用户，那么我在hosts下面加入了172.20.0.39 master这行语句之后，我再使用ping master实际上是ping 172.20.0.39 。 因此我们最好把hosts文件修改为如下的形式： 使用 1sudo vim /etc/hosts 配置如下： 1234127.0.0.1 localhost172.20.0.39 master172.20.0.38 slave01172.20.0.37 slave02 然后master需要修改~/.ssh/config文件，如果没有此文件，自己创建文件。 123456Host master user elrond // 阿臻的电脑Host slave01 user hadoop // 丹丹的电脑Host slave02 user yuhongzhong // 宇宏的电脑 注：这里的user和host对应的是命令行下的elrond@master，等于在master主机下的elrond用户。 这时我们尝试下面操作，应该就能登录到丹丹的电脑上了。 1ssh slave01 Hadoop配置 修改master主机修改Hadoop如下配置文件，这些配置文件都位于/usr/local/hadoop/etc/hadoop目录下。 slaves这里把DataNode的主机名写入该文件，每行一个。这里让master节点主机仅作为NameNode使用。 12slave01slave02 core-site.xml 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; //注：这里只能是奇数，代表当前允许的datanode个数，即slaver个数。 &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml(复制mapred-site.xml.template,再修改文件名) 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 1234567891011&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 此时阿臻的电脑上 /usr/local 文件夹下面有一个hadoop.tar文件，我们使用scp指令将其传送到其他的slave结点上。 如传送到宇宏的电脑，我们使用：scp hadoop.tar slave02:/usr/local，传送之前要保证宇宏执行了了sudo chmod 777 /usr/local/。 传送完毕文件之后（丹丹和宇宏的电脑都已经有了），解压，在 slave01，slave02 节点上执行 sudo chown -R hadoop /usr/local/hadoop，以保证 master 有权限去修改 slaver 的这个目录。 在 master 主机上执行如下命令： 123cd /usr/local/hadoopbin/hdfs namenode -formatsbin/start-dfs.sh 运行过程中理论上不应该输入密码，如果要输入密码，重回 SSH 的步骤，检查 SSH 是否能够相互无密码登录。 理论情形如下： 我们通过浏览器来查看整个集群的HDFS状态，地址为：http://172.20.0.39（master的内网IP）:50070/dfshealth.html#tab-overview]]></content>
      <categories>
        <category>分布式计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[11-29-异常检测周报]]></title>
    <url>%2F2017-11-29%2F11-29-%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E5%91%A8%E6%8A%A5%2F</url>
    <content type="text"><![CDATA[这周因为机器学习的考试和软工课的一些其他事情。 我还没有进行我的计划：对于DBSCAN针对于时间序列数据的代码级别改进，但是从思路上考虑到了一些方向，下面主要结合这段时间对于这个问题的思考以及一切实验来进行汇报。 数据平滑之后的结果首先我考虑对数据集进行平滑处理，目前能够使用的是如窗口移动平均这类做法，这类做法的时间复杂度为 $O(N)$ ，常数较大，但是复杂度在可以接受的返回值内。 平滑的目的是将工业中因为传感器误差而引入的背景噪声消除，在平滑了曲线后我再使用差分的方法找出局部的最大最小值，效果更为理想（注意在演示中因为使用的数据太大，太密，导致了我们没有办法很好的细粒度的展示） 平滑的含义如下： 在平滑之后的数据，使用差分标记可能的异常值（红点里标记的为局部的最大最小值，我们通过让用户设置阈值，规定差距大于 $\epsilon$ 时，我们检测这个异常值，可以实现针对不同类型的传感器数据报出异常）。 对差分之后的结果进行 K-means和DBSCAN聚类的结果分别如下。 K-MEANS的效果： 但是很可惜的是，K-MEANS的效果是基于空间的聚类，经过更大规模数据的观察，显然它是更多的在根据Y轴坐标进行聚类的划分。 以下是DBSCAN的效果，DBSCAN能够识别出数据大的模式的转化，下一步我打算在红点处加重权值 minEps，希望能够成功的使用DBSCAN进行模式的划分。 MRDBSCAN最近一段时间，我接触到了分布式计算，并且在尝试使用Spark完成一些分布式计算的任务。我也了解到了一个比较前沿的分布式使用DBSCAN的算法：MRDBSCAN。 下面对这个算法做一些简单的记录以及报告： 第一步将平面划分为等点数的几个子平面。 将每个子平面交给一个Worknode去进行DBSCAN的划分。 收集每个子结点计算的结果，考虑子平面边界的情况，对于边界的点，能够向哪边传播，就属于哪边的簇，如果两边都能够同时传播，那么它就属于两边的簇，这时说明两边的簇能够连接到一起去，这时将两边修改为同样的颜色。 下一步方向规划目前我的方法是基于差分-&gt;DBSCAN-&gt;LSTM进行的组合方法的异常点+异常模式段的识别。 下一步我打算分为以下两个阶段进行： 去广泛的使用其他的算法进行异常点（和差分比较）和异常模式（和DBSCAN+LSTM比较）识别检测的效果比较。 将差分找异常点的算法打成包或者写成函数，向外提供调用的接口。 尝试针对于时序数据特性改进DBSCAN算法，并且尝试实现此函数。 一些疑惑 能够实现同样效果的有很多其余的算法，浩哥也在研究其他的一些算法，但是有一些算法时间空间效果不高，对于工业大数据要求的尽可能 $O(N)$ 级别的处理速度，这些算法还有其研究的意义吗？我们有没有必要去广泛的收集各类算法进行比较，测试？即使它看起来很慢而且效果不见得会很好。 目前我们的数据感觉存在低质量的情况，很多时候做不出和理论相近的效果，这方面可能会牺牲一些，需要用户自己多设置一些参数，去调整最后的效果。 最后用户希望从我们的系统检测出来的异常值获取什么信息，能够对工业上有什么帮助呢？]]></content>
      <categories>
        <category>异常检测</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习-第四章-SVM（Updating）]]></title>
    <url>%2F2017-11-26%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E5%9B%9B%E7%AB%A0-SVM%2F</url>
    <content type="text"><![CDATA[主要梳理以下三个方向：支持向量机，核函数，序列最小最优化算法 支持向量机支持向量学习方法包含构建线性可分支持向量机（Linear support vector machine in linearly separable case）、线性支持向量机（Linear support vector machine）及非线性支持向量机（non-linear support vector machine）。 在提到SVM以及逻辑回归，有如下几种情况。 当我们使用线性可分的数据集时，通过硬间隔最大化（hard margin maximization），学习一个线性的分类器，即线性可分支持向量机。 当训练数据近似可分，通过软间隔最大化（soft margin maximization），也学习一个线性分类器。 再则，当训练数据线性不可分时，通过引入核技巧（kernel trick）及软间隔最大化，学习非线性支持向量机。 定义假设在给定的特征空间训练集如下：$T = \{(x_1,y_1),…(x_n,y_n)\}$，其中，$x_i \in \chi= \Re^n, y_i \in \{1,-1\}$ 假设训练数据集是线性可分的，我们需要找到一个分离的超平面 $w \cdot x + b$ 能够将不同的实例分成两部分，我们利用间隔最大化求最优的分离超平面，这时，解是唯一的。 函数间隔和几何间隔函数间隔： 定义给定的数据集 $T$ 和超平面 $（w，b）$. 定义超平面 $(w,b)$ 和样本点 $(x_i,y_i)$ 的函数间隔为：$\hat{\gamma} = y_i(w \dot x_i + b)$. 定义超平面 $(w,b)$ 和数据集 $T$ 的函数间隔为：$\hat{\gamma} = min_{1…n}(\hat{\gamma}_i)$. 几何间隔 由于函数间隔在成倍数变化时，虽然超平面不变动，但是数值却在发生变化，因此我们继续定义几何间隔来对函数间隔进行规范化。 几何间隔：$\gamma_i = \frac{w}{\mid \mid w \mid \mid} \cdot x_i + \frac{b}{\mid \mid w \mid \mid}$ 为什么要间隔最大化 对于训练数据及找到几何间隔最大化的超平面意味着以充分大的确信度（我们可以定义离超平面越远的点对于分类面的确信度越大）对训练数据进行分类，这样的分类结果对于最难分类的点也有足够大的确信度将其分开，这样的超平面应该对未知的新实例有更好的泛化的效果。 支持向量和间隔边界 在线性可分的情况下，训练数据的样本点与分离超平面距离最近的样本点的实例称为支持向量（Support Vector）.支持向量是使约束条件式子等号成立的点，即 $y_i(w \cdot x_i + b) - 1=0$ 对于 $y_i = +1$ 的正例点，支持向量在超平面：$H_1:w \cdot x + b = 1$ 上。 对于 $y_i = -1$ 的负例点，支持向量在超平面：$H_1:w \cdot x + b = -1$ 上。 在决定分离平面时只有支持向量起作用，支持向量两侧的其余点并不起作用，所以我们将这种分类模型称为支持向量机，支持向量的个数一般很少，所以支持向量机由很少的『重要的』训练样本决定。 拉格朗日对偶 通过求解对偶问题的解来获取原问题的最优解，被称为Linear SVM的对偶算法，这样做的优点在于，一是，对偶问题往往更容易求解，二是，引入核函数，进而推广到非线性分类问题。 定义拉格朗日函数：$L(w,b,\alpha) = \frac{1}{2} \mid \mid w \mid \mid^2 - \Sigma^N_{i} \alpha_i y_i(w \cdot x_i + b) + \Sigma^N_{i} \alpha_i$. 对于支持向量需要尽量满足 $y_i(w \cdot x_i + b) - 1 \geq0$，根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：$\max \limits_{\alpha} \cdot \min \limits_{w,b} \{L(w,b,a)\}$. 核技巧当输入空间为欧式空间或者离散集合，特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积，通过核函数可以学习到非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机，这样的方法称为核技巧。 概述的说，线性SVM+核技巧 = 非线性SVM。 希尔伯特空间：抽象空间中的极限与实数上的极限有一个很大的不同就是，极限点可能不在原来给定的集合中，所以又引入了完备的概念，完备的内积空间就称为Hilbert空间。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习复习-第三章-逻辑回归（Updating）]]></title>
    <url>%2F2017-11-26%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%89%E7%AB%A0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习复习-第二章-概率]]></title>
    <url>%2F2017-11-25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%A6%82%E7%8E%87%2F</url>
    <content type="text"><![CDATA[几个公式 贝叶斯公式：$P(B \mid A) = \frac{P(B) \cdot P(A \mid B)}{P(A)}$ 全概率公式：$P(A) = \Sigma_{n} P(A \mid B_n) \cdot P(B_n)$. 生成模型与判别模型监督学习的任务就是学习一个模型，应用这一模型，对给定的输入预测相应的输出，这个模型的一般形式为决策函数：$Y = f(x)$，或者条件概率分布：$P(Y \mid X)$ 监督学习方法又可以分为生成方法（Generative Approach）和判别方法（Discriminative Approach），所学习到的模型分别称为生成模型和判别模型。 生成方法生成方法由数据学习联合概率分布 $P(X,Y)$，然后求出条件概率分布 $P(Y \mid X)$ 作为预测的模型. 即生成模型：$P(Y \mid X) = \frac{P(X,Y)}{P(X)}$ 这样的方法之所以称为生成方法，是因为模型表示了给定输入 $X$ 产生输出 $Y$ 的生成关系，典型的生成模型有：朴素贝叶斯方法和隐马尔科夫模型。 判别方法判别方法由数据直接学习决策函数 $f(X)$ 或者条件概率分布 $P(Y \mid X)$ 作为预测的模型，即判别模型。判别方法关心的是对给定的输入 $X$，应该预测什么样的输出 $Y$。典型的判别模型包括：$k$ 近邻法，感知机，决策树，逻辑斯蒂回归模型，最大熵模型，支持向量机，提升方法和条件随机场等。 评价分类指标准确率（accuracy） 精确率（precision） 召回率（recall） 朴素贝叶斯医生对病人进行诊断就是一个典型的分类过程，任何一个医生都无法直接看到病人的病情，只能观察病人表现出的症状和各种化验检测数据来推断病情，这时医生就好比一个分类器，而这个医生诊断的准确率，与他当初受到的教育方式（构造方法）、病人的症状是否突出（待分类数据的特性）以及医生的经验多少（训练样本数量）都有密切关系。 回忆朴素贝叶斯中的公式 $P(Y = y_k \mid X_1,X_2,\ldots X_n) = \frac{P(Y = y_k)\prod_i P(X_i \mid Y = y_k)}{\Sigma_j ( P(Y = y_j)\prod_i P(X_i \mid Y = y_j))}$ 对比与医生看病，就是医生利用他已知的发病的征兆，结合病人的征兆去获取这个 $\prod_i P(X_i \mid Y = y_k)$，然后再结合发病的概率，去推断病人现在身上的征兆，有多大的可能发病。 朴素贝叶斯学习和分类器与其他相比可以非常快。在条件独立的假设下，类条件特征分布的解耦意味着 每个分布可以独立估计为一个一维分布，这反过来又有助于缓解维灾难问题。 高斯朴素贝叶斯的训练过程 训练：对于每个 $y_k$： 估计 $\pi_k (即P(Y = y_k))$ 对于 $X$ 的每一个属性 $X_i$，估计类条件均值和方差 $\mu_{i,k},\sigma_{i,k}$. 分类： $Y_{new} \leftarrow argmax_{y_k} \prod_i {P(X^{new}_i \mid Y = y_k)}$ $Y_{new} \leftarrow argmax_{y_k} \pi_k \prod_i N(X^{new}_i,\mu_{i,k},\sigma_{i,k}) $ 对于均值和方差的计算如下： 一些问题 怎么对两组数据进行独立性检验？ 皮尔森卡方检验 标准的朴素贝叶斯分类器的决策面看起来是什么样的？ 对于高斯分布的话，如果两个类的协方差相同，决策面是线性的超平面]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习复习-第一章-决策树（Updating）]]></title>
    <url>%2F2017-11-24%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[在这里我记录下在学习决策树之后，我自己的思考以及重点，不会包含太多介绍性的内容。 新手入门的话推荐周志华的《机器学习》和李航的《统计学习方法》。 信息熵，基尼系数，分类误差定义关于过拟合决策树十分容易产生过拟合的情况，也就是说当前的决策树有很多的节点，整个决策树很大，此时会把样本中一些并不需要拿来作为分类的属性学习到，因此学习的决策树模型并不是最优的模型，这就是机器学习中的过拟合现象，与此相关的概念就是欠拟合，欠拟合主要是因为当前的样本不够，导致实际在学习的时候可以用来学习的信息很少，因此也会出现学习不好的现象。 预处理（Pre-Prunning） 在树已经成长成为完全树之后停止算法。 对于结点的剪枝规则 当所有类可以被分到一个类别时。 当某些类别的数量小于用户设定的阈值。 当某个结点并不能带来显著（高于阈值）的基尼系数（这里可以参考CART算法）或者信息熵的增长时。 后处理（Post-Prunning）使用MDL的思想，对于决策树自底向上逐层扫描剪枝。 注意预剪枝和后剪枝都可以使用验证集作为是否分裂的标准，与决策树的构建先后无关。但两者所获得的结果是有区别的，往往后剪枝获得的决策树要比预剪枝获得的大，且欠拟合风险很小，泛化性能往往更优，但毫无疑问的是，后剪枝的开销更大。 斜决策树单变量决策树的每个节点都是使用一个属性，这样生成的决策树如果用坐标空间来刻画（属性即坐标轴），划分的边界都是平行于坐标轴的。 但有时候单一的属性很难刻画分类的边缘，会造成抖动，而这个抖动只需要一条斜边就可以很好的解决了，其实所谓的斜边就是属性的线性组合，即节点使用多个属性的线性表达式来作为评判标准。 缺失值对于缺失值需要考虑两个问题： 样本集属性缺失，如何根据缺失的属性来选择划分节点？ 在不做缺失值填充的情况下，显然只能使用在该节点没有确实值的子样本集进行计算，然后信息增益的计算最后需要乘上一个不缺失系数（即该节点不缺失的样本集数与总样本的比） 。 假如你使用 ID3 算法，那么选择分类属性时，就要计算所有属性的熵增(信息增益，Gain)。假设10个样本，属性是 $a,b,c$。在计算 $a$ 属性熵时发现，第10个样本的 $a$ 属性缺失，那么就把第10个样本去掉，前9个样本组成新的样本集，在新样本集上按正常方法计算 $a$ 属性的熵增。然后结果乘 0.9（新样本占raw样本的比例），就是 $a$ 属性最终的熵。 这里给出一个课程 handouts 中的图示。 预测集属性缺失 若预测时，某个属性缺失，则以一定概率将该样本划分到该节点的各个取值中。至于这个概率如何选取，参考资料决策树是如何处理不完整数据的？]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[11-24-异常检测周报]]></title>
    <url>%2F2017-11-24%2F11-24-%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E5%91%A8%E6%8A%A5%2F</url>
    <content type="text"><![CDATA[这周的工作主要有了两个方向的进展，下面的报告分以下两个方面阐述。 一、异常点的识别首先要解释下，其实单一维度和多维度异常点识别可以看成是两类异常情况的识别，并不能简单的将单一维度看成是多维度的简化版本，对于我们手里的这组传感器数据，理性的思考，多维度的异常其实并不容易发生，很多时候其实就只是某个传感器的数据出了问题，但是我们利用多维的异常检测，比如iforest，DBSCAN聚类去扫描的方法，其实是不容易看出这点异常的，而且也有可能存在，本来就是单点异常，但是我们却把这个时段的所有点标记成异常，导致可能工人需要去检查所有的传感器，这样实际操作中费时费力且算法层面并不高效（iforest还好，但是DBSCAN最少都是nlogn的）。 因此目前我认为，不能先入为主的拿多维去做异常检测，而是首先对单一维度做异常的检测，也可以对多个强相关的维度进行协同检测。 下面首先讲述下我对于单一维度异常检测的一些思考和实验结果。 单一维度：差分方法 思路 利用多阶段的差分，我们可以识别出单点以及模式的异常。这个idea来自于梯度，当函数突然出现了一个异常的峰值点，或者是出现了突然地连续下降时，这个点的领域的梯度一定是有别于周围的其他点的，但是对于求区间的梯度（或者是斜率），我们首先要知道我们怎么去划分这个区间，但是这个区间的长度却会因数据的变化而动态变化（注意这里跟周期不是一个概念，虽然周期也是动态变化的），我们不容易确定这个值，因此我们选择利用差分，我们初始的差分可以从前后两个点开始，然后逐渐的调整差分的参数（实际上也就是去动态的找这个区间），显然单点异常一定会在差分之后的结果中突出出来，而模式的的异常，我们在增加差分的区间长度之后，也会显示出来。 下面的图中，第一行是原始数据，第二行是diff(1)的结果，第二行是diff(100)的结果。 从图中可以看出在diff（1）的时候，已经可以看出异常点了，而在diff（100）的时候，我们成功的检测到了梯度的突然下降。 再回到差分这个方法，虽然这个思想简单，比不得高大上的其他机器学习算法，但是目前这是我能够想到的效果不错，且效率最高的算法，因为这是一个在线的算法，它不需要累计一段时间的数据然后统一的进行处理（我使用过isolation forest和lof，效果不如差分好，且它们并不能实时的计算），我们可以部署到传感器上，自己就能轻松的实现报警。 还需改进的地方 目前差分对于简单异常点的识别非常暴力且轻松，但是对于比如某个区间梯度突然地变化这种异常情况，会比较依赖于差分参数的选择，而这个差分参数怎么自动化调整，是一个需要进一步解决的问题，我也在思考，目前有些思路比如『每个点统计前后n个点的均值方差，然后判断这个点究竟是需要进行一阶差分还是需要更高阶的差分』，这些思路其实也可以和聚类结合起来，因为最终我们的目的是去识别出函数断层的地方！，能够实现这个，我们的异常检测就能够粗略的达到了要求。 多维协同：孤立森林方法 这个方法对于单维数据，我并不推荐，我认为它的用处可以用作多维协同（多维一定要选取强相关的维度，否则一定会出错，而自动判定维度之间是否强相关是一个复杂度很高的问题！）的异常检测，这里我就不再赘述。 二、异常模式的识别首先也需要解释，我对于异常模式识别的理解，首先我们的目标是要识别出正常的模式，然后有别于这种模式的其他段就是异常。 ​ 而识别正常模式的第一个阶段，我认为需要去对总的数据进行分类（不能拿整体数据进行模式的识别），对于上面的数据，如果我们能够大致的判断出其中包括了三类区间，就能够用神经网络（LSTM）对小区间的模式进行识别，那么我们针对于这种情况，能够想到的只有聚类这个方式。目前常用的聚类方法我认为可行的只有K-Means和DBSCAN，下面分别对这两种方法进行说明。 K-Means 对于kmeans其实效果都还挺好，但是这个方法的局限性在于我们需要为我们的数据指定固定的cluster的个数。 下面是对15，16，17，18列的数据进行协同的模式分析（这里也要说明，如果要对多维数据协同分析，一定要利用先验知识或者是互相的拟合关系说明他们是强相关的，在下面的四组数据中实际上我为了观察聚类效果能否将其他维度的非模式段拉到本维度的正常模式，而放松了这个界限） DBSCAN DBSCAN相比如K-MEANS最具有优势的两点在于，其一，其不用指定数据的聚类，其二，其可以对于非凸的cluster进行较好的聚类，大致效果就是这个意思。 ![这里写图片描述](https://ws4.sinaimg.cn/large/006tKfTcly1flsjmxbcr4j309b0l1djr.jpg) 显然DBSCAN聚类的效果才是我们希望达到的目标，但是需要注意的是，这里的eps的选取对聚类效果影响很 大，同样这里也是影响到我对于windmachine数据测试的聚类效果一大关键的点。 在我调整参数的过程中出现了两个极端的情况。 第一种则是所有数据聚成一类 第二种情况则是算法自动将1000多个点分为了700多个类，效果五彩斑斓 目前这个问题还没有解决，但是我有两个还没有实验的想法。 我推测是没有正确的使用时间这个属性，权重？或者是对于已知异常点动态的加入参数。 对于1000个点分为了700类的情况，在图中实际可以看出类与类之间也有相似度的区别（从颜色就可以看出来），那么再对这个类的质心进行一次聚类，是否可以能够划分为更少的大类？ 后者方法其实我认为非常可行与可解，最近时间比较紧凑，但是我会尽快去做相应的尝试。 聚类之后 说完了聚类之后，那么对于较小区间的模式识别其实已经是有了现成的方法（但是时间复杂度很高，因此我们必须要尽可能多的去进行区间的识别，去划分有可能存在周期函数的区间出来再往上并行的套神经网络，不能对所有的层都做一遍线性的神经网络，复杂度太高了！） 如上图，我使用周期函数加了高斯噪声，并在中间这个地方加入了人工的异常点，进行了一组测试，LSTM是可以找出异常的模式并加以权重十分高的Error，我认为这个效果非常的好，所以我们只要能打通聚类的那一步，我认为对于区间函数下异常模式，其实是可以很好解决的。 小结 差分目前主要的问题在于需要去细化差分的参数，使其不仅能够找出异常点，而也能够对区间的异常有一定的检测作用，这方面得去看一些金融时间序列的东西，希望能够从统计这里得到一些信息。 对于聚类，目前认为DBSCAN以及其各种改进，比如OPTICS，SNN是一些较好的方法，但是聚类我认为应该结合具有相关性的几个维度去做，然后做模式的区分，目前使用K-Means已经有了一个较可观的实现，希望能够有一定的效果。 我之后的方向是希望去细化研究上述1，2点，能够通过对DBSCAN的细化研究，希望能够将DBSCAN在特定的数据集上做一些自己的优化，然后还能够研究下DBSCAN在时间序列下的更适配的应用（要注意时间序列的时间这一维特性，是否完全不用？我认为不行，但是要怎么用，我也不太清楚） LSTM是否是唯一最好的用于这种区间函数下异常模式识别的方法？我对神经网络了解的不多，我不太清楚，还需要请教一些高人。]]></content>
      <categories>
        <category>异常检测</category>
      </categories>
      <tags>
        <tag>大数据异常检测</tag>
      </tags>
  </entry>
</search>
