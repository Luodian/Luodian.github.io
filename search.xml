<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Libra-Brief Understanding]]></title>
    <url>%2F2019-06-27%2FLibra-Brief-Understanding%2F</url>
    <content type="text"><![CDATA[Libra-Brief Understanding]]></content>
  </entry>
  <entry>
    <title><![CDATA[How China Created Its Own Internet]]></title>
    <url>%2F2019-05-27%2FHow-China-Created-Its-Own-Internet%2F</url>
    <content type="text"><![CDATA[个人感觉这个视频还是有点反华主义，主要攻击点在于网络管制。之前看过彭博的另外一个视频也是类似的对个人隐私监控的报道，有一些负面的情绪。 https://www.youtube.com/watch?v=qBksg5JPLz0 Across the Great Wall we can reach everywhere in the world 1987年中国向德国发送了第一封电子邮件，内容如上。 Future Shock by Alvin Toffler 江泽民和许多爱国主义者深受这本书的影响，这本书中讲述了一个在管制和调控下高速成长的超工业社会（Super-Industrial Society）。 Cyber Sovereignty 习大大在第二届世界互联网大会开幕式上，尊重每个国家选择自己的网络发展道路和网络管理模式，从而保护网络主权。 Bb指出Cybersecurity的法令使得中国政府具有审查外国（注意这里只用了foreign一词）公司的硬件及软件，从而达到监控敏感用户信息的能力。 Bb指出网络上关于Winnie的照片会受到审查和禁止（azhen的github头像就是这个 China’s Young Generation are perfectly happy living inside the firewall 这点我不是很同意，因为现在年轻一代很多都具有翻墙的能力，我们也知道墙的存在，并不是愚昧的一代。 2018年Freedom House的调查指出，中国现在是网络自由度最差的国家，同时中国向排名相近的几个国家出口网络控制技术。 Bb最后还用1987年的邮件反讽了中国的网络建设与初衷截然相反。 一直没找到一个视频或者报道，能够很thoroughly的分析出如此大的国家建立网络审查制度的原因和必要性，pros and cons的对比等等，我很好奇这些。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Huawei's Recently Affairs]]></title>
    <url>%2F2019-05-26%2FHuawei's%20Recently%20Affairs%2F</url>
    <content type="text"><![CDATA[BBC Click: Inside Huawei and 5G https://www.youtube.com/watch?v=_8HqbPBRiS4 第一个视频是BBC Click对华为近况以及受到美国制裁的原因的一个简介，时间很短，态度很中立。其中一部分表明了任正非创立华为的艰辛历程，把其比作中国的乔布斯，给足了respect的态度。第二部分则分析了为什么全世界对于华为存在怀疑甚至敌视的态度，其中任正非之前是军队的背景，并且其品牌名字华为表明了其有较为强烈的民族立场。华为目前在5G的设备供应商领域处于世界一流的水平，英国方面既依赖于华为的5G设备建立起新一代的网络设施，但是也惧怕其受中国政府的操作，在销售往国外的芯片和设备中藏有黑客后门，从而在战争阶段可以利用这些后门发起攻击。其中提到了在2017年，中国政府通过了一项法令要求所有的中国公司和中国公民都有义务协助国家保障网络信息安全，这其中暗藏了中国政府有能力控制华为公司要求它们在芯片中藏进后门。 Mr. Ren was once a member of the Chinese Army. In 2017, China passed the national intelligence law, which means that all chinese companies and citizens must help government to assist national intelligence security. The problem is, if Huawei supplies any country’s 5G network, Chinese government could exploit it. Many people think that Chinese government would likely to eavesdrop on other countries. The most important threat is in the future, our own economy and even our lives are run by computers over 5G. China could order Huawei to set backdoor in its 5G devices. Huawei 5G LIES https://www.youtube.com/watch?v=IfxfdHJ3k9Y 火锅大王的一个视频，这个视频因为主要是放在中文社区，所以言论是挺华为一派的，不过中间很多论点说的很有道理。 华为和中国军队有关系？那你知道有多少美国公司和美国军队有关系吗？ Apple, Google, Microsoft… 大王指出，从2007年布什在位时，美国曾指出华为和中国政府有所合作，但这种指控十分的weak，因为任何一个国家的政府都需要和自己国家的大公司进行合作，难道政府需要自己研究手机吗？此后美国曾联合西方世界不断地对华为进行审查和指控，但是所有的指控都没有证据。 有证据表明，华为曾对德国和英国的安全部门开放源码进行审查，审查得出的结论是华为并未有后门行为。任正非本人也在采访中声称”如果被世界发现我们藏有后门，那么我们的生意就没办法做下去了，我们不会做这么傻的决策。” 接着火锅大王指出，美国敢承认自己对美国和其他国家进行了多久的监听吗？ 当华为利用优质的5G技术逐渐进入西方市场时，以美国为首的西方国家开始严厉指责华为是一个危险的大型间谍黑客公司。在棱镜门事件曝光之后，美国被证明为拥有世界上最彻底的黑客攻击产业和活动，大量依赖于软件中的漏洞进行监听，在wiki leaks中记录了美国CIA发现并掌握了全世界许多软硬件漏洞，却不将其公开，而是利用其监听公众的隐私。 https://wikileaks.org/ciav7p1/ 我们能够理解CIA要为了国家安全和防止恐怖主义等原因监控其公民，同时中国也有其理由让华为以及其他中国企业监控中国公民。 既然中国政府都容忍民众使用苹果手机，微软的系统，为什么美国要一直指责华为作为间谍公司？ 火锅大王指出了一个来自华为的Eric Xu提出的一针见血的公开质疑。 “他们是否真的在考虑其他国家人民的网络安全和隐私保护？或者有其他的背后的动机，有人说美国正在努力的寻找中美贸易谈判的筹码。” “如果华为的设备在西方国家被使用，美国的机构会发现更难获取民众们的隐私信息，更难拦截他们的移动通信。” 特朗普示意，作为交易协商的一部分，他将考虑撤销对华为的控诉。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Here's How Chinese Consumers Feel About Apple Amid the Trade War]]></title>
    <url>%2F2019-05-24%2FHere-s-how-Chinese-consumers-feel-about-Apple-amid-the-trade-war%2F</url>
    <content type="text"><![CDATA[Link: https://www.youtube.com/watch?v=BJ7ZscDt_Mw Walter Piecyk, managing director at BTIG.总体来说态度是比较中和的，不赞同这种暂时的摩擦去确切的影响跨国大公司未来几年的决定。 Trade War does have impact on consumer’s sentimentApple is pretty important to China, there’s a lot of jobs, consumers and taxes. I think they can repair this relationship. I don’t think Apple is irreparably harmed but again if things worsen and Apple starts to look for manufactoring out of China, such as to Vietnam. Piecyk表示Apple对于中国的消费者和中国的制造业具有重要的作用，苹果离开中国是一个双输的局面。 Greater China represented ~20% of Apple’s revenue and operating profit. For every 5% drop in Greater China sales, Apple’s earnings per share should fall about 15 cents, according to analyst Matthew Cabral. 中国市场占据了苹果营收的一大部分，目前苹果大部分的商品在关税限制之外，所以更需要担心的更多的是消费者的情绪和需求。在贸易摩擦不升级的前提下。目前很多分析者都并未调低对苹果的期望值。 EPS：https://zhuanlan.zhihu.com/p/23813840 You should do it cost effectivelyOnce you choose to leave, it’s not easy to come back. Handling the supply chain is Tim Cook’s specciality, he should be planning now but the changes will be made through 2 or 3 years. All the companies done it in the most recent quarter, it has been the very negative impact on their results. Piecyk表示现在表态，做出决定支持trade war的公司都收到了严重的影响，这是不明智的决定。]]></content>
      <categories>
        <category>Know the World</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Why This Analyst Is Buying Chip Stocks During Huawei Fallout]]></title>
    <url>%2F2019-05-21%2FWhy-this-analyst-is-buying-chip-stocks-during-Huawei-fallout%2F</url>
    <content type="text"><![CDATA[Chips stocks under pressure近期美国发布禁令限制美国企业向华为销售芯片。 Intel, Nvidia、高通、博通等芯片企业股价预计会上涨。中美两边持续紧张的贸易战关系可能导致不断加税的可能性，但是基于目前两家主流显卡厂商Nvidia，AMD，主流芯片设计厂商基本全部为美国企业的现实来看，未来中国无法摆脱受制的困境，如果没有这些厂商，基本等同于无法持续发展AI。 Huawei issue, U.S./China relationship are negative overhangs on semi-conductor sector. A lift of either would send the industry 5-10% higher. Huawei Fallout华为关于5G的战略将持续受到美国打压。任何将要使用华为5G网络，从商用终端到消费者设备，都会受到很大的短期影响，但评论家对美国芯片企业信心更足。]]></content>
  </entry>
  <entry>
    <title><![CDATA[The Bull Case for Uber]]></title>
    <url>%2F2019-05-19%2FHere-s-how-traders-are-looking-at-Uber-s-market-value%2F</url>
    <content type="text"><![CDATA[Uber创业九年，经历各种风波，10年亏损80亿美元，至今尚未盈利。 5.13开盘45美元，5.17收盘报42美元。 几位评论员综合的看法还是uncertain，而一旦看法不确定，导致short的可能性就更大了。 中文报道：https://wallstreetcn.com/articles/3510392 源视频link：https://www.youtube.com/watch?v=3OzHsKGRvgY How much subsidizing is going to be done over ? 这个部分评论员主要针对uber的盈利模式，补贴政策以及和司机的关系（如何让司机更多的开Uber）。这部分的评价大多是uncertain或者是short的，因为目前并没有明晰的在摆脱补贴政策下让乘客和司机都满意的方式。 Josh Brown Each ride as if it’s not very profitable but at least a little bit profitable, they are not very yet and there remains an open quesition whether or not they ever will. Shannon Saccocia Concerning about the labour force and how it utilize its drivers as its strength. How much the actual revenue the riders will get? Comparing with Lyft Uber在美国的另一个竞争对手：Lyft. 有评论员认为Uber是一个iconic的公司，Lyft作为后来者并不能取代其位置，Cuban则认为Uber太老了，花了九年的时间才上市，『it’s not a growth money』。 另外这里也有把特斯拉拎出来挨打的，dream company不能给股市带来足够的动力和信心去一直支持你。 Josh Brown Uber is a name there, a verb and a company that has literally changed human behavior forever. Jon Najarian I will short Lyft. Uber is estimated as 120 billion but obviously nowhere near that. Mark Cuban It’s a nine year old company, you can’t expect anything new and explosive from him. It’s not a growth company. Stephen Weiss It’s great comsumer experience to use Uber. I just can’t trade company(Tesla) that don’t have that path however there’s only dream about the future. The reason that why the price going high is that there’s just massive floods of capital going into VC funds/Private Equity funds. When will there be auto-drive? 这里感觉评论员不算太了解auto-drive，2022年的预估可能还为时尚早。评论区也有人criticize了，目前Uber和Lyft也都在大力做自动驾驶的事情。因为很明显的，自动驾驶出租车一出来，原有的这些公司都没得玩，要么被人干掉，要么自己开始储备转型。 Probably in 2020~2022, and Robo-taxi’s appearance will wipe out both companies(Uber, Lyft and their homo-products).]]></content>
  </entry>
  <entry>
    <title><![CDATA[Tom Friedman Explains Why He Agrees With Trump's China Trade Approach]]></title>
    <url>%2F2019-05-18%2FTom-Friedman-explains-why-he-agrees-with-Trump-s-China-trade-approach%2F</url>
    <content type="text"><![CDATA[Two fundamental things happen today totally changed relationship 在访谈里，Tom分析了Trade War现在的形势，美国所处的状态以及中国在如『智造2025』，『5G』，『华为芯片』等方面所不断制造的威胁。称其同意川普的做法但是或许有更明智的方式。 Youtube Link: https://www.youtube.com/watch?v=nVBqSOPWIUw Bio of Tom Friedman: https://en.wikipedia.org/wiki/Thomas_Friedman China 2025One is they are competing head to head with our most cutting-edge companies. Artificial Intelligence. New material aerospace. Super Computing. Those technology are all dual uses. Turn on those technologies are turning on your missiles The game has to be calledThe China is getting rich over the last 30 years using these things. Hard work and smart investment on infrastructure. Emphasis on Education. Delayed Gratification. Cheating WTO and stealing IP from U.S. Non reciprocal trade arrangements. U.S. presence in the Pacific assured everyone around China can at most be dominated by China economically but not politically and geopolitically. China bought KUKA KUKA: probably the best robotic company in the world. This is much more than a trade imbalance, cause China’s building your future on their telecom platform. TPPTom don’t agree with Trump’s means but agree with his aims. He said he would sign TPP if he was the President. Wiki：什么是TPP 知乎：为什么中国不能加入TPP 知乎：为什么川普退出TPP Got EU completely on U.S. side instead of waste ammo on aluminum and steel tariffs with them. U.S. got more leverage and we can sit and talk but don’t use Twitter to ellicit it as an explosive nationalist political problem.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Conclusion-ROAD-Reality Oriented Adaptation Network]]></title>
    <url>%2F2019-04-18%2FConclusion-ROAD-Reality-Oriented-Adaptation-Network%2F</url>
    <content type="text"><![CDATA[ROAD-Reality Oriented Adaption For Semantic Segmentation of Urban Scenes.Motivation Traditional methods overfitting to synthetic dataset. Domain shift between synthetic and real world. Work Propose a target guided distillation approach to learn the real image style. Distill from a pretrained model on real data, let synthetic model imitate the result of real model. Propose a spatial-aware adaption scheme to effectively align the distribution of two domains. Results]]></content>
      <categories>
        <category>Computer Vision</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Focal Loss/Retina Network 论文阅读]]></title>
    <url>%2F2019-03-15%2FFocal-Loss-Retina-Network-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[因为CVPR 2019出了一篇新的FSAF，看那个的时候根据需求就补了Retina Network和SSD，跟组里的同学们讨论了之后也算是有些感慨，另外这篇论文是ICCV 2017的best student paper。 先放上两篇论文的链接 Focal Loss: https://arxiv.org/abs/1708.02002 FSAF: https://arxiv.org/abs/1903.00621 回到正题，文章主要有两个方法组成，Focal Loss和Retina Network，这两个方法都非常简明但却有效，在论文中做出了和比Faster RCNN更好的效果，但似乎两者不是公平对比，不过这也算是写论文的一个方式啦？让人一看就觉得『喔？Single Stage的超过了Two Stage的』，给人一种不明觉厉的感觉。 论文的效果是这样的，当然后续被yolo v3以非常风骚的方式超过了…具体可以去看论文，挺皮。 另外我看focal loss时没有借助知乎和博客的帮助，但写之前也搜了搜大概有多少人在写这个东西，就看到很多人说这是Kaiming和RBG的工作，为什么忽略 Tsung-Yi Lin，这个人可是搞出了FPN的大佬，COCO主页排在第一位的人…难道连科研也带着吃瓜心理吗？ 下面的整理会分为两个部分 Focal LossClass Imbalance文章首先分析了为什么 single stage 的网络会比 two stage 的效果差，其原因在于 single stage 没有像 two stage 那样对proposals的筛选能力（faster rcnn在训练阶段会对RPN输出的rois按照1:3的比例提取正负样本，送往fast head进行进一步的回归和分类） 对于 single stage 的网络，其在最后的feature map中逐点采样 anchor，会生成大约200k个anchor，而这其中大部分都是 negative proposals, 即这些样本的类别为bg，能够包裹住目标的 positive proposals 比例很少。这样的类别不均衡是导致单阶段网络学习效率低的重要因素，其具体还指出了两点问题： training is inefficient as most locations are easy negatives that contribute no useful learning signal. the easy negatives can overwhelm training and lead to degenerate models. 大部分区域都是负样本，在这些区域的学习是无意义的。 过多的负样本让模型不能很好地学会什么是一个好的正样本。 这个道理是很明确的，即使我们在最后对object进行分类的时候，也希望各个类别在train中出现尽量均衡，不然就会出现对于量少的类别检测效果不好的情况。 Two Stage 有自己的分配的方法，也有人提出Online Hard Example Mining，将分类错的样本设置更高的loss权重使得模型在正负样本数量不均衡的情况下，最后的loss能够平衡。而本文是对loss直接下手，在传统的Cross Entropy上面增添了一个modulationg factor。 由下图可以看到，在well-classfied examples的区域，FL相比于CE的loss更小，也即其能够更好地忽略掉容易学习的负样本。 当然作者也提到了，之前也有人考虑到了这个类别不均衡的问题，然后提出了可以把CE改写成这样的格式，其中$\alpha \in [0,1]$ for class 1, $1 - \alpha$ for class 0. 但是这样的话，其实是利用 $\alpha$ 这个参数来平衡正负样本数量上的类别不均衡，而FL的方式则是更进一步的进行难以样本的区分，其中论文提到对于 $p = 0.9$ 时loss会比原来CE计算出来的小100倍，$p = 0.986$ 时loss会比原来计算的小1000倍，这就很好的做到了对 well-classified 的样本给予更小的loss。 作者实验结论显示 $\gamma = 2$ 时，并且加入前人的系数 $\alpha_t = 0.25$ 进行的实验效果最好，超参数可能和具体的anchor的参数和数据集类型有关。 Model Initialization文章中也谈到了对于模型初始化的一些问题，说实话这点细节我是想不到的。具体来说就是，模型的二分类分支传统以来的方式都是$\mu = 0, \sigma = 0.1$ 来进行初始化的（因为模型要输出1，-1这两个结果），现在样本这么不均衡，那么就要考虑给参数进行一个 $\mu = \pi$ 的初始化，以便于让模型输出的 $p$ 是靠近类别较多的那部分样本。 RetinaNet DetectorRetinaNet结合了FPN，其实可以想到的是，使用FPN，以及FPN类似于RPN或者是DeepMask式的Anchors对RetinaNet可能是其涨点最多的存在，文中有这么一句话。 While many design choices are not crucial, we emphasize the use of the FPN backbone is; preliminary experiments using features from only the final ResNet layer yielded low AP. 从底层到上层（$P_3…P_7$）Anchor的采样设置分别为：$32^2, ….,512^2$. 每层anchor的采样比例为 $\{0.5,1,2\}$，同时为了更dense的去覆盖（同时也会增加计算量叭），文章还在每一层anchor的基础上使用了 $\{2^0, 2^{\frac{1}{3}}, 2^{\frac{2}{3}}\}$，这样每层的每个点就会有9个尺寸的anchor，可以很好地覆盖数据集中各种大小的图片。 接下来是FPN的每层都会外接两个FCN分支，分别用来做cls和bbox reg的工作，这两个分支之间是不share weights的，但是每个单独的分支在FPN的各层之间共享权重。FL就是用在cls branch上面的，在最后计算FL的时候会针对于assigned anchor（即被挑出来用于下一步回归的anchor）的loss进行一次normalization，这样的操作主要是让模型在回归正样本是的能力也更robust. 最后作者对于本文中提到的很多参数做了尝试： 文中最后和各种检测模型的比较： Architecture Design最后作者还针对于 anchor density 和 speed versus accurancy 进行了一些讨论。 Anchor Density作者提到，对于single stage detector，最重要的一个设计就是anchor density，要让anchor覆盖到更多不同尺寸的目标存在的区域才能更有效地提高精度，对于two stage detector，其proposals可以依赖于后面的roi pooling等操作让其重新获得更大的感受野，更好的获知目标的信息。 本文对anchor density进行了一系列的测试：在同样使用resnet-50作为backbone，对于每个位置产生1个anchor的情况可以获得30.3的mAP，而按照比例在一个位置产生9个anchor，可以很惊人的提高到34.0的mAP（这点之前在做比赛的时候我也感受到了密集anchor的作用，retina的名称我想可能一部分是来源于此的） 最后再增加6-9个anchors不会获得更多的提升，说明anchor的作用在这里就趋于饱和了。 Speed versus Accuracy图标在table 1e（上上个图）可以看到. 众所周知，更深更复杂的backbone可以提高精度，但是却会降低速度。这里对比了RetineNet-R101-FPN-600能够和RestNet-101-FPN-FR-CNN达到同等的精度，但是Retina每张图片用时122ms，FR-CNN用时172ms。同样作者承认，如果需要更快的速度，就需要在整体的设计上进行新的改进（如yolo）。 总的来说，本文观察到了一个大家可能已知但是并没有很好解决的class imbalance这一点，然后提出了FL的解决思路，最后再结合自己之前论文中的FPN，成功做出了一个很好的结果。简洁又有效，果然配得起best student paper这个称号。]]></content>
      <categories>
        <category>Computer Vision</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Trident Network 阅读笔记]]></title>
    <url>%2F2019-03-12%2FTrident-Network-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Trident Network本名：Scale-Aware Trident Networks for Object Detection 这篇论文的作者自己分析了这篇论文，我自己写这个单纯出于两个想法： 自己记录笔记，督促自己总结并且再精读一遍。 把笔记公开，让更多的人看到不同的文字的分析，我自己经常会搜不同的论文解读，往往只看一个人的解读还是会存在疑惑，而不同的解读针对的侧重点不同，融合之后会让我有更全面的理解。 其实想更通透肯定得看原文，看分析没有提升的，只能起到一个快速浏览的作用，万一我在瞎解读呢~ 本文一开始即分析了，在目标检测中尺度变化（Scale Variation）通常是我们面临的最大的问题。本文首先对尺度问题做了实验分析，并且针对于实验分析中的结果，提出了Trident Network这种网络结构来产生在不同尺度上表达能力都相同的feature map. 解决尺度变化（Scale Variation），即小目标难以在小尺寸图像中检测，单纯放大图像又会很不memory efficient（这基本算是一个共识性的难题），这样一个基本目前还没有很好的解决的问题。在这个痛点上，TN提出了它们的方法并且刷出了目前ResNet 101+Deformable Conv的单模型在COCO上的新的SOTA：48.4。 在这之前其实有一些尝试去解决这个问题的方法，比如： Multi Scale Image Pyramid 单纯的通过尺度变换放缩图像其实是非常暴力的做法，让固定尺度的感受野去识别不同尺度的物体，简单有效，刷比赛的时候基本都会用上。 FPN 同样在固定尺度的感受野上，使用不同尺度的feature map，不同的层负责不同尺度的anchor，和1同理，也很有效，在小物体上效果明显。 SNIP, SNIPER 这两者是同一个组先后做出来的结果，后者比前者效率高很多，使用了新颖的方法做数据预处理，在不同尺度上挑选出和GT相近的框，切图之后作为新的训练集。解决了单纯放大图像所带来的memory inefficient的问题。新生成的chips尺度为：512x512，能够在同等情况下使用更大的batchsize来进行训练，更大的bs意味着更robust的batch normalization，好处多多。 本文解决尺度变化问题采取的方法是变换在backbone的卷积操作，增加dilation conv来增加感受野，因为使用了并行的三个不同尺度（0，1，2）的dilation conv，因此称为Trident Network（三叉戟网络）。 本文针对这个现象做了一组实验，发现dilation rate和不同尺度的检测性能正相关，即大的dilation rate对应大物体的检测性能更好，这个也是比较容易理解的。 在有了这个实验结果的基础上，作者就开始考虑将这几个分支的优点并行起来，并且参照了SNIP系列的思想，将不同尺度范围的框分开使用不同感受野的conv进行训练，同时这几个分支还是weight sharing的，因为它们所负责的检测任务除了尺度不一样之外都是一致的（类别，变换的操作等）。 作者同时也尝试了在backbone不同的stage加入trident blocks所带来的影响，在conv4上做效果是最好的，因为此时已经具有了全局语义信息的感受能力。在conv4中的trident blocks增加到15个时，性能会达到饱和。（不得不说实验做的很充分） 各类目标检测算法的一个对比图，图森居然做了这么多实验，真厉害啊。不过这里要说明的是，TridentNet*是融合了multiscale training/testing, soft-nms, dcn, large-batch BN的最终版本才取得了48.4的SOTA。（突然想起来ECCV上看到何恺明看我们poster的时候，说了一句『改了改backbone就能这么大提升么？』） 最后在inference的阶段中，作者发现如果让所有尺度的样本都在三个分支上进行充分的训练，那么可以只使用中间的分支便能获得不错的效果，使用single branch的Trident Network*最终能够取得47.6的mAP。]]></content>
      <categories>
        <category>Computer Vision</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Car/Pedestrian Detection Paper Reviews]]></title>
    <url>%2F2018-06-25%2FCar-Pedestrian-Detection-Paper-Reviews%2F</url>
    <content type="text"><![CDATA[Car/Pedestrian detections and datasets review[TOC] 1. Car / Pedstrains detection reviews1.1 How Far are We from Solving Pedestrian Detection1.1.0 Abstract Encouraged by the recent progress in pedestrian detection, we investigate the gap between current state-of-the-art methods and the “perfect single frame detector”. We enable our analysis by creating a human baseline for pedestrian detection (over the Caltech dataset), and by manually clustering the recurrent errors of a top detector. Our results characterise both localisation and background-versus-foreground errors. To address localisation errors we study the impact of training annotation noise on the detector performance, and show that we can improve even with a small portion of sanitised training data. To address background/foreground discrimination, we study convnets for pedestrian detection, and discuss which factors affect their performance. Other than our in-depth analysis, we report top performance on the Caltech dataset, and provide a new sanitised set of training and test annotations. 2.1.1 Analyzing the state of the art.这篇论文是一个分析性质的论文，主要针对于Caltech的数据集进行错误分析，文章让domain experts在Caltech的数据集上进行人工识别，并提出机器检测算法应该达到至少人类水平，甚至应该超过人类水平。 文中的人工检测采用人工基准线，即从人的头到脚画一条线。为了公平比较，关注于单帧单目检测，注释器需要根据行人外表和单帧环境来注释。 但目前主流的基于ICF的方法都被人完爆。 We find that the human baseline widely outperforms state-of-the-art detectors in all settings, indicating that there is still room for improvement for automatic methods. Since most top methods of figure 1 are of the ICF family, we expect a similar behaviour for them too.Methods using convnets with proposals based on ICF detectors will also be affected. 2.1.2 Failure analysis我们需要知道为什么，何时会出错。 检测器主要会出现两种错误： 假阳性（检测到了背景，或者很弱的定位检测），FP聚类成11个分类： These categories fall into three groups: localisation, background, and annotation errors. 假阴性（低得分率或者错过某些行人检测，检测不全） 2.1.3 Improving Annotation论文指出Caltech的数据集有错误，如下： 这里提出了新的标注方式，即可以观看完整视频来判断是不是有人，同时在人群中时，也要注意不能重复标记一个人。 2.1.4 Analysis Pruning benefits 从原始到修剪注释的主要变化是删除注释错误，从修剪到新的，主要的变化是更好的对齐。更强的检测器更好地受益于更好的数据，并且检测质量的最大增益来自移除注释错误。 Alignment benefits 为了利用新的10%注释来利用90%剩余数据，我们在新的注释上训练模型，并使用该模型在90%部分上重新对准原始注释。 因为新的注释更好地对齐，所以我们期望该模型能够修复原始注释中的轻微位置和缩放错误。结果表明，使用检测器模型来提高整体数据对准确实是有效的，并且更好地对准训练数据导致更好的检测质量（在MRO和MRN中） 总的定位和前景背景错误都很重要，在原dataset 上，去除一些错误的标注，和校准一些不太准确的标注，都有助于提高检测质量。 convnet 在图像分类和目标检测上性能很好，但对于小物体的定位还有一定的局限性，可能与pooling有关系，这时候Bounding box regression和NMS就显得比较重要。而前景背景的区分度上也有待提升，说明convnet对于分类还有提升空间。 2.1.5 论文信息会议：CVPR 2016 团队：Max Planck Institute for Informatics（德国的研究机构） 数据集：在Caltech上进行分析，并且自己提出了一个更干净的CityPersons数据集 1.2 Is Faster R-CNN Doing Well for Pedestrian Detection1.2.1 Abstract本文主要是分析了一下Faster R-CNN用于行人检测效果不好的原因，并对比提出了解决方案，本文提出的RPN+BF算法似乎从时间还是准确度上，提升都非常的大。 Faster R-CNN用于行人检测效果不好的原因有两个： 行人在图像中的尺寸较小（比如 28×70 for Caltech）。对于小物体， Region-of-Interest (RoI) pooling layer 在 low-resolution feature map（特征层的尺寸又减小了很多啊） 提出的特征没有什么区分能力。 针对该情况，论文在更大尺寸的浅层特征上提取特征，以此提高提出特征的区分能力。 行人检测中的误检主要是背景的干扰，广义物体检测主要受多种类影响。 2.2.2 ApproachOur approach consists of two components (illustrated in Fig. 2): an RPN that generates candidate boxes as well as convolutional feature maps, and a Boosted Forest that classifies these proposals using these convolutional features. 本论文提出了一种方法，使用Boosted Forest直接训练 RPN (Region Proposal Network) 提出的深度卷积特征。 2.2.2.1 Region Proposal Network for Pedestrian Detection这里针对行人检测进行单个物体检测，对 RPN 进行了一些修改。adopt anchors of a single aspect ratio of 0.41 (width to height)。主要人的长宽比是相对固定的。 针对多尺度问题，论文使用了9个尺度作为检测标定，这样我们就不用建立特征金字塔来解决多尺度行人检测。 2.2.2.2 Feature ExtractionRPN 产生候选区域，然后使用 RoI pooling 得到固定长度的特征，使用这些特征训练 Boosted Forest，下面是不同候选区域数目的影响。 2.2.2.3 Boosted Forest当RPN已经产生了region proposals, confidence scores, and features. 接下来论文采用BF的方式对这些特征进行训练。 论文采用了RealBoost algorithm[4]进行训练. Train阶段采用的数据集：∼50k on the Caltech set BF过程中一共使用了6个stage，每个stage的树分别是 {64, 128, 256, 512, 1024, 1536}，其实没有必要同等的处理初始的 proposals，因为我们的 proposals 在 RPN 之后得到了初始的 score，因此可以把RPN过程看成是 $f_0$. 2.2.3 Comparasion不同候选区域数目的影响 不同的Classfiers的组合 How Important is Feature Resolution 上图中使用R-CNN（用VGG-16网络）方法实现了13.1的MR，略好于独立RPN检测器（14.9%的MR），它使用的窗口和上面提到的RPN是一样的。R-CNN从图像上剪切的目标候选区域，并且调整到224x224的尺度，因此它受小目标的影响比较小。这表明如果提取224x224精细的特征，下游的分类器可以提升精度。 同样在RPN提取的候选窗口上训练一个Fast R-CNN分类器，性能掉到了20.2%。尽管R-CNN在这个任务上工作很好，但是Fast R-CNN却产生了更糟糕的结果。 这个问题部分是因为低分辨率的特征。在Conv5上使用a trous trick，把stride从16减少到8个像素，这个问题得到了缓解，实现了16.2%的MR。这说明更高的分辨率是有帮助的。 不同特征的影响 Bootstrap的作用 时间效率 各大数据集测试结果 2.2.5 Conclusion总体而言，本文在做实验的基础上，发现了 faster RCNN 在行人检测方面的不足，在实践中发现科研问题，深度挖掘身体背后的原因，并且结合已有的技术，完美的将其解决，本身就为我们提供了一个很好的科研思路和科研案例，值得我们学习！ 值得一提的是，本文的大佬是个一线的研究+开发者，论文中提供了训练过程中的一些实现细节，甚至开放了源码： https://github.com/zhangliliang/RPN_BF 2.2.5 论文信息会议：EECV 2016 团队：Zhang Li Liang from 中山大学林倞教授研究组 &amp; MSRA何凯明 1.3 A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection1.3.1 Abstract这是一篇UCSD-SVCL实验室和IBM研究院一起研究的结果，主要是关注多尺度快速目标检测的问题。 这篇文章主要解决多尺度同时存在时的检索问题，设计了MS-CNN。MS-CNN主要包含proposal sub-network和detection-subnetwork 这两个网络结构面向的任务分别是： 针对多尺度问题（proposal subnetwork）： 类似于FCNT跟踪方法，该文章也是观察到了卷积网络不同层得到的特征特点的不同，对不同层的特征采用不同的利用方式。比如conv-3的低网络层，有更小的感受野，可以进行小目标的检测；而高层如conv-5，对于大目标的检测更加准确。对于不同的输出层设计不同尺度的目标检测器，完成多尺度下的检测问题。 在卷积神经网络中，感受野的定义是卷积神经网络每一层输出的特征图（feature map）上的像素点 在原始图像上映射的区域大小。 针对速度问题（detection subnetwork）： 使用特征上采样代替输入图像的上采样步骤。通过设计一个去卷积层，来增加特征图的分辨率，使得小目标依然可以被检测出来。这里使用了特征图的deconvolutional layer（去卷积层）来代替input图像的上采样，可以大大减少内存占用，提高速度。 去卷积层一直用于分隔和边缘检测，该论文第一次用它加速和提高检测率。 1.3.2 Related Work (a) Learn a single classifier and rescale the image multiple times, so that the classifier can match all possible object sizes. 但这种方法非常的耗时间。 (b) [5]中提出，针对a方法的一个改进：Apply multiple classifiers to a single input image. 避免了重复对一个feature maps的重复计算。However, it requires an individual classifier for each object scale and usually fails to produce good detectors. (c) rescale the input a few times and learn a small number of model templates. (d) this consists of rescaling the input a small number of times and interpolating the missing feature maps. （这里不太理解）. 这种方法能够在极小的准确率损失的情况下，实现较高的速度提升。 (e) the R-CNN of [6] simply warps object proposal patches to the natural scale of the CNN. features are computed for patches rather than the entire image. 这是一个类似于a的方法，但是我还不是很理解。 (f) RPN的检测机制，multiple sets of templates of the same size are applied to all feature maps. 因为从输入的图像到输出的model templates都是相同大小的，在对于不同大小物体的检测上会存在着很大的问题。 (g) 本文提出的 multi-scale strategy，可以被看成是c的CNN拓展版本。It exploits feature maps of several resolutions to detect objects at different scales. 这可以输出不同大小的感知层，从而实现不同大小的目标检测。 这个方法是e和f两者的结合产物，也就是做了一个trade-off 1.3.3 Proposal Network文章的网络结构分为proposal提取和目标检测，两个部分独立进行。proposal子网络和目标检测子网络结构图分别如下： Object Proposal Network是基于VGG来进行设计的，其像一棵树。主干就是原始的VGG，分支上是另外为了实现多尺度目标检测而设计的网络，构成相同。 注意该网络有多个检测分支，每个检测分支都是最终的proposal检测结果。该网络有个标准的主干CNN，一组单一检测分支。这里在conv4-3之后开始建立检测分支，因为在之前，回传的梯度会对后面的检测结果具有较大影响，造成训练的不稳定。 整个结构算是 faster RCNN 的 multi-scale 版本。因为越浅层的 branch 反馈的梯度，对主干 network 的影响越大。这样会造成训练的不稳定。The Buffer Convolution 层防止 branch 的梯度直接回传到主干上。 1.3.3.1 Multi-Branch Loss $W$是网络参数 $Y_i=(y_i,b_i)$ 是 Ground Truth $X_i$是训练图像块 $a_m$是损失权重 $S=(X_i,Y_i)^N_{i=1}=S^1,S^2,…,S^M$ 是训练样本 $M$ 是检测分支的数量 1.3.3.2 Loss of each detection layerThe loss of each detection layer combines these two objectives. 一个是多类分类损失 一个是定位损失，背景样本不提供定位损失 1.3.3.3 Sampling对于每个检测层，$S^m={S^m +,S^m -}$ Anchor尺寸是与filter的尺寸相关的，anchor 与标记 $IoU \geq 0.5$ 认为是正样本，$≤ 0.2$ 认为是负样本。 对于自然图像，目标和非目标的数量具有极大的不平衡。采样是为了补偿这种不平衡。考虑三种采样策略：随机，bootstrapping，混合。 随机采样获得的样本大量为随机样本，我们知道困难样本挖掘能提升性能 Bootstrapping策略，用物体性得分对样本排序。然后从高到低收集负样本 混合策略，就是随和和bootstrapping一半一半，在我们的实验中其效果和bootstrapping相似 为了保证每个检测层只检测特定尺度范围的目标，训练集按照相应的范围组织。但是，在一张图中，一些尺度可能没有正样本，导致正负样本不平衡，学习不稳定。为了解决这个问题，交叉熵损失修改为： 1.3.4 Object Detection Network RPN本身虽然也可以作为检测器，但滑窗不能很好覆盖目标，所以不是很好。为了提高精度，因此增加了检测网络。ROI pooling层在特征图上提取固定维度的特征（7×7×512）然后送入fc层。这里增加了一个反卷积层来提升特征图分辨率。因此，多任务损失扩展为： 1.3.4.1 CNN Feature Map Approximation在faster RCNN中采用两次上采样的方式将小的目标放大，但是KITTI数据集，其含有大量的小体积物体需要做unsampling. And input upsampling also has three side effects. large memory requirements slow training slow testing 本文考虑了一些 increase the resolution of feature maps 的方式。 Unlike input upsampling, feature upsampling does not incur in extra costs for memory and computation. Our experiments show that the addition of a deconvolution layer significantly boosts detection performance, especially for small objects. 1.3.4.2 Context Embedding（创新点）本文专注于从多个区域中提取语义。在一些研究中[7,8]已经证明其有用了。 结合下图来看，其从目标区域（绿色）和语义区域（蓝色）提取特征，然后堆叠在一起。语义区域是目标区域的1.5倍，后面增加一个卷积层减少通道数，从而保证在不增加模型参数的同时不损失精度。 上下文嵌入的好坏也是一个trade-off的过程，虽然其可以提升性能，但是却几乎是参数数量翻倍了。 1.3.5 Experimental Evaluation1.3.5.1 Proposal Evaluation文章在KITTI和Caltech数据集上做的评测，其表示VOC和ImageNet的图片太小？ KITTI contains three object classes: car, pedestrian and cyclist. 跟随[7]，他们同样训练了一个model来检测car，一个用来检测pedestrian。使用oracle recall用于评估指标。car的IOU大于等于70%，行人、骑车人IOU大于等于50%。 输入图像尺寸的作用：图5显示了proposal网络与输入图像尺寸的关系。行人和车辆的proposal网络对图像尺寸较为鲁棒，这说明，proposal网络不需要增加图像输入尺寸也能得到较好的proposal结果。 独立检测分支的作用：表2显示了不同检测分支与行人高度的检测精度关系。与期望的一致，尺度匹配的精度越高。 与其他先进方法比较： 图像第一排是IoU确定时，召回率与候选样本数量的关系。第二排是100个候选样本，召回率与IoU的关系。MS-CNN在只有100个proposal的情况下实现了98%的召回率。 1.3.5.2 Object Detection Evaluation输入图像上采样的影响 Table 3 shows a significant improvement is obtained by up- sampling the inputs by 1.5∼2 times. 采样策略的区别 Table 3 compares sampling strategies: random (“h576- random”), bootstrapping (“h576”) and mixture (“h576-mixture”) CNN 特征估计 本文尝试了三种方法，分别是 1）双线性差值权重（bilinearly interpolated weights） 2）双线性差值权重初始化，然后通过反向传播学习； 3）高斯噪声初始化，然后反向传播学习； 我们发现第一种方法最好。表3中显示，反卷积能在输入图像尺寸较小时提升。 嵌入语义 嵌入语义信息后，精度也有提升，但是参数会增加。 1.3.5.3 在KITTI和其他方法进行比较MS-CNN使用 h768-ctx-c 模型（没用反卷积，使用的是context encoding和dimensionality reduction）. 2.3.5.4 MS-CNN使用“h720-ctx” 1.3.6 Conclusion 提出了一个统一的深度卷积神经网络MS-CNN，用于快速多尺度目标检测 在多个中间网络层进行检测，使得感知域匹配目标尺寸 探究了CNN特征估计（反卷积），作为输入升采样的另一种选择，能节省计算和内存开销 综上，MS-CNN能实现15fps的检测速度 1.4 Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition1.4.1 Abstract传统的图像识别一般都是识别花、鸟、汽车等不同类别物体，而细粒度图像识别则是要识别同一类物体下的不同子类。举个例子，识别一张图片是猫、狗、汽车还是飞机就是传统的图像识别，而识别一张图片是贵宾犬、边境牧羊犬、吉娃娃还是斗牛犬，则是细粒度图像识别。 细粒度（fine-grained）图像识别主要有两个难点： 类间差异小（都属于同一个物种下的小类） 类内差异大（受姿态、尺度和旋转等因素影响） 这篇文章主要证明 selecting useful deep features 对细粒度识别有很大的作用。文中提出的Mask-CNN模型是“全卷积网络”，并基于part annotations利用了FCN来。 现在图像识别大都使用卷积神经网络CNN，卷积层会针对整个图像（不论是背景还是物体）提取特征，而细粒度图像识别重点在于物体的一些关键部分，如此一来CNN提取的有很多特征向量都是没用的。 本文提出了Mask-CNN模型（M-CNN），它在训练时仅需要part annotations和image-level标签这两个信息。其中part annotations分成两个集合：头部和躯干，如此part localization就成了一个三类分割问题。 完整的网络可见下图，M-CNN是一个四线模型（four-stream），四个输入分别为完整图像、检测到的头部、检测到的躯干和检测到的完整物体，每条线程通过卷积最后都得到了deep descriptors（应该是常说的特征图），进而得到1024-d向量，将四个向量拼接在一起，通过 L2 正则化、全连接层和softmax，最后得到类别。 1.4.2 Learning Object and Part Masks在数据集CUB200-2011中，每个鸟类细粒度图像都有许多part annotations，比如左腿、右腿、喉、喙、眼睛、肚子、前额等等，它们都以key points的形式标注。 本论文将这些key points分成头、躯干两大类，简单地连接这些点来生成头和躯干两个Mask，剩下的都是背景。学习Mask的网络结构如下所示： 以下是部分图像学习到的Mask。红色的称作head mask，蓝色的称作torse mask，这两个合并在一起就是一个完整的物体，称作object mask 1.4.3 Implementation Details Caltech-UCSD 2011 bird dataset有200种鸟类，其中每类都有30个训练图像，每张图像还有15个标注点，用来标记鸟类的身体部位。 四线程网络中每一条都有一个VGG-16模型，其参数通过在ImageNet分类上预训练得到。 作者还用水平翻转的方法使训练图像翻倍。在测试时将原图和对应的翻转图像的预测求平均，并输出得分最高的那个分类。 这个还挺有意思的 直接使用softmax的结果要比使用logistic回归差 1.4.4 Comparision1.4.4.1 Classfication Accuracy这里使用的方法都没有借助于BBOX，直接使用标注的part。 我有点疑惑Bounding BOX对于训练的帮助有多大。 这里也提供了准确率优化的历程，从这里看过来的。 M-CNN准确率的提升之路： 一开始，输入的图像是224*224，M-CNN的准确率有83.1%； 将输入图像变为448*448后，准确率提升到了85.3%； 提高4-stream M-CNN的输入大小到448*448后，准确率反而有些下降； 如果从relu5_2层来提取deep descriptors，并且用Mask过滤一遍，提取出4096-d向量，再和pool5提取出来的拼在一起，变成一个8096-d向量，后续操作相同。该模型称作“4-stream M-CNN+”，它的准确率提升到了85.4%； 用SVD whitening方法将上述的8096-d向量压缩到4096-d，准确率提升到了85.5%； 如果CNN部分采用和part-stacked CNN一样的 Alex-Net模型，准确率只有78.0%，但还是比part-stacked CNN高。关键是替换后的参数只有9.74M了。 1.4.3.2 Part Localization Results本文采用常用的PCP准则（Percentage of Correctly Localized Parts），该准则指的是与ground-truth相比，IOU大于50%的bounding box的比例，下表是和其他方法相比的分割结果。作者的方法和Deep LAC相比躯干的准确率更低，主要是因为Deep LAC在测试时有用到Bounding box，而M-CNN没有。 1.4.4.3 Object Segmentation Performance上图也包含了物体分隔的结果，第二排是ground truth，第三排是M-CNN输出的结果，M-CNN在分割鸟的细微部分（例如爪子）存在一些困难。 1.4.5 Conclusion这篇文章主要证明“selecting useful deep features”对细粒度识别有很大的作用。文中提出的Mask-CNN模型是“全卷积网络”，并基于part annotations利用了FCN来： 定位关键部位（头部、躯干） 生成带weighted object/part mask。 由于丢弃了全连接层，所以Mask-CNN相对于其他算法，速度更快效率更高；在两个鸟类数据集上取得了state of art的结果。 1.5 Mask R-CNN 其实本来是想看这个的，看起来这个是语义分隔，而上篇则是细粒度物体检测。但是也误打误撞的看了Mask CNN，两者似乎除了mask（掩模）这个概念外没有什么直接的联系，看来应该去了解下mask这个概念。 还正在阅读中，留图占坑。 2. Dataset2.1 CityPersons2.1.1 简述CityPersons数据集是脱胎于语义分割任务的Cityscapes数据集，对这个数据集中的所有行人提供 bounding box 级别的对齐性好的标签。 其数据是在3个不同国家中的18个不同城市以及3个季节中采集的，其中单独行人的数量明显高于 Caltech 和 KITTI 两个数据集。实验结果也表明，CityPersons 数据集上训练的模型在 Caltech 和 KITTI 数据集上的测试漏检率更低。表明CityPersons数据集的多样性更强，因而提高了模型的泛化能力。 2.1.2 数据格式 数据集将所有humans分为四类：pedestrian（walking，running，standing up），rider（riding bicycles or motorbikes），sitting person，other person（非正常姿势） 一共5000张图像，35k个行人，13k个忽略区域。 2.1.3 遮挡处理Cityscapes数据集是通过车辆进行采集，其中包含一些著名城市的中心，如法兰克福、汉堡。有些图片中包含100多个行人，每个都有很多的遮挡。如此多的遮挡情况在其他数据集上是很少见的。图4显示了不同遮挡等级的行人的分布。我们注意到Caltech种包含60%的行人是完全可见的，而CityPersons中是30%。这表明，我们有更多的遮挡情况，这也使得我们的数据集对处理遮挡更有兴趣。并且在Resonable子集中，Caltech大部分都是非遮挡行人，而CityPersons得遮挡情况更多。 为了更好的理解那种情况的遮挡更多，我们将所有遮挡模式量化为11种，图5显示了其中的9种。如图5所示。前两种遮挡基本覆盖了resonable，有55.9%。第三、第四个情况是左边或右边遮挡。除了这些之外，还有30%的其他遮挡类型。遮挡类型的分布多样性使得数据集更加具有挑战性。 最后，论文链接在这里：https://arxiv.org/abs/1702.05693 2.2 CalTech-Pedestrian Database2.2.1 简述这个应该是很早期的一个数据集了。 该数据库是目前规模较大的行人数据库，采用车载摄像头拍摄，约10个小时左右，视频的分辨率为640x480，30帧/秒。标注了约250,000帧（约137分钟），350000个矩形框，2300个行人，另外还对矩形框之间的时间对应关系及其遮挡的情况进行标注。 数据集分为set00 - set10，其中set00 - set05为训练集，set06 - set10为测试集（标注信息尚未公开）。 这个数据集似乎被上面的一篇论文diss了，加上时间原因我没有做详细，在此标注一下，以后来补。 下载链接在：http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/ 2.3 KITTI2.3.1 简述该数据集用于评测立体图像(stereo)，光流(optical flow)，视觉测距(visual odometry)，3D物体检测(object detection)和3D跟踪(tracking)等计算机视觉技术在车载环境下的性能。 2.3.2 数据形式数据首先分为下面几个类别： 每个类别有如下的视频流图像： 可以在这里看到更详细的信息：http://www.cvlibs.net/datasets/kitti/raw_data.php 标注如下图的形式： 各个物体类型以及上述标注数据的格式描述如下： 2.4 CalTech Archive最近才发现CalTech除了有Pedestrian的数据之外，这里还有各种各样的数据集。 http://www.vision.caltech.edu/archive.html 比如这些车的数据集，在Car detection中应该会有所用处。 再比如这些传统物体的数据集（下图为CalTech-CUB200），都还挺有趣的。 References How Far are We from Solving Pedestrian Detection? / CVPR 2016 Filtered channel features for pedestrian detection. / CVPR 2015 Is Faster R-CNN Doing Well for Pedestrian Detection? / EECV 2016 Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors) Benenson, R., Mathias, M., Timofte, R., Gool, L.J.V.: Pedestrian detection at 100 frames per second. In: CVPR. (2012) 2903–2910 Girshick, R.B., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: CVPR. (2014) 580–587 Object detection via a multi-region and semantic segmentation-aware CNN model. ICCV(2015) Object proposals for accurate object class detection. NIPS(2015) Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition Mask R-CNN / FAIR 何恺明]]></content>
      <categories>
        <category>Computer Vision</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Joseph Sifakis-如何保障自动驾驶系统的安全性]]></title>
    <url>%2F2018-06-24%2FJoseph-Sifakis-%E5%A6%82%E4%BD%95%E4%BF%9D%E9%9A%9C%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AE%89%E5%85%A8%E6%80%A7%2F</url>
    <content type="text"><![CDATA[被抓壮丁去参与的一次讲座，去之前任何信息未知。 到场才发现是关于自动驾驶的内容，于是很感兴趣的开始记笔记。 因为自己到的比较迟（主要是很嫌弃这种抓壮丁的行为），在最后一排不太看得清楚，但应该是系统容错检测的一些东西，查资料发现lecturer Joseph Sifakis是2007年的图灵奖得主，获奖工作是model checking，现在他在自动驾驶方向上的研究也是和整车系统有关的。 讲课的过程很匆匆，估计他发现听众都不太明白他的深意，很多细节性的东西都很快的跳过了，我及时拍照做了一下记录（或者是纪念把）。 这是匆匆记下的一些笔记 最后交流过程还蛮长的，我留下来听了很久Joseph的答疑，问了一些关于当今自动驾驶技术的挑战和难点。 他强调的重点是data based learning，以及如何让每一台车及时获得更新的数据或者是模型，来减少这辆车还未遇见过的bad situations。 席间有个一同问问题的EE的女生口语真的很好，对比起来我都不太好意思开口，不过Joseph是个法国人，说英语也挺含糊的。 讲座时坐我旁边的一个哥斯达黎加的小哥，最后也留下来等Joseph，他是一个很年轻的教授，头发也很长。来的目的主要是想邀请Joseph去哥斯达黎加做演讲，最后Joseph表示有意愿。 一直接话的印度人有所耳闻，很牛逼的一个人，在工大做研究，但是同时和MIT在合作一个项目，未来会去硅谷。 真是羡慕死了！ 最后快要走的时候我又问了Joseph，世界上除了Tesla和Waymo还有哪家自动驾驶技术研究的很牛逼，他给出的答案也符合我的预期，就是关注传统车企，他比较看好Volvo. 然后关于学校和国家，回答是美国or欧洲的名校，显然他的领域里似乎加澳已经被抛弃了。]]></content>
      <categories>
        <category>Autonomous Driving</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Review for Week-9：RCNN(Regions With CNN Features)]]></title>
    <url>%2F2018-05-02%2FReview-for-week-9-RCNN%2F</url>
    <content type="text"><![CDATA[从RCNN开始接触目标检测，这个东西起源于Rich feature hierarchies for Accurate Object Detection and Segmentation，在2014年的CVPR上发布，不得不感慨，发展的好快啊。 论文链接如下： https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf?spm=5176.100239.blogcont55892.8.pm8zm1&amp;file=Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf 基础知识无监督预训练网络越深，需要的训练样本数越多。 若用监督则需大量标注样本，不然小规模样本容易造成过拟合，深层网络意味着特征比较多，机器学习里面临多特征：1、多样本. 2、规则化. 3、特征选择. 无监督预训练（unsupervised pre-training）即训练网络的第一个隐藏层，再训练第二个…最后用这些训练好的网络参数值作为整体网络参数的初始值。 常用的方法有：stacked sparse-autoencoder，stacked denoise-autoencoder AutoEncoder什么是autoencoder：自编码器尝试学习出一个 $h_{w,b}(x) = x$ 的函数。 Autoencoder 可以学习到数据的一些压缩表示：例如如果输入数据为100维，隐藏层为50个，那么就需要从这50维的数据中重构出100维的输出，使这个输出接近于100维的输入。因此这个隐藏层的50维的数据就必然会包含着输入数据的一些相关性。所以说autoencoder就是为了学习到输入数据的相关性表示的一种方法。 监督下的Fine-tuning利用已有模型训练其他数据集，去掉最后一层，然后其它的网络层参数就直接复制过来，继续进行训练。 有点像迁移学习 IOU的定义IOU定义了两个bounding box的重叠度，计算公式为重叠面积占并集面积的比例，如下图所示： 非极大值抑制这个还是很有意思的，RCNN会从一张图片中找出n多个可能是物体的矩形框，然后为每个矩形框为做类别分类概率。 先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。 从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值. 假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。 从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。 就这样一直重复，找到所有被保留下来的矩形框。 RCNN对于物体检测，其需要定位出物体的位置，把它看做是一个回归问题：求解一个包含物体的方框。 简述RCNN所作的事情就是。 首先输入一张图片，我们先定位出2000个物体候选框。 采用CNN提取每个候选框中图片的特征向量，特征向量的维度为4096维。 采用svm算法对各个候选框中的物体进行分类识别。 流程如图所示 候选框搜索搜索出所有可能是物体的区域，采用传统文献的算法search for object recognition，通过这个算法我们搜索出2000个候选框。 因为候选框长宽不一定相等，因此我们接下来进行缩放。 各向异性缩放 这种方法很简单，就是不管图片的长宽比例，管它是否扭曲，进行缩放就是了，全部缩放到CNN输入的大小227*227，如下图(D)所示； 各向同性缩放 因为图片扭曲后，估计会对后续CNN的训练精度有影响，于是作者也测试了各向同性缩放方案。这个又分两种做法： A、直接在原始图片中，把bounding box的边界进行扩展延伸成正方形，然后再进行裁剪；如果已经延伸到了原始图片的外边界，那么就用bounding box中的颜色均值填充，如下图(B)所示; B、先把bounding box图片裁剪出来，然后用固定的背景颜色填充成正方形图片(背景颜色也是采用bounding box的像素颜色均值)，如下图(C)所示; IOU评价候选框位置在CNN阶段，如果用selective search挑选出来的候选框与物体的人工标注矩形框的重叠区域IoU大于0.5，那么我们就把这个候选框标注成物体类别，否则我们就把它当做背景类别。 网络结构文章是直接用Alexnet的网络，初始的参数值也是Alexnet已经训练好的模型参数，然后再fine-tuning训练。Alexnet特征提取部分包含了5个卷积层、2个全连接层，在Alexnet中p5层神经元个数为9216、 f6、f7的神经元个数都是4096，通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量。 在Fine-Tuning阶段：假设要检测的物体类别有N类，那么我们就需要把上面预训练阶段的CNN模型的最后一层给替换掉，替换成N+1个输出的神经元(加1是表示还有一个背景)。 开始的时候，SGD学习率选择0.001，在每次训练的时候，我们batch size大小选择128，其中32个正样本、96个负样本。 SVM训练完CNN之后（仅用作特征提取），最后加一层SVM用作分类。 CNN在训练的时候，对训练数据做了比较宽松的标注，比如一个bounding box可能只包含物体的一部分，那么我也把它标注为正样本，用于训练cnn；采用这个方法的主要原因在于因为CNN容易过拟合，所以需要大量的训练数据，所以在CNN训练阶段我们是对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5都被标注为正样本了)。 理想情况是，只有当bounding box把整量车都包含在内，那才叫正样本。如果bounding box 没有包含到车辆，那么我们就可以把它当做负样本。 但是我们还存在很多非理想的case，因此需要使用到IOU，作者测试了IOU阈值各种方案数0,0.1,0.2,0.3,0.4,0.5。最后我们通过训练发现选择IOU阈值为0.3效果最好。 输入是f7的特征，f7的输出维度是2000 x 4096，输出的是是否属于该类别，训练结果是得到SVM的权重矩阵W，W的维度是4096 x N，N是特征类别数目. 最近比较忙，未完待续~争取接下来做下实现。]]></content>
      <categories>
        <category>Computer Vision</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Review for Week-9：cs231n-Lecture-3]]></title>
    <url>%2F2018-05-02%2FReview%20for%20week-9%EF%BC%9Acs231n-Lecture-3%2F</url>
    <content type="text"><![CDATA[Lec-3回顾这节课内容很多，主要在讲loss function和optimization. Loss functionQuantifies our unhappiness with the scores Hinge Loss我们希望在预测正确时的惩罚为0，在预测错误时加大惩罚，但为了获取更好的泛化能力，不要过多的无限放大失误。 Regularization在Hinge Loss当中，当W是一个最优解时，那么2W也是一个最优解。从情理上我们希望选取更小的W数值会获得更好的泛化效果（奥卡姆剃刀原则），由此引入了正则项。 还有各种花样的正则方式，但注意这里L1正则和L2正则的区别（onenote里面记载过一段） Softmax从概率的角度，最大化likelyhood，注意这里的log函数的来源（图形非常利于迭代），也算是很受教了。 Softmax的计算过程如下： 最后就是注意区分score function和loss function，有助于理清楚算法框架。 OptimizationGradient Descent这里提到了两个词，Numerical Gradient和Analytic Gradient. （上学期某丁就用前者方法在做GD，当时我还不知道有前者这个说法，把他批判了一番）. 前者的主要用处是验证后者的导数求对了没。 SGDmini-batch，通常选32，64，128这样的数。 FeatureMotivation让数据尽量的线性可分，或者是更容易划分，可是似乎通常难以实施。 其中包括Color Histagram方法，给图片主色调分类，当成新的维度。 HoG 方法，根据人体视觉系统得来。 Bag of words，词袋模型那里衍生过来的，利用给定的image codebook做一次编码。 总结机器学习方法+特征提取是较早采用的方法，目前流行的ConvNet，只要层数足够深，可以直接在raw data中去学习出这些特征。 不知道ConvNet是否真的不需要这些特征提取所做的工作？]]></content>
      <tags>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Review for Week-9：cs231n-Lecture-2]]></title>
    <url>%2F2018-05-01%2FReview%20for%20week-9%EF%BC%9Acs231n-Lecture-2%2F</url>
    <content type="text"><![CDATA[Lec-1是纯入门，就不做记录了。 Lec-2回顾Challenges of recongnition illumination deformation occlusion clutter intraclass variation Nearest Neighbors一种很神奇的初级思路，train只需要$O(1)$，predict需要$O(N)$. 需要被预测的对象与训练集根据Metric作差，值最小的那个训练集对象的类别即是所预测的类别。 进一步采用KNN，K个最近邻的对象，对类别投票。 以特征维度划分空间，其中可能存在空白区域，这些地方是算法的短板，做随机的预测。 Distance Metric L1，就是曼哈顿距离 L2，几何距离 其中关于这两个距离需要注意的是它们的衡量对象，课程中强调了，L1距离所代表的是一维尺度（提到depends on coordinates）上的距离，比如一个老师对象，拥有很多一维属性。比较老师和老师的不同可以采用L1距离。 L2距离则考量的是二维上尺度，较适合图片这种点与点关联属性比较大的对象。 至于效果嘛，显而易见，随缘。课程也提到了，希望设计的机器学习算法，将predict和train分开，将load放在train这个环节，服务商自己使用服务器做，不去麻烦使用服务的客户。 Curse of Dimensionality维度灾难，上ML课的时候一直没听懂，现在好像有点感觉了。 维度的增加，导致KNN这种方法的运算量呈指数倍增。 Linear Classfication课程暂时没细说，但应该就是Logistic Regression, SVM做多分类。 线性分类器和神经网络是可以组合的，当做lego brick一样使用，怎么组合暂时还不是太理解。 Li Fei Fei做的图片的语义理解，这个真的厉害。]]></content>
      <categories>
        <category>Computer Vision</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2018Spring-Week7-Anomaly Detection Review]]></title>
    <url>%2F2018-04-12%2F2018Spring-Week7-Anomaly-Detection-Review%2F</url>
    <content type="text"><![CDATA[对目前工作的思考相关工作简述 There are a variety of models that capture and produce such a distribution, each fitting different types of time series signal behaviors. For example, time series models such as HoltWinters, ARIMA models, and Hidden Markov Models, all capture temporal dynamics of a time series and produce a generative distribution for predicting the ranges of future values. But can these models be used for any type of signal? The answer is no - each model comes with its own assumptions, affecting what type of signal can be fitted well with it. For example, ARIMA and Holt-Winters models assume the the time series is ergodic, with a noise model that is Gaussian. A Markov model makes assumption on the memory of the process (current state depends only on previous state), and so on. Let’s say the definition of an anomalous data point is one that deviates by a certain standard deviation from the mean. $P_{Normal}(X_t | X_{t-1}, \ldots X_0)$ such that we can determine the likelihood of the data point $X_{t+1}$ to have originated from $P_{Normal}(X_{t+1} | X_{t+1} , X_{t-1}, \ldots X_0)$. Once we choose the model that would describe the metric the best, we must fit the best model parameters for it. Depending on the type of model, this step may take different forms - we use maximum likelihood estimation (MLE) for this step. 关注到一个做异常检测的公司：Anodot Anodot Real Time Anomaly Detection 数据提取-数据分析-异常检测-报告提醒，一条龙服务的公司 有一篇相关的论文写得非常的好，Real-time anomaly detection system for time series at scale, KDD 2017 思路修正 Seasonality detection可能是有必要的，不能简单地说机器的数据没有季节性。 The purpose of seasonality detection is to automatically find the seasonal patterns present in the time series. But we are facing to several challenges. In the context of business data, typical seasonal periods are linked to human activity : weekly, daily, hourly, etc. The range of possible seasons is large : from 10^3s to 10^6s, which is three order of magnitude - this imposes a challenge to standard algorithms. An algorithm must be robust to anomalies. One cannot assume that the anomalies are not present in the data used to detect seasonal patterns. Multiple seasonal patterns in the time series, at low and high frequencies are challenging as high frequency seasonal patterns may prevent accurate detection of the lower frequency patterns (which we care about the most for time series modeling). An algorithm must be efficient in CPU and memory, otherwise it cannot be applied to millions of metrics. The algorithm must be robust to various types of unexpected conditions, such as missing data, pattern changes, etc. 其中对我们来说最主要的困难是，1和2。即假设异常存在的情况如何检验周期性。 目前的方法主要还是ACF那一套（对序列做频解分析），不是很熟，得仔细思考怎么应用。 其余方法 简单的情况：SMA+LOW FILTER. 复杂一点的方法：Moving Average Using Discrete Linear Convolution SCREEN：有用，但是处理过程稍显复杂，可以尝试用它来做比较 咱们的模型使用的是统计学方法，数据假设服从高斯分布，当数据偏离中心过多时，被标注为异常， 这里说明了这类生成式模型（不知道可不可以这么说）的数学应用背景。 异常情况分类 Point anomalies: A single instance of data is anomalous if it’s too far off from the rest. Business use case: Detecting credit card fraud based on “amount spent.” Contextual anomalies: The abnormality is context specific. This type of anomaly is common in time-series data. Business use case: Spending $100 on food every day during the holiday season is normal, but may be odd otherwise. Collective anomalies: A set of data instances collectively helps in detecting anomalies. Business use case: Someone is trying to copy data form a remote machine to a local host unexpectedly, an anomaly that would be flagged as a potential cyber attack. Normal Behavior分类 异常检测方法的分类（可以用作论文补充） Statistical Methods Holt-Winters Chatfield (1978). classical ARIMA and seasonal ARIMA models Chatfield (2016). Density-Based Anomaly Detection Clustering-Based Anomaly Detection Support Vector Machine-Based Anomaly Detection Moving Average Using Discrete Linear Convolution 异常检测的用途Some of the important applications of time series anomaly detection are : Detecting anomalous heart beat pulses using ECG data (8; 9) : Usually ECGdata can be seen as a periodic time series. An anomaly in this case would be thenon-conforming pattern e.g., in terms of periodicity or amplitude, which couldindicate a health problem. Attack detection in recommender systems : Shilling attacks, in which the attackersintroduce biased ratings in order to influence future recommendations (10). Detection of anomalous flight sequences using sensor data from aircrafts: Typicalsystem behavior of flights is characterized by the sensor data information of dif-ferent parameters which change during the course of flight. Any deviation fromthe typical system behavior us anomalous (11). Shape anomalies : Finding the shapes which interestingly differ from others, whereeach shape is converted to a time series (12; 13). In the field of medical datamining, given several shapes of a species, a shape that differs from others mightindicate an anomaly caused by genetic mutation. In anthropological data mining,different shapes of interest can be pottery, bones, arrowheads etc. Outlier light curves in catalogs of periodic stars : Detection of outliers in periodicvariable stars involves finding the statistical deviance from the rest. The out-liers correspond to some interesting intrinsic physical differences, such as slowly changing period or amplitude, which introduce noise in the light curve. Eco-system disturbances using earth science data such as vegetation or temperature. LSTM用法Training with sliding windows 这是Uber的做法，https://eng.uber.com/neural-networks/ The training dataset for our neural network required sliding windows X (input) and Y (output) of desired lag (e.g., input size), as well as a forecast horizon. Using these two windows, we trained a neural network by minimizing a loss function, such as Mean Squared Error. Both the X and Y windows slide by a single increment to generate training data, as demonstrated below:]]></content>
      <categories>
        <category>异常检测</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[随想-皇后大道东]]></title>
    <url>%2F2018-03-30%2F%E9%9A%8F%E6%83%B3-%E7%9A%87%E5%90%8E%E5%A4%A7%E9%81%93%E4%B8%9C%2F</url>
    <content type="text"><![CDATA[有人称香港回归，有人称香港沦陷。 皇后大道東-公視 閃亮的年代 - 羅大佑音樂特輯 序言找到这首歌的原因是因为偶然了解到，在香港中文大学的讲座中，林夕称自己后悔写《北京欢迎你》这首歌。 作为从小听这首歌长大的我，深喜爱其旋律，歌词，和倾举国之力参与的演员阵容。当然随着我慢慢的长大，我也渐渐的明白，这首歌背后投射出的更是，整个中国在2008年举办奥运会的过程所展示出来的巨大的政治能量。 要知道那一年的南方雪灾，汶川地震，又逢首次举办夏季奥运会。多难虽不一定能推导出兴邦，但是经历过大的磨炼，确实能够锻炼出人与国，共同的韧性。 所以自然我是不能理解林夕的话的。 但林夕说，『文学创作追求的最高境界是绝对自由的意志，「北京欢迎你」虽非什么作奸犯科之作，并非在出卖什么，但是做了一次官方喉舌之后，却也给了自己很多反省。』 皇后大道东这首歌的演唱者为罗大佑，很巧合的是，罗大佑也正好有两首态度相反的歌，一首便是官方用作宣传的『东方之珠』，而另一首便是这首恨不得处处封禁的『皇后大道东』。 『皇后大道东』是一首显然的带有政治讽刺色彩的作品。 政治在中国是一个敏感的话题。从政治上要打倒一个人也是最容易的。就像是历史上发生过的那么多的事情和今天也在发生的事情，我个人认为国家是不应该试图从政治上清算一个文人的，他写的东西是他的思考，他反对的是他不喜欢的政治。当然今天的国家在思想言论上所做的事情，也是出于国家层级的考虑，因为迷茫的大众看不清世界，容易被文人的东西给误导。 『皇后大道东』中写道： 有个贵族朋友在硬币背后，青春不变名字叫做皇后（香港硬币背面是英女皇）。 到了那日同庆个个要鼓掌，硬币上那尊容变烈士铜像（按时硬币上会换成毛主席的头像）。 会有铁路城巴也会有的士，但是路线可能要问问何事（97之后路线方向未知，表达作为港人的迷茫）。 我在知乎上看人写道，安拉大神的答案 硬币上那尊容变烈士铜像，一句概括足矣。 最矛盾的地方，就是歌名皇后大道东。取名来自英女王，回归后，这种大逆不道的名字会否被改？硬币上的英女王头像，会否被变成烈士头像？（最后证实变了紫荆花） 回归对于香港人，并不是那么普天同庆的事情，时至今日，还依稀可见矛盾。记得看过李碧华的书，很多都映射临近回归的那些年，是香港的移民潮，有能力移民的人，全都移民国外。 人类天生对未知的事物恐惧，天生害怕改变，更何况是一个崭新的政体，这种特区形式，在以前从未有过。 彷徨、担忧、疑问。充满自由意识的香港人，选择在文艺作品中体现自己的体会和诉求。 歌词中有碎碎念的片段，色即是空，空即是色。对于97年的香港，皇后大道中五光十色的夜景，那么这种色经过97回归以后，会不会变成像97年的大陆的景象，繁华全部变成一场空呢？ 后记20年后发生的事情，我们都已然知晓。 虽然也总是发生各种『港独』的事件，但作为一个认知能力已经达到成年的我来说，我也逐渐理解他们在争取什么，他们在渴望着什么。 走过了很多地方，中国的日渐强大与繁荣是能够让世界和我们共同感受到的，虽然还是各种各样的精日精美，低素质的游客等等。但我认为国人的主核力量和思想在逐步的提升，当你和你的国家都做的好足够好的时候，你会骄傲于你来自哪里。 无论你来自香港，台湾，还是大陆，在历史的各种危难时刻，我们的命运都是相通的，因为每一张黄色面孔下血脉相连的搏动频率，其实正扣连着整个种族在大时代的归属渴求。]]></content>
      <categories>
        <category>随想</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2018Spring-Week4-Anomaly Detection Review]]></title>
    <url>%2F2018-03-25%2F2018-Week4-Anomaly-Detection-Review%2F</url>
    <content type="text"><![CDATA[第四周周报伪代码研究了论文格式的伪代码书写，使用latex重写了伪代码的部分，具体如下图。 注：这里使用的是全局数据版本（比如直接一个数组，从0-&gt;length这样读取），没有表现出来我们算法的在线特征，当然也可以写成在线处理数据流的版本，一般对于流处理数据算法的相关论文中都是写全局版本，然后强调online的特征，不知道我们的伪代码应该写哪个版本的更合适。 宋老师论文选读这周主要读了他的两篇论文。 Time Series Data Cleaning : From Anomaly Detection to Anomaly Repairing-VLDB SCREEN: Stream Data Cleaning under Speed Constraints-SIGMOD 我先读的第二篇也是一种名为SCREEN的在线检测的算法，其中提到了异常检测读的很详细（一字一句的读了一半多）。对于第一篇论文，我粗略的看了Intro，也是关于在线算法，其中提到了很多很传统的模型，如AR等来做异常修复（注意他更多的考虑是修复，因为他认为检测比较好做，修复更有挑战）。 我马上把第二篇 2015 年的读完了再读 2017 年的第一篇论文，想看下宋老师这两年来用同一种思路做异常修复问题的进展和变化。 下面主要介绍，也算是记录第二篇论文给我的启发。 论文首先将Time Series所面对的Anomaly Detection刻画为一种优化问题，将异常数据设为Dirty，用X表示，我们使用Smooth数据作为修复，用X&#39;表示，其中 $min(\Sigma(X’_i - X_i))$ 即为我们要面对的优化函数。 然后在这个优化目标的基础上作者提出了Speed Constraints的概念，这个概念主要是去惩罚上升或者下降过快的数据，将其判定为异常点并进行修复。 其中Speed Constraints的概念简述如下： 定义 $S = (S_{min},S_{max})$ 是在窗口大小为 $w$ 下的约束。 我们称序列 $X$ 满足约束为 $X \models S$ ，这当且仅当在一段窗口内 $i,j \in w , j-i \leq w $，存在 $S_{min} \leq \frac{X_j - X_i}{j-i} \leq S_{max}$ 其中定义局部最优为对单点的最优修复，误差最小，全局最优为对所有点的误差求和最小。 根据 Speed Constraints 的概念，我们可以将优化函数写成如下的形式，即满足修复在速度约束下达到全局最优。 后面他根据Median Principle的思想通过讨论上述优化函数的近似解去得到了一种Smooth的方法（似乎是通过MA的方法求出Smooth值，然后取Smooth和Dirty值的中点作为修复值），我还没有看完论文，所以这部分就先不详细描述。 令我感兴趣其实更多的不是Speed Constraints这个检测异常的方法，而是后续的实验设计，如何的去量化指标，这些是对我有启发的。 因为单纯看待Speed Constraints这个方法，其实其粒度是要比我们的EA更粗，我暂且并不认为SC方法用于异常检测的效果会比EA更好，当然这得依赖于我们做下一步的实验。这些东西以及论文中对于实验的设计对我的启发会在下一个部分以列表的方式讨论。 此外作者讨论了目前大数据异常检测，尤其是在线算法所面临的几个挑战： 数据噪声：数据噪声太大，对数据的标记不足，缺少先验知识，无法获取正常的数据。 这点我是认同的，能够使用的自然数据的数据标记不足，只能靠人工加入噪声的方式，但人工加入异常存在着隐患，因为你并不清楚加入的这一段区间原本是异常的还是正常的，亦或是正常+异常间隔着的，因为我们使用的在线方法的基础是统计学，这些方法的假设都是数据与历史信息存在很深的关联的，可能对于t点，t-m点的值变动，或多或少都会影响到t点值的计算。 在线计算：在线计算的优势是时间效率，但考虑的是局部最优，并不能向离线算法一样结合当前点之后的信息去做全局的最优化。 乱序到达：这点是指网络延迟中，1，2，3时刻的数据到达的顺序可能为2，1，3，在这种情况如果使用原有的在线算法会导致数据点的缺失，从而影响学习的效率。 吞吐量：这点涉及到对window size的选取，主要是考虑到传感器上的硬件的缓存，如果window size过大传感器会存不下那么多数据点，如果过小会影响学习的效率，作者在论文中专门做了adaptive window size的实验，利用数据说明了如何选取window size，这一点也是我们应该学习的。 对比实验设计目前想到的主要思路有以下几点 学习上述论文，做对于关键参数选取的实验，在论文中说明如何adaptive的选取最优化的参数。 对对比实验环节中，异常检测的结果做量化，比如一段区间内人工加入的噪声点数为 $W$，我们比较各类算法在区间内检测到的异常点 $w$ 的大小，越接近 $W$ 代表算法效果越好。 第五周时做和 SCREEN 的对比实验，似乎作者论文中在和其他的几种方法对比，我继续研读后设计一个加入了我们的算法的对比实验，以第二点提到的做法作为量化指标。]]></content>
      <categories>
        <category>异常检测</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Latex书写伪代码的模板-Algorithmic和algorithmicx]]></title>
    <url>%2F2018-03-25%2FLatex%E4%B9%A6%E5%86%99%E4%BC%AA%E4%BB%A3%E7%A0%81%E7%9A%84%E6%A8%A1%E6%9D%BF-algorithmic%E5%92%8Calgorithmicx%2F</url>
    <content type="text"><![CDATA[现在正在写时间序列的那篇论文，想要学习标准论文的伪代码写法，就上网查到了这两个包，做一个记录，其他朋友遇到使用困难也可以一起讨论。 下面是标准文档： algorithmicx : http://tug.ctan.org/macros/latex/contrib/algorithmicx/algorithmicx.pdf algorithmic : https://en.wikibooks.org/wiki/LaTeX/Algorithms 其实学习了才发现，algorithmic和algorithmicx的很多命令都是一样的，只是algorithmic的命令都是大写，algorithmicx的命令都是首字母大写。下面是algorithmic的基本命令123456789101112131415\STATE &lt;text&gt;\IF&#123;&lt;condition&gt;&#125; \STATE &#123;&lt;text&gt;&#125; \ELSE \STATE&#123;&lt;text&gt;&#125; \ENDIF\IF&#123;&lt;condition&gt;&#125; \STATE &#123;&lt;text&gt;&#125; \ELSIF&#123;&lt;condition&gt;&#125; \STATE&#123;&lt;text&gt;&#125; \ENDIF\FOR&#123;&lt;condition&gt;&#125; \STATE &#123;&lt;text&gt;&#125; \ENDFOR\FOR&#123;&lt;condition&gt; \TO &lt;condition&gt; &#125; \STATE &#123;&lt;text&gt;&#125; \ENDFOR\FORALL&#123;&lt;condition&gt;&#125; \STATE&#123;&lt;text&gt;&#125; \ENDFOR\WHILE&#123;&lt;condition&gt;&#125; \STATE&#123;&lt;text&gt;&#125; \ENDWHILE\REPEAT \STATE&#123;&lt;text&gt;&#125; \UNTIL&#123;&lt;condition&gt;&#125;\LOOP \STATE&#123;&lt;text&gt;&#125; \ENDLOOP\REQUIRE &lt;text&gt;\ENSURE &lt;text&gt;\RETURN &lt;text&gt;\PRINT &lt;text&gt;\COMMENT&#123;&lt;text&gt;&#125;\AND, \OR, \XOR, \NOT, \TO, \TRUE, \FALSE 下面是algorithmicx的模板 1234567891011121314\State &lt;text&gt;\If&#123;&lt;condition&gt;&#125; &lt;text&gt; \EndIf\If&#123;&lt;condition&gt;&#125; &lt;text&gt; \Else &lt;text&gt; \EndIf\If&#123;&lt;condition&gt;&#125; &lt;text&gt; \ElsIf&#123;&lt;condition&gt;&#125; &lt;text&gt; \Else &lt;text&gt; \EndIf\For&#123;&lt;condition&gt;&#125; &lt;text&gt; \EndFor\ForAll&#123;&lt;condition&gt;&#125; &lt;text&gt; \EndFor\While&#123;&lt;condition&gt;&#125; &lt;text&gt; \EndWhile\Repeat &lt;text&gt; \Until&#123;&lt;condition&gt;&#125;\Loop &lt;text&gt; \EndLoop\Require &lt;text&gt;\Ensure &lt;text&gt;\Function&#123;&lt;name&gt;&#125;&#123;&lt;params&gt;&#125; &lt;body&gt; \EndFunction\State \Return &lt;text&gt;\Comment&#123;&lt;text&gt;&#125;]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-Add Two Numbers]]></title>
    <url>%2F2018-03-22%2FLeetCode-Add-Two-Numbers%2F</url>
    <content type="text"><![CDATA[前段时间为了找实习，开始准备刷LeetCode（以下简称LC），虽然只写到了第二题就拿到了滴滴视觉计算团队的CV实习offer，很开心，也就不打算再密集型的锻炼代码，可以开始看CV相关的论文和书籍了。 但是总体感觉LC题的质量还不错，比较偏工程应用，也不会像传统的ACM题那样有着很长的题干，就是输入输出，简单直接，很适合用来做C++的暖手。 比如这道题就让我重新熟悉了指针和链表的一些操作。 一开始其实很简单，多项式大数求和，但是我快忘了链表操作，而且还有很细的边界条件需要考虑。 比如这几种 Test case Explanation l1=[0,1] l2=[0,1,2] When one list is longer than the other. l1=[] l2=[0,1] When one list is null, which means an empty list. l1=[9,9] l2=[1] The sum could have an extra carry of one at the end, which is easy to forget. 还有就是对链表头的处理，开始我一直在考虑应该在循环内p = new ListNode()还是应该p-&gt;next = new ListNode()。 前者在循环外初始化需要使用这种方式，但是这样是有问题的，因为p3获取不到h3的地址（h3根本也没地址）。 12ListNode *h3 = nullptr;ListNode *p3 = h3; 正确的方式应该是 12ListNode *h3 = new ListNode(0);ListNode *p3 = h3; 但是这样就会存在问题，比如我每次都在循环内执行 p-&gt;next = new ListNode()，那么对于加到最后一个数时，实际上我是多初始化了一个结点的。还需要保存当前访问节点的前置指针，可行但是有点麻烦。 后来看到题解里使用了dummyHead这样一个思想，就是头结点不存信息，指向的第一个结点是真正的头。 这样就可以一致化的使用下面的方式初始化结点了。 1234567ListNode *h3 = new ListNode(0);ListNode *p3 = h3;......p3-&gt;next = new ListNode(temp_sum);......//最后注意return h-&gt;next; 刚看到时，觉得天秀，但其实本来就是数据结构课里讲过的东西，当时的作用是为了统一化删除操作，不用区分是删到了头结点还是其他结点。 题解如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Solution&#123; public: ListNode *addTwoNumbers (ListNode *l1, ListNode *l2) &#123; ListNode *p1,*p2; p1 = l1; p2 = l2; ListNode *h3 = new ListNode(0); ListNode *p3 = h3; int carry = 0; while(p1 != nullptr || p2 != nullptr) &#123; int temp_sum = carry; if (p1 != nullptr) &#123; temp_sum += p1-&gt;val; &#125; if (p2 != nullptr) &#123; temp_sum += p2-&gt;val; &#125; if (temp_sum &gt;= 10) &#123; temp_sum -= 10; carry = 1; &#125; else &#123; carry = 0; &#125; p3-&gt;next = new ListNode(temp_sum); if (p1 != nullptr) &#123; p1 = p1-&gt;next; &#125; if (p2 != nullptr) &#123; p2 = p2-&gt;next; &#125; p3 = p3-&gt;next; &#125; if (carry == 1) &#123; p3-&gt;next = new ListNode(carry); &#125; return h3-&gt;next; &#125;&#125;;]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Network-Week1-Questions]]></title>
    <url>%2F2018-03-11%2FNetwork-Week1-Questions%2F</url>
    <content type="text"><![CDATA[每次Coursera上最难完成的就是周末习题。 这次的计网习题涵盖了第一周各种小的知识点，记下来方便回顾。 A network segment where only one device can communicate at a time is known as a collsion domain. A device that knows how to forward traffic between independent networks is known as a router. When data can flow across a cable in both directions, this is known as duplex communication. MAC address stands for media access control address. The number system that has 16 numerals is known as hexadecimal. The transmission method that sends data to every device on a LAN is known as a broadcast transmission. The technique that allows you to have multiple logical LANs operating on the same physical equipment is known as a VLAN. The most common data link layer protocol for wired connections is ethernet. Something that requests data from a server is known as a client. A device that connects lots of devices and remembers which ones are connected to each interface is known as a switch. When an electrical pulse on one wire is accidentally detected on another nearby wire, this is known as crosstalk. The first part of an Ethernet frame is known as the preamble. A defined set of standards that computers must follow in order to communicate properly is known as a protocol.]]></content>
      <categories>
        <category>Network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Network-Week1-Questions]]></title>
    <url>%2F2018-03-11%2FNetwork-Week1-Questions~%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[随想-「方便」的暴政]]></title>
    <url>%2F2018-03-09%2F%E9%9A%8F%E6%83%B3-%E6%96%B9%E4%BE%BF%E7%9A%84%E6%9A%B4%E6%94%BF%2F</url>
    <content type="text"><![CDATA[本文原载于 nytimes，作者 Tim Wu，由 ONES Piece 翻译计划翻译。ONES Piece 是一个由 ONES Ventures 发起的非营利翻译计划，聚焦科技创新、生活方式和未来商业。 当今之世，「方便」是最被低估，也是最少被理解的力量。作为人类决策中的驱动力之一，它或许没法提供弗洛依德笔下的无意识性渴求那般背德的刺激，也缺少经济学动因分析那种数理式的优雅。「方便」很无聊，可无聊并不意味着无足轻重。 在二十一世纪的发达诸国，方便——亦即更高效简单地完成个人事项的方式——可能渐已成为塑造个人生活乃至整体经济的最强大力量。在美国尤其如此。虽然欢颂自由与独立的赞歌于美国四处可闻，但你也许会怀疑是否方便才是事实上的王道。 正如 Twitter 的联合创始人伊文 · 威廉姆斯（Evan Williams）近日所指的那样：「方便决定一切。」鼓吹着「人们的想象揭示了他们真正想要的东西」，「方便」似乎已令我们别无选择。（我喜欢手冲咖啡，但星巴克速溶是如此方便，以至我几乎从不去做我「喜欢」的选择。）简单就是好。简单至极，则再好不过。 「方便」有着令其他选项变得不堪设想的能力。一旦用过洗衣机，再去手洗衣物便显得匪夷所思，就算它也许更便宜。一经体验流播电视，则守着特定时间看剧的做法就似乎蠢得有点不像样。要抵制「方便」——不拥有手机，不使用谷歌搜索——所需的牺牲，是常被认作举止乖僻，甚至走火入魔。 「方便」对个体决策形成过程的影响力，生发自日积月累的集体决策过程，也源于它在现代经济构建中的诸多所为。特别是在技术相关行业，打赢「方便」之战就意味着市场占有率的胜利。 美国人总把他们嘉勉竞争、选择极大丰富、「小人物梦想」之类的话挂在口上。而在规模经济和习惯的力量互相加成下，我们对方便的胃口会招致来更多的「方便」。亚马逊用起来越方便，它即会变得越强大——因此亚马逊用起来就愈加方便了。「方便」与垄断，似天然乃一丘之貉。 既然「方便」作为一种理想、一派价值观、一类生活方式业已风生水起，那么就值得一问，我们对它的固恋，于我们自己和我们的国家有何影响。我并不想把「方便」说成是邪恶的力量，「省烦从简」本身亦无可指摘。恰恰相反，它时常开启那些过去令人目迷五色的可能性。而对那些不堪挑起人生重担的人而言，「方便」总能让日子过得不那么费劲些。 可我们错就错在，只因它与我们所珍视的其他理想之间关系错综复杂，便假定「方便」永远都是好的。固然「方便」总被理解、被宣传为平权解放的工具，但其亦有昏昧的一面。它应许我们以顺畅无阻和事半功倍，可也威胁到了那些给予生活以意义的挣扎与挑战。「方便」被创造出来给予我们自由，但它也限制了我们的意愿，由此微妙地奴役了我们。 将拥抱「不便」作为处世原则诚然会有悖常理，可当我们纵容「方便」做出一切决定之时，我们妥协得委实太多。 我们现在所知的「方便」，乃十九世纪末二十世纪初，节约劳力的家用设备被创造并推向市场时的产物。个中重要节点，包括首批「方便食品」的发明（例如罐装猪肉焗豆子和桂格即食麦片）、第一代电动洗衣机、「老荷兰」去污粉之类的清洁产品以及其他新奇事物，比如电动吸尘器、蛋糕预拌粉和微波炉。 「方便」是另一个十九世纪末的概念「工业效率」的家用版本，且多与「科学管理」一齐出现。它代表着工厂时代的精神风貌对家庭生活的浸染。 别瞧它眼下何等稀松平常，「方便」这位免人类于劳作的解放者，曾经还是个乌托邦式的理想。通过节约时间和消除苦役，它替休闲创造了空间。而正因有了休闲，我们方有可能去花时间于学习、兴趣或其他什么我们真正在意的事物上。「方便」让普罗大众也可享从前贵族独占的、提高自我修养的自由。以此看来，方便也极大抹平了社会差距。 「方便即解放」这个概念令人沉醉。而那些对其忘乎所以的刻画，也出现在科幻作品以及未来学家对二十世纪中叶的想象中。从《大众机械》（Popular Mechanics）这类严肃杂志到动画片《杰森一家》（The Jetsons）这种无脑娱乐里，我们皆可窥见未来生活将是百分百的「方便」光景。按下电钮，美食上桌；自动人行道可免去走路之烦；衣服将会清洁自己，甚至在穿过一天后自我解体。至少在这副如意算盘里，人存于世的奋斗早已不复存在。 「方便」的美梦，乃建立于「体力劳动」的噩梦之上。但劳动永远都是噩梦么？我们真想「不劳永逸」么？有时，或许那些不便而耗时的行为和追求反而更能展现人性。或许这也就是为什么，「方便」纵有万般好处，但总有些人将其拒之门外。这些人诚然有其顽固刻板的一面（还由于他们有这份奢侈去抵制），可也因为他们看到了它对人之为人的威胁，看到了「方便」对他们在意之事的掌控。 到上世纪六十年代，第一次「方便革命」开始星火燎原。充斥着「方便」的前景已不再是全社会的热望所在。方便意味一致性，而反文化运动则恰恰关乎民众表达自我、发挥个人潜能的需求，关乎与自然和谐共处而非寻求力克自然带来的不适。弹吉他并不方便，自己种菜或自己修理摩托车也并不方便。但它们本身作为结果被视作有价值的事情。彼时的人们，正在重新开始追求个人趣味。 而也许是大势所趋，第二波「方便技术」的浪潮（也就是我们生活的这个时间阶段）袭来，将这种理想裹挟了进去。个人趣味，被「方便化」了。 你或可将索尼随身听问世的 1979 年认作这个时间阶段的元年。由随身听，我们可知「方便」在方法论上一个微妙但根本的转变。若第一次方便革命的纲领是让你的生活和工作更轻松简单，第二次的目的，则是让「做你自己」更轻松简单。新科技成了个性的催化剂，赐予自我表达以效率。 想象一个八十年代初期的人，带着随身听，塞着耳机，在街上晃荡。他被自己选择的音乐环境隔绝于世，正在当众享受那种曾经只能于自家陋室中体验的自我表达。新的科技让他更容易地就能展示自己，哪怕是只对自己展示。他在天地间高视阔步。他是自己专属的电影里的大明星。 这幻象是如此诱人，以至于它支配了我们的存在，以至于过去几十年内我们创造的那些重要而强大的技术，多数都是为个性化与个人趣味服务的。想想看吧——录像机、音乐播放列表、Facebook 页面，以及 Instagram 账号——这些「方便」与费不费劲无关，反正我们也多半不会经常做这些事。它关乎的是精神资源的节省，免却的是面对诸多自我表达的选项时难以抉择的精神操劳。「方便」是「一键某某」，是一站式购物，是即插即用的无缝体验。新的理想状态，是不费吹灰之力就实现个人偏好。 我们当然心甘情愿为方便支付溢价——并且付得比我们自以为乐意的还多。举例来说，在上世纪九十年代末，Napster 之类的音乐发行技术使得从网上免费下载音乐成为可能，而许多人也的确对此利用颇多。但现在，尽管不花钱就能搞到音乐的办法依旧简单，却绝少有人这么干了。为什么？因为 2003 年 iTunes 商店的出现让购买音乐比非法下载还要方便。「方便」打败了「免费」。 随着各科事项桩桩件件地变得简单，对「方便」日益增长的期待也让这样一种压力水涨船高：要么继续将其他东西也简单化，要么便被时代抛弃。我们被即时性宠溺坏了，碰到还停留在老旧时效水平的事情就不胜其烦。当你可以无需陷于窗口长龙而在手机上买演唱会门票时，要排队等待为选举投票简直就令人无名火起，对那些从来不必排队的人来说尤其如此。（这也许有助于解释年轻人的低投票参与率。） 实际情况的吊诡之处在于，今日之个性化技术，乃是一种「群体个性化」的技术。定制化的产物也可能会出人意料地面目均匀。例如，所有人（或几乎所有人）都在 Facebook 上，这是与亲友——也就是理论上代表了你和你的人生独特之处的这群人——保持联系最方便的方式。但 Facebook 似乎把我们弄得毫无差别。它的固定格式和习惯套路将我们剥离得只剩下最肤浅的那层个性表达，比如将哪张沙滩或山峦的美图设为页面背景这类琐事。 我无意否认让事情变得简单能对我们起重大作用，能推动我们在餐厅、打车服务、开源百科上的选择变得从无到有，从少到多。但我们的人生不只是「拥有并做出选择」这一件事。我们还要处理突如其来的情况，还要克服值得面对的挑战，完成困难的任务——这些奋斗，有助于塑造我们成为自己。当此等障碍、吃力、限制和准备门槛都消失不见，人生体验会变成何等模样？ 今天的方便文化无法坦然面对人生体验的基本特点——「困难」，因为「方便」只在乎终点，不关心旅途。但哪怕你最后都立于同一海拔，攀援一座高峰与乘缆车登顶的过程依然存在天壤之别。我们正在成为那种仅仅（或者主要）在意结果的家伙，正冒着把人生变成一连串痴苶的缆车观光的风险。 为了不使「方便」的唯一导向是更多的方便，它必须为更大的目标服务。贝蒂 · 弗里丹（Betty Friedan）在她 1963 年的经典著作《女性的奥秘》（The Feminine Mystique）里曾观察了家庭科技对女性的影响，并且得出了它们只是创造了更多需求这样的结论。「就算是用上所有那些节省劳力的电器，」她写道，「一位现代美国家庭主妇花在家务上的时间也或许比她祖母还要多。」当事情变得简单，我们会用其他「简单」的事情填补空白的时间。在某些时刻，无关痛痒的细小琐事已经取代奋斗，专制统治着我们的人生。 生活在一个什么东西都是「简单」的世界里的一个令人厌恶的结果，就是唯一重要的技能只剩下多任务同时处理。在极端情况下，我们其实没做任何事，只是排列了要做之事的顺序罢了。而基于此的人生，则无异于沙上之塔。 我们须在更多时间（虽不至于每时每刻）里树立去主动拥抱「不便」的自觉。至少做若干个「不便」的选择吧，在这个时代，只有这样，你才能寻获真正的个人趣味。不是说你非得自己做黄油或者去打猎，但如果你想要成就点什么，你不能让「方便」凌驾于其他价值之上。有时「麻烦」并不是问题，而是答案：它可以成为「你是谁」这个问题的答案。 而「拥抱不便」或许听起来古怪，但其实我们在不自知的情况下已经在这么做了。只是仿佛为了掩盖这点，我们替那些不便的选择起了其他名字：爱好、副业、内心的召唤和热情。这些非工具性的行为有助于定义我们。我们能从中收获自己的人格——因为它们都包含了人类区区肉身与自然法则之间充满意义的抗衡，例如制作木雕、组装原材料、修理家电、写代码、记录海浪的规律，或者直面长跑时腿和肺开始不听使唤的那一刻。 这样的行动需要时间，可它们终能使人受益。它们将我们暴露在沮丧与失败的风险之下，但它们亦将教会我们理解世界，以及我们在其中的角色。 让我们审思这场「方便」治下的暴政，更积极地尝试抵御它的愚民威力吧，且看结果如何。我们不可忘记缓慢与艰苦的乐趣所在，不可忘记不用「最方便」的办法带来的成就感。能保全我们于完全高效率同质化人生的，也许惟有那些灿若繁星的、「不方便」的选择。]]></content>
      <categories>
        <category>随想</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Next主题下如何让图片自动居中]]></title>
    <url>%2F2018-03-08%2FNext%E4%B8%BB%E9%A2%98%E4%B8%8B%E5%A6%82%E4%BD%95%E8%AE%A9%E5%9B%BE%E7%89%87%E8%87%AA%E5%8A%A8%E5%B1%85%E4%B8%AD%2F</url>
    <content type="text"><![CDATA[在 Hexo博客中，图片的渲染由主题控制，对于 Next主题的scheme:Mist，其是可以通过设置css来控制图片的左右边距来实现自动居中的。 网上查到的方法只说了修改第二个，但是我尝试了过后无效，加上fancybox的修改后可以完美的实现图片居中。 设置fancybox 在theme/next/_config.yml里的注释区有一段话 1234# Fancybox. There is support for old version 2 and new version 3.# Please, choose only any one variant, do not need to install both.# For install 2.x: https://github.com/theme-next/theme-next-fancybox# For install 3.x: https://github.com/theme-next/theme-next-fancybox3 根据里面的github链接，我们去安装fancybox3（我这个人喜欢更新的而非老而稳定的版本）。 https://github.com/theme-next/theme-next-fancybox3 进入这个链接里，按照一步一步的提示，很容易就能够安装好fancybox3，然后再到_config.yml文件里将fancybox: true设置下变好了。 修改.styl文件 打开 themes/next/source/css/_schemes/Mist/_posts-expanded.styl 文件，找到 .posts-expand 中的 .post-body img { margin: 0; }，将之修改为 .post-body img { margin: 0 auto; } 即可。 最终实现的图片居中效果如下（当然这张图片本身也是居中的）]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Ignite框架下解决Opencv中Mat类型传递的问题]]></title>
    <url>%2F2018-03-08%2F%E5%9C%A8Ignite%E6%A1%86%E6%9E%B6%E4%B8%8B%E8%A7%A3%E5%86%B3Opencv%E4%B8%ADMat%E7%B1%BB%E5%9E%8B%E4%BC%A0%E9%80%92%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[作者：落点 目前在做一个分布式计算的应用，使用当下很新的Apache Ignite框架，因为中文相关博客几乎没有，我们的开发小组计划着写一点文字用于记录下我们遇到的问题，希望能够帮助到后人。 昨天在写前端摄像头获取图像，后端处理图像识别人脸的过程中，在后端从DataGrid的cache中获取opencv的特定图像数据类型Mat时遇到了问题，总是无法获取，提醒为null的情况。 Ignite并没有详细的对异常原因作出提示，我们经过排查之后发现了可能是序列化的问题（因为java内置的许多数据类型如string,int等都可以传输）。 而Mat类型的实现中是没有序列化的，因此我们需要手动的序列化。 该如何做呢？ 显然不是写一个 1class MyMat implements Serialization 这么简单。 我们这里提供一种思路，将其转化为字节数组byte[]进行序列化。 序列化的过程如下，这里要将矩阵Mat的结构信息传输过去，方便对方根据字节数组byte[]还原矩阵。 123456789101112131415IgniteCache&lt;Integer,byte[]&gt; igniteCache = ignite.getOrCreateCache ("stream_data");IgniteCache&lt;Integer,int[]&gt; matcache = ignite.getOrCreateCache ("matcache");Mat m1 = new Mat();byte[] bytes = new byte[(int) (m1.total()*m1.elemSize())];m1.get(0,0,bytes);int[] mat_profile = new int[5];mat_profile[0] = m1.type ();mat_profile[1] = m1.rows();mat_profile[2] = m1.cols();igniteCache.put(1,bytes);matcache.put(1,mat_profile); 反序列化的过程如下： 123456789IgniteCache&lt;Integer,byte[]&gt; igniteCache = ignite.getOrCreateCache("stream_data");IgniteCache&lt;Integer,int[]&gt; matcache = ignite.getOrCreateCache("matcache");int[] matprofile = new int[5];matprofile = matcache.get(1);byte[] bytes = igniteCache.get(1);Mat m2 = new Mat(matprofile[1],matprofile[2],matprofile[0]);m2.put(0,0,bytes); 这里通过两个cache，一个用来传输字节数组，一个用来传输矩阵信息，先压缩，再解压，最终能够成功的实现序列化。 最后附上小广告 在Ignite的道路上，我们都是初学者，但是我们会努力的在开发的过程中，不断的去写技术博客的形式，来传播我们的经验。 想要加入我们？或者是获取更多的知识？ 可以联系我的QQ：1121058986，备注Ignite即可。 也可以加入我们的QQ群：481810803，群内有技术大牛和在分布式系统方面经验丰富的指导老师为大家答疑解惑。 让我们一起，点燃这火，ignite~]]></content>
      <categories>
        <category>Apache Ignite</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Contest-934-a-Compatible Pair]]></title>
    <url>%2F2018-03-06%2FContest-934-A-Compatible-Pair%2F</url>
    <content type="text"><![CDATA[A题： http://codeforces.com/contest/934/problem/A 讲道理这道题是要比 B 更难的，在 case 10 这里 WA 掉了好多次。 这个题 Alice 可以藏一个自己的数，Bob 则想办法从A那里拿一个数和自己这里的某个数相乘，使得最终的结果最大。开始的策略想的是排序，但是发现在负数的时候会出问题。 现在考虑一个场景，当 Alice 和 Bob 都有负数的时候，负负得正，-3,-3,2,4这样的，排序取最大相乘答案就错了。 再考虑了下解决负数时也可以分类讨论，将A和B的序列都分成 A_minus 和 A_plus 来搞，也有另外的解法，可以直接模拟，每次 Alice 拿走一个，然后 Bob 做 m * n的枚举找剩下序列中的最大值，然后最后再求所有的 m * n 的最小值即是最终的解。 code如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#include&lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;iomanip&gt;#include &lt;string&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;cstdlib&gt;#include &lt;vector&gt;#include &lt;queue&gt;#include &lt;stack&gt;#include &lt;cmath&gt;#include &lt;bitset&gt;#include &lt;unordered_set&gt;#include &lt;numeric&gt;#include &lt;set&gt;#include &lt;list&gt;#include &lt;map&gt;using namespace std;#define lower_bound LB#define upper_bound UB#define mem(a, x) memset(a,x,sizeof(a))#define rep(i, a, n) for (int i=a;i&lt;n;i++)#define per(i, a, n) for (int i=n-1;i&gt;=a;i--)#define mp make_pair#define all(x) (x).begin(),(x).end()#define SZ(x) ((int)(x).size())#define IT iterator#define test puts("OK")#define lowbit(x) x &amp; -x#define PRQ priority_queue#define PB push_back#define gcd(a, b) _gcd(a,b)typedef long long LL;typedef unsigned long long uLL;typedef pair&lt;int, int&gt; pii;typedef vector&lt;int&gt; VI;typedef pair&lt;int, int&gt; PII;typedef vector&lt;PII&gt; VPII;const LL mod = 1000000007;const double PI = acos (-1.0);const double eps = 1e-8;const int INF = 0x3f3f3f3f;const LL LINF = 0x3f3f3f3f3f3f3f3f;int n,m;const int maxn = 55;LL ar_n[maxn];LL ar_m[maxn];int main ()&#123;#ifndef ONLINE_JUDGE freopen ("A.txt", "r", stdin);#endif ios::sync_with_stdio (false); cin.tie (nullptr); cout.tie (nullptr); while(cin&gt;&gt;n&gt;&gt;m) &#123; mem(ar_n,0); mem(ar_m,0); for (int i = 0; i &lt; n ; ++i) &#123; cin&gt;&gt;ar_n[i]; &#125; for (int i = 0; i &lt; m; ++i) &#123; cin&gt;&gt;ar_m[i]; &#125; LL suit1 = LINF; LL suit2 = -LINF; for (int i = 0; i &lt; n; ++i) &#123; suit2 = -LINF; for (int j = 0; j &lt; n; ++j) &#123; if (j != i) &#123; for (int k = 0; k &lt; m; ++k) &#123; suit2 = max (suit2,ar_n[j] * ar_m[k]); &#125; &#125; &#125; suit1 = min(suit2,suit1); &#125; cout&lt;&lt;suit1&lt;&lt;"\n"; &#125; return 0;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Next主题下MathJax与Markdown不兼容问题]]></title>
    <url>%2F2018-03-04%2FNext%E4%B8%BB%E9%A2%98%E4%B8%8BMathJax%E4%B8%8EMarkdown%E4%B8%8D%E5%85%BC%E5%AE%B9%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题描述由于mathjax和markdown的冲突，在某些mathjax语句中，会出现如下这种渲染冲突的情况。 这是因为在Markdown语法中，两个下划线之间的文本会被转换为斜体，所以这个错误是由于Markdown本身没有支持Latex，Markdown文本先交由marked.js（Hexo默认渲染器）对文本进行渲染时，将_替换成了&lt;em&gt;标签，然后才被Mathjax交由mathjax.js进行渲染，导致无法正确识别公式。同样的问题也发生在\\经过转义后变成\，MathJax渲染时不能正确识别换行符。 解决方案 Pandoc 理解了这个问题的本质是Markdown与LaTeX语法冲突后，我们来理一理解决问题的思路。最根本的解决方法当然是从Markdown语法本身入手，换用有着更strong的语法的标记语言来避免冲突，比如pandoc。pandoc大法固然好，但是为了保持博客的轻量级（当初就是为了这个从Wordpress转到了Markdown+Hexo），暂时还不打算动用pandoc这个核武器。感兴趣的小伙伴可以去了解一下pandoc，与之对应的Hexo插件hexo-renderer-pandoc。 保护公式块 第二个思路就是利用Markdown特有的rawblock标签保护LaTeX代码块，如使用下图的双美元符号，将公式整体括起来。 ​ 替换渲染引擎 修改如下： 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 小结我个人推荐使用3+2的解决方法，这样可以完美无误的书写任何公式，在使用kramed渲染+使用双美元符号时不用换行，直接在行内书写即可，最终的效果如下。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[异常检测-面向大数据的异常检测算法设计分析]]></title>
    <url>%2F2018-03-04%2F%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-%E9%9D%A2%E5%90%91%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[15-李博 时间序列 时间序列是指按照时间先后顺序排列的各个观测记录的有序集合，广泛存在于商业、经济、工程、社会科学和医学等领域。随着时间的推移 ，时问序列通常包含大量的信息，是建模和预测的主要依据。对时间序列进行分析，可以揭示事物运动、变化和发展的内在规律，对于人们正确认识事物并据此做出科学决策具有重要的现实意义。 时间序列的一个固有特征是相邻的观测值之间存在着相互的依赖性，这种相互的依赖性往往蕴含着被观测事物或现象在特定环境或特定时刻的大量信息，研究与分析这种相互的依赖性具有极大的实用价值。 异常数据异常是数据中的特定模式，这些模式与事先定义的正常模式存在不一致。时间序列异常是时间序列上下文中与正常模式存在不一致的时间子序列。在时间序列中，数据每一时期都受到多种因素的共同作用，通常产生异常点的原因主要包括： 数据受到新机制的作用，如欺诈、入侵、疾病的爆发、不寻常的实验结果等。这些异常点出现是因为有新事物出现或者新情况发生，比如经济领域时间序列研究中，某种经济政策的出台；地质模型中某种可能含有矿藏的地层的发现；由于罢工、广告促销、突发性政治或经济重大事件、物理系统的突变等，这些因素会造成不同于寻常模式的观测结果。这类异常点通常蕴涵着具体的意义，也往往是研究者感兴趣 的，异常点诊断旨在识别出这些现象背后的本质起因。 数据变化固有规律引起，这是自然发生的，反映了数据的分布特征，如气候变化、基因突变等。 数据测量收集误差引起，主要是由于人为差错、测量仪器故障。 时间序列是受监测事物状态的具体表现，而异常数据同样也包含着不可忽略的重要信息，数据的异常表明着数据所表征的物理设备或者是统计信息在局部或者是周期出现了异常的波动，而这些波动是我们需要在生产生活中去检测到并加以避免的。 基本算法针对于异常检测，前人已经做了很多相关的工作，这里我们列举一些异常检测的基本算法思路。 基于假设检验的方法 假设检验是最早用来发现异常样本的基于统计学原理的方法，它基于对小概率事件的判别来实现对数据样本异 常性的鉴别，如 t 检验、Dixon 检验 、Grubs 检验等。 这种方法的难点在于对于分布特征未知的数据，使用先验假设具有很大的不确定性和局限性。 基于线性模型的统计学方法 对原始数据变换弱化原时间序列的相关性，使其满足经典线性回归的各项假设条件。在回归框架下又细分为残差分析方法和影响分析方法，前者是根据线性框架的拟合效果来判别数据是否异常，后者则是根据数据对于统计量的影响来判别。 这种方法的效率很高，但是对于数据选择有一定的局限性，大部分的经典的线性回归模型，如ARIMA等，都需要数据满足一些前提假设。 基于聚类的算法 这类算法将数据集分成若干类 ，不属于任何类的数据点就是异常点，比较典型的算法有 DBSCAN，Isolated Forest等。这类方法看似好用，但是却存在着算法效率较低，而且特定的算法对于数据集以及参数的要求较高，较难真正的普适到更多的应用场景。 基于密度的算法 基于密度的方法主要有（LOF：local outlier factor），LOF即为数据对象邻域的平均可达密度与其自身的可达密 度之比，LOF越大，其离群程度越高。LOF检测的效果不错，但是同样依赖于参数而且算法效率较低。 基于极值理论的算法 大道至简，近年来，很多学者开始将极值理论应用到时间序列异常诊断中，模型异常点诊断的关键就是决定检验统计量在一定的显著水平下是否超越某一临界值。这种方法可行且好用，可以作为 2 类方法设计的补充或者是一部分，且关于如何选取阈值大小，以及阈值种类，都是有着很多的学问以及相关研究。 工业级别大数据的异常检测随着技术的发展，可监测对象的种类越来越多，采集数据的设备数量越来越大。时间序列的多样性与规模化对时间序列异常检测方法提出了新的要求。 上述的算法在面向工业数据应用时，普遍存在着检测效果以及检测性能的问题。 现有异常检测方法在检测效果方面无法适应时间序列多样化的要求，大多方法脱离了论文，应用到实战中，便需要结合对于数据的深刻理解，才能够调整出合适的参数，识别出异常。 另外大部分方法基于机器学习算法设计，算法检测效果虽然较好，但是一旦数据集过大，在内存和时间方面的效率远不能达到我们理想的要求。 在结合大数据方面的相关文献以及我们对于异常检测问题的粗浅理解，在下面我们简单的分析针对于工业级别大数据的异常检测算法的设计思路。 通过降维减小数据维度 通常，我们的时间序列数据是多维的，多维时间序列可用于描述受监测事物的多个方面状态与情况，然而随着维度的增长，多时间序列异常异常检测的计算时间会快速增加。通过观察发现当时间序列存在与异常发生原因无关的维度时，进行一些相关性分析，去除无关维度不会对异常检测的准确性产生决定性影响。 使用简单的线性判别 在时间序列的研究当中，很多时候少即是多，时间序列受随机性，趋势性影响较大，往往越多的参数，越复杂的模型意味着很难去长时间，更泛化的拟合真实的数据。而模型的复杂通常也意味着复杂度的提升，因此我们提倡在大数据算法的设计当中，可以适当的为了泛化能力以及时间效率，选择较为简单的线性方法进行粗略的异常判别，后续再使用更精确的算法对子区间进行判别。 尽量设计在线的算法 目前大多数异常检测方法均为静态方法，即对历史中特定段落的时间序列进行分析并得出结果。静态的时间序列方法不能应用于实时的时间序列异常检测。然而在许多应用场景中时间序列是不断增长的，因此我们对于实时获得的时间序列中的异常的需求同样迫切。而且在线的算法也意味着效率近似于线性，是一件非常让人愉悦的事情。 通过并行的思想改善耗时部分的性能 大数据的异常检测对于检测方法提出了存储能力与计算能力的新要求。单个计算结点的存储能力与计算能力无法满足这些要求，我们需要利用并行化计算的方法改善异常检测方法的检测性能。 如何设计 Master 和 Slaver 结点的算法，以及它们划分，合并的关系，将哪个部分正确的，有效的用于并行计算，是这个方向上的研究重点。 做好时间与检测效果的 Trade Off 针对于大数据所设计出的算法，应该有相应的调整系数，能够平衡效率和检测效果的不同侧重。对于某些不需要高精度检测，但是实时性较强的环境，可以通过调整参数来达到相应的需求。 以框架的方式设计算法 以上所提出的几点，均是这个方向上的算法的设计思路，或者说期望达到的目标，一个算法想要很好的结合这三点是很困难的。因此我们认为或许可以使用框架的方式，将各种手段或者技术进行合理的整合，比如异常序列的粗预警，细预警的分离，多维数据，多指标检测的并行分离，等等手段，都可以通过框架的方式进行有机的整合，最终形成一个良好可用的算法。 总结总的来说，时间序列的异常检测是一个在当下 CV 和 NLP 等异常火热的时代下，看起来似乎不那么耀眼的问题，但是它对于这个世界的生产生活具有同等，似乎还有更重要的意义。 目前对于普通的异常检测可能研究相对较多，但是考虑大数据背景的时序数据的异常检测的相关研究目前还不是很充分，这是一个急需去占领的高地，希望我们能在未来，看到更多的相关问题的解决方案。]]></content>
      <categories>
        <category>异常检测</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Contest-911-D-Inversion Counting]]></title>
    <url>%2F2018-01-24%2FContest-911-D-Inversion-Counting%2F</url>
    <content type="text"><![CDATA[http://codeforces.com/contest/911/problem/D 题目大意为：逆序对问题，事先求出有多少个逆序对，只用知道是奇数个还是偶数个即可，然后现在我们对l,r区间的序列进行 reverse 之后再问序列的逆序对是奇数个还是偶数个。 因为 $l,r$ 区间反转之后，如果原来的全是顺序对，那么现在会造成总共 \frac{(len(l,r) * len(l,r) - 1)}{2} 个逆序对，所以总的逆序对个数为 原有逆序对（区间外逆序对个数）+ 新逆序对，如果新逆序对个数为奇数，则总逆序对的奇偶性发生改变。 如果原来的区间里含有m个逆序对，区间外逆序对个数为 $p$ 个，那么反转之后，逆序对会变成顺序对，原有顺序对会变成逆序对，同样总共会有新的 \frac{(len(l,r) * len(l,r) - 1)}{2} - m 个逆序对产生，总的逆序对数变为\frac{(len(l,r) * len(l,r) - 1)}{2} - m + p 个，原逆序对数为 $m + p$ 个，怎么在只知道 $m + p$ ，不知道 $m$ 的情况下，推出的奇偶 \frac{(len(l,r) * len(l,r) - 1)}{2} - m + p 性呢？ 可以利用 \frac{(len(l,r) * len(l,r) - 1)}{2} - m + p = \frac{(len(l,r) * len(l,r) - 1)}{2} + m + p - 2m ，然后就可以利用 $m + p$ 的奇偶性，以及 \frac{(len(l,r) * len(l,r) - 1)}{2} 的奇偶性，推出 \frac{(len(l,r) * len(l,r) - 1)}{2} - m + p 的奇偶性了。 代码很简单： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#include &lt;iostream&gt;#include &lt;cstdio&gt;#include &lt;iomanip&gt;#include &lt;string&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;cstdlib&gt;#include &lt;vector&gt;#include &lt;queue&gt;#include &lt;stack&gt;#include &lt;cmath&gt;#include &lt;bitset&gt;#include &lt;unordered_set&gt;#include &lt;numeric&gt;#include &lt;set&gt;#include &lt;list&gt;#include &lt;map&gt;using namespace std;#define lower_bound LB#define upper_bound UB#define mem(a,x) memset(a,x,sizeof(a))#define rep(i,a,n) for (int i=a;i&lt;n;i++)#define per(i,a,n) for (int i=n-1;i&gt;=a;i--)#define mp make_pair#define all(x) (x).begin(),(x).end()#define SZ(x) ((int)(x).size())#define IT iterator#define test puts("OK")#define lowbit(x) x &amp; -x#define PRQ priority_queue#define PB push_back#define gcd(a,b) _gcd(a,b)typedef long long LL;typedef unsigned long long uLL;typedef pair&lt;int,int&gt; pii;typedef vector&lt;int&gt; VI;typedef pair&lt;int,int&gt; PII;typedef vector&lt;PII&gt; VPII;const LL mod=1000000007;const double PI = acos(-1.0);const double eps = 1e-8;const int INF = 0x3f3f3f3f;int main()&#123; #ifndef ONLINE_JUDGE freopen("D.txt","r",stdin); #endif ios::sync_with_stdio(false); cin.tie(nullptr); cout.tie(nullptr); int n,m; int l,r; vector&lt;int&gt; ar; ar.clear(); while(cin&gt;&gt;n) &#123; ar.clear(); for (int i = 0; i &lt; n; ++i) &#123; int temp; cin&gt;&gt;temp; ar.PB(temp); &#125; bool ret = 0; for (int i = 0; i &lt; n; ++i) &#123; for (int j = i; j &lt; n; ++j) &#123; if (ar[j] &lt; ar[i]) &#123; ret ^= 1; &#125; &#125; &#125;// cout&lt;&lt;ret&lt;&lt;endl; cin&gt;&gt;m; for (int i = 0; i &lt; m; ++i) &#123; cin&gt;&gt;l&gt;&gt;r; int permutations = (r - l) * (r - l + 1) / 2; if (permutations &amp; 1) &#123; ret ^= 1; &#125; if (ret &amp; 1) &#123; cout&lt;&lt;"odd\n"; &#125; else &#123; cout&lt;&lt;"even\n"; &#125; &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[17周周报-使用Holt-Winters模型通过比对预测值进行异常检测]]></title>
    <url>%2F2017-12-24%2F17%E5%91%A8%E5%91%A8%E6%8A%A5-%E4%BD%BF%E7%94%A8Holt-Winters%E6%A8%A1%E5%9E%8B%E9%80%9A%E8%BF%87%E6%AF%94%E5%AF%B9%E9%A2%84%E6%B5%8B%E5%80%BC%E8%BF%9B%E8%A1%8C%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[基于预测比较模型的异常检测 16周时，开始尝试一些新的思路与原有方法的对比，在比较了传统的滑动平均和现有的指数平均方法之后，我们采用Holt Winters方法从预测的角度来做误差分析。 新的方法在序列数据的异常检测过程中，我们既可以直接使用对序列进行异常检测的算法，也可以先对序列数据进行特征提取然后转化为传统的离群点检测。 离群点检测方法 方法描述 方法特点 基于统计 大部分的基于统计的离群点检测方法是构建一个概率分布模型，并计算对象符合该模型的概率，把具有低概率的对象视为离群点 基于统计模型的离群点检测方法的前提是必须知道数据集服从什么分布；而对于高维的数据，可能每一维度服从的分布都不太一致，所以通常对高维数据来讲通常效果较差。 基于邻近度 通常可以在数据对象之间定义邻近性度量，把远离大部分点的对象视为离群点。 算法假定离群点是离散的，低维数据我们可以作图观察，而高维数据我们无法观察，所以难以确定有效的参数和全局阈值，效果较差。 基于聚类 一种利用聚类检测离群点的方法是直接丢弃远离其他簇的小簇；另一种是对数据点属于簇的程度进行评价，去除得分较低的点。 聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大，对数据的可分类性要求较高 之前考虑的算法方向主要是在『基于统计』+『基于聚类』的这个方向来考量。 而如今我发现了一种新的方法可以作为采用与尝试，即上图中『基于临近度』，也是一种使用历史数据判断当前数据的方法。 基于预测的异常检测模型如下图所示，$x_t$ 是真实数据，通过预测器得到预测数据，然后 $x_t$ 和 $p_t$ 分别作为比较器的输入，最终得到输出 $y_t$，$y_t$ 是一个二元值，可以用+1（+1表示输入数据正常），-1（-1表示输入数据异常）表示。 如果说我们设置异常检测的模型如此，那么我们可以从两个以下方面入手，一是预测器的优化，二是比较器的优化。 预测器优化同比环比预测器同比环比是比较常用的异常检测方式，它是将当前时刻数据和前一时刻数据（环比）或者前一天同一时刻数据（同比）比较，超过一定阈值即认为该点异常。如果用图模型来表示，那么预测器就可以表示为用当前时刻前一时刻或者前一天同一时刻数据作为当前时刻的预测数据。 如果将不同日期、时刻的监控数据以矩阵方式存储，每一行表示一天内不同时刻的监控数据，每一列表示同一时刻不同日期的监控数据，那么存储矩阵如下图所示： 假如需要预测图中黄色数据，那么环比使用图中的蓝色数据作为预测黄点的源数据，同比使用图中红色数据作为预测黄点的源数据。 基线预测器（MA方法）同比环比使用历史上的单点数据来预测当前数据，误差比较大。$t$ 时刻的监控数据，与$t-1,t-2,…$ 时刻的监控数据存在相关性。同时，与$t-k,t-2k,…$ 时刻的数据也存在相关性（k为周期），如果能利用上这些相关数据对t时刻进行预测，预测结果的误差将会更小。 比较常用的方式是对历史数据求平均，然后过滤噪声，可以得到一个平滑的曲线（基线），使用基线数据来预测当前时刻的数据。该方法预测 $t$ 时刻数据（图中黄色数据）使用到的历史数据如下图所示（图中红色数据）： Holt-Winters预测器同比环比预测到基线数据预测，使用的相关数据变多，预测的效果也较好。但是基线数据预测器只使用了周期相关的历史数据，没有使用上同周期相邻时刻的历史数据，相邻时刻的历史数据对于当前时刻的预测影响是比较大的。对于 Holt-winters 预测期模型，它建议使用黄色点左上方的所有数据。 Holt-Winters是三次指数滑动平均算法，它将时间序列数据分为三部分：残差数据 $a(t)$，趋势性数据 $b(t)$，周期性数据 $s(t)$。使用Holt-Winters预测 $t$ 时刻数据，需要 $t$ 时刻前包含多个周期的历史数据。 详细信息看这里：https://www.otexts.org/fpp/7/5 在实际的异常检测模型中，我们对Holt-Winters预测器进行了简化。预测器的趋势数据表示的是时间序列的总体变化趋势，经过分析，如果以天/小时为周期看待传感器数据的订单量时间序列，是没有明显的趋势性的，如下的分解图也证明了这一点。因此，我们可以去掉其中的趋势数据部分。 各部分迭代的简化计算公式如（其中 $k$ 为周期）： $a[t] = \alpha(Y[t] - s[t-k]) + (1-\alpha) a[t-1]$ $s[t] = \gamma(Y[t] - a[t]) + (1 - \gamma)(s[t-k])$ 预测值：$Y[t+h] = a[t] + s[t-k+1 + (h-1) mod k]$ 为了将算法应用到线上的实时预测，我们可以将 Holt-Winters 算法拆分为两个独立的计算过程： 定时任务计算序列的周期数 $s(t)$。 $S(t)$ 不需要实时计算，只用按照周期性更新即可，使用 Holt-Winters 公式计算出时间序列的周期性数据。 对残差序列做实时预测。 计算出周期数据后，下一个目标就是对残差数据的预测。使用下面的公式，实际监控数据与周期数据相减得到残差数据，对残差数据做一次滑动平均，预测出下一刻的残差，将该时刻的残差、周期数据相加即可得到该时刻的预测数据。对于分钟数据，则将残差序列的长度设为60，即可以得到比较准确的预测效果。 红线为预测数据，蓝线为真实数据 ​ 比较器优化预测器预测出当前时刻传感器的预测值后，还需要与真实值比较来判断当前时刻数据是否异常。一般的比较器都是通过阈值法，比如实际值超过预测值的一定比例就认为该点出现异常，进行报警。这种方式错误率比较大。在传感器数值模型的报警检测中没有使用这种方式，而是使用了两个串联的Filter，只有当两个Fliter都认为该点异常时，才进行报警，下面简单介绍一下两个Filter的实现。 离散度Filter根据预测误差曲线离散程度过滤出可能的异常点。一个序列的方差表示该序列离散的程度，方差越大，表明该序列波动越大。如果一个预测误差序列方差比较大，那么我们认为预测误差的报警阈值相对大一些才比较合理。离散度 Filter 利用了这一特性，取连续 15 分钟的预测误差序列，分为首尾两个序列（e1,e2），如果两个序列的均值差大于 e1 序列方差的某个倍数，我们就认为该点可能是异常点。 阈值Filter根据误差绝对值是否超过某个阈值过滤出可能的异常点。利用离散度 Filter 进行过滤时，报警阈值随着误差序列波动程度变大而变大，但是在输入数据比较小时，误差序列方差比较小，报警阈值也很小，容易出现误报。所以设计了根据误差绝对值进行过滤的阈值 Filter。阈值 Filter 设计了一个分段阈值函数 $y=f(x)$，对于实际值 $x$ 和预测值 $p$ ，只有当 $|x-p|&gt;f(x)$ 时报警。实际使用中，可以寻找一个对数函数替换分段阈值函数，更易于参数调优。 模型最终架构每天定时抽取历史10天数据，经过预处理模块，去除异常数据，经过周期数据计算模块得到周期性数据。对当前时刻预测时，取60分钟的真实数据和周期性数据，经过实时预测模块，预测出当前传感器数值。将连续15分钟的预测值和真实值通过比较器，判断当前时刻是否异常。 参考来源： https://www.jianshu.com/p/6fb0408b3f54 https://www.otexts.org/fpp/7/5]]></content>
      <categories>
        <category>异常检测</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mecari-Analysis(LB=0.4229~rank 17 / 1090)]]></title>
    <url>%2F2017-12-23%2FMecari-Analysis-LB-0-4229-rank-17-1090%2F</url>
    <content type="text"><![CDATA[Mecari其实从情理上来说这个比赛有一点奇怪，因为全凭给出的 feature 似乎并不能很好的去 fit 结果的 price。 所以如果不能从特征工程的角度去挖掘数据的信息的话，只拿已给出的信息扔进 xgboost 或者是 lgbm，似乎就会和大部分人在同一个水平线。 数据处理NLP-RelatedDocument-term matrixA document-term matrix or term-document matrix is a mathematical matrix) that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. There are various schemes for determining the value that each entry in the matrix should take. D1 = “I like databases” D2 = “I hate databases” then the document-term matrix would be: I like hate databases D1 1 1 0 1 D2 1 0 1 1 也可以使用tf-idf schema对其进行计数。 Bags of words model下列文件可用词袋表示: 以下是两个简单的文件: 1(1) John likes to watch movies. Mary likes movies too. 1(2) John also likes to watch football games. 基于以上两个文件，可以建构出下列清单: 123456789101112[ "John", "likes", "to", "watch", "movies", "also", "football", "games", "Mary", "too"] 此处有10个不同的词，使用清单的索引表示长度为10的向量: 1[1, 2, 1, 1, 2, 0, 0, 0, 1, 1] (2) [1, 1, 1, 1, 0, 1, 1, 1, 0, 0] 每个向量的索引内容对应到清单中词出现的次数。 标签二值化LabelBinarizer 是一个用来从多类别列表创建标签矩阵的工具类: 123456789&gt;&gt;&gt; from sklearn import preprocessing&gt;&gt;&gt; lb = preprocessing.LabelBinarizer()&gt;&gt;&gt; lb.fit([1, 2, 6, 4, 2])LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)&gt;&gt;&gt; lb.classes_array([1, 2, 4, 6])&gt;&gt;&gt; lb.transform([1, 6])array([[1, 0, 0, 0], [0, 0, 0, 1]]) 在 mecari 中我们对 brand_name 进行二值化处理，生成了一个item * brand_count 大小的矩阵。 One-hot 编码和 get_dummies离散特征的编码分为两种情况： 离散特征的取值之间没有大小的意义，比如color：[red,blue],那么就使用one-hot编码 离散特征的取值有大小的意义，比如size:[X,XL,XXL],那么就使用数值的映射{X：1,XL：2,XXL：3} 使用pandas可以很方便的对离散型特征进行one-hot编码。 12345678910111213141516import pandas as pd df = pd.DataFrame([ ['green', 'M', 10.1, 'class1'], ['red', 'L', 13.5, 'class2'], ['blue', 'XL', 15.3, 'class1']]) df.columns = ['color', 'size', 'prize', 'class label'] size_mapping = &#123; 'XL': 3, 'L': 2, 'M': 1&#125; df['size'] = df['size'].map(size_mapping) class_mapping = &#123;label:idx for idx,label in enumerate(set(df['class label']))&#125; df['class label'] = df['class label'].map(class_mapping) 对于有大小意义的离散特征，直接使用映射就可以了，{‘XL’:3,’L’:2,’M’:1} Using the get_dummies will create a new column for every unique string in a certain column. 1pd.get_dummies(df) Tf-idfTF：Term Frequency. 单词在文章中出现的频率。 Idf：Inverse Document Frequency. 逆文档频率：为了衡量单词在该文章中的重要程度（在Mecari中我们是衡量单词在这条评论中的重要程度）。 如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。 TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。所以，自动提取关键词的算法就很清楚了，就是计算出文档的每个词的TF-IDF值，然后按降序排列，取排在最前面的几个词。 Boosting 思想 不断的强化模型 The term Boosting refers to a family of algorithms which converts weak learner to strong learners. Ada BoostWhy often use decision tree? Decision trees are non-linear. Boosting with linear models simply doesn’t work well. The weak learner needs to be consistently better than random guessing. You don’t normal need to do any parameter tuning to a decision tree to get that behavior. Training an SVM really does need a parameter search. Since the data is re-weighted on each iteration, you likely need to do another parameter search on each iteration. So you are increasing the amount of work you have to do by a large margin. Decision trees are reasonably fast to train. Since we are going to be building 100s or 1000s of them, thats a good property. They are also fast to classify, which is again important when you need 100s or 1000s to run before you can output your decision. By changing the depth you have a simple and easy control over the bias/variance trade off, knowing that boosting can reduce bias but also significantly reduces variance. Boosting is known to overfit, so the easy nob to tune is helpful in that regard. GBM回归树总体流程类似于分类树，区别在于，回归树的每一个节点都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化平方误差。也就是被预测出错的人数越多，错的越离谱，平方误差就越大，通过最小化平方误差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 Lgbm(Light Gradient Boosting Model)Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm. Below diagrams explain the implementation of LightGBM and other boosting algorithms. Parameters Tunning it is not advisable to use LGBM on small datasets. Light GBM is sensitive to overfitting and can easily overfit small data. Their is no threshold on the number of rows but my experience suggests me to use it only for data with 10,000+ rows. Control Parameters max_depth: It describes the maximum depth of tree. This parameter is used to handle model overfitting. Any time you feel that your model is overfitted, my first advice will be to lower max_depth. min_data_in_leaf: It is the minimum number of the records a leaf may have. The default value is 20, optimum value. It is also used to deal over fitting feature_fraction: Used when your boosting(discussed later) is random forest. 0.8 feature fraction means LightGBM will select 80% of parameters randomly in each iteration for building trees. bagging_fraction: specifies the fraction of data to be used for each iteration and is generally used to speed up the training and avoid overfitting. early_stopping_round: This parameter can help you speed up your analysis. Model will stop training if one metric of one validation data doesn’t improve in last early_stopping_round rounds. This will reduce excessive iterations. lambda: lambda specifies regularization. Typical value ranges from 0 to 1. min_gain_to_split: This parameter will describe the minimum gain to make a split. It can used to control number of useful splits in tree. max_cat_group: When the number of category is large, finding the split point on it is easily over-fitting. So LightGBM merges them into ‘max_cat_group’ groups, and finds the split points on the group boundaries, default:64 Core Parameters Task: It specifies the task you want to perform on data. It may be either train or predict. application: This is the most important parameter and specifies the application of your model, whether it is a regression problem or classification problem. LightGBM will by default consider model as a regression model. regression: for regression binary: for binary classification multiclass: for multiclass classification problem boosting: defines the type of algorithm you want to run, default=gdbt gbdt: traditional Gradient Boosting Decision Tree rf: random forest dart: Dropouts meet Multiple Additive Regression Trees goss: Gradient-based One-Side Sampling num_boost_round: Number of boosting iterations, typically 100+ learning_rate: This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates. Typical values: 0.1, 0.001, 0.003… num_leaves: number of leaves in full tree, default: 31 device: default: cpu, can also pass gpu Metric parameter metric： again one of the important parameter as it specifies loss for model building. Below are few general losses for regression and classification. mae: mean absolute error mse: mean squared error binary_logloss: loss for binary classification multi_logloss: loss for multi classification IO parameter max_bin： it denotes the maximum number of bin that feature value will bucket in. categorical_feature: It denotes the index of categorical features. If categorical_features=0，1，2 then column 0， column 1 and column 2 are categorical variables. ignore_column： same as categorical_features just instead of considering specific columns as categorical, it will completely ignore them. save_binary： If you are really dealing with the memory size of your data file then specify this parameter as ‘True’. Specifying parameter true will save the dataset to binary file, this binary file will speed your data reading time for the next time. Knowing and using above parameters will definitely help you implement the model. Remember I said that implementation of LightGBM is easy but parameter tuning is difficult. So let’s first start with implementation and then I will give idea about the parameter tuning. Ridge线性最小二乘拟合解析解 当XTX的行列式接近于0时，我们将其主对角元素都加上一个数k，可以使矩阵为奇异的风险大降低。于是： B(k)=(XTX+kI)−1XTYB(k)=(XTX+kI)−1XTY (I是单位矩阵) 随着k的增大，B(k)中各元素bi(k)的绝对值均趋于不断变小，它们相对于正确值bi的偏差也越来越大。k趋于无穷大时，B(k)趋于0。b(k)随k的改变而变化的轨迹，就称为岭迹。实际计算中可选非常多的k值，做出一个岭迹图，看看这个图在取哪个值的时候变稳定了，那就确定k值了。 X不满足列满秩，换句话就是说样本向量之间具有高度的相关性（如果每一列是一个向量的话）。遇到列向量相关的情形，岭回归是一种处理方法，也可以用主成分分析PCA来进行降维。 岭回归的原理较为复杂。根据高斯马尔科夫定力，多重相关性并不影响最小二乘法估计量的无偏性和最小方差性，但是，虽然最小二乘估计量在所有线性估计量中是方差最小的，但是这个方差都不一定小，而实际上可以找到一个有偏估计量，这个估计量虽然有较小的偏差，但它的精度却能够大大高于无偏的估计量。岭回归分析就是根据这个原理，通过在正规方程中引入有偏常熟二求的回归估计量的。 辅助函数preprocessing.MinMaxScaler： The MinMaxScaler is the probably the most famous scaling algorithm, and follows the following formula for each feature: xi–min(x)max(x)–min(x) It essentially shrinks the range such that the range is now between 0 and 1 (or -1 to 1 if there are negative values). This scaler works better for cases in which the standard scaler might not work so well. If the distribution is not Gaussian or the standard deviation is very small, the min-max scaler works better. 使用这种方法的目的包括： 对于方差非常小的属性可以增强其稳定性。 维持稀疏矩阵中为0的条目。 Cross Validation在解决实际问题中，我们可以将所有的数据集 dataset ，划分为 train_set（例如70%）和test_set（30%），然后在 train_set 上做 cross_validation ，最后取平均之后，再使用test_set测试模型的准确度。 K-Fold A model is trained using k-1 of the folds as training data; the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy). The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. Grid Search12345678910111213141516171819import pandas as pdfrom sklearn import svm, datasetsfrom sklearn.model_selection import GridSearchCVfrom sklearn.metrics import classification_reportiris = datasets.load_iris()parameters = &#123;'kernel':('linear', 'rbf'), 'C':[1, 2, 4], 'gamma':[0.125, 0.25, 0.5 ,1, 2, 4]&#125;svr = svm.SVC()clf = GridSearchCV(svr, parameters, n_jobs=-1)clf.fit(iris.data, iris.target)cv_result = pd.DataFrame.from_dict(clf.cv_results_)with open('cv_result.csv','w') as f: cv_result.to_csv(f) print('The parameters of the best model are: ')print(clf.best_params_)y_pred = clf.predict(iris.data)print(classification_report(y_true=iris.target, y_pred=y_pred)) Konstantin的建议Sure, let me give some examples: after looking at explained predictions, I see that “t-“ in word “t-shirt” is not highlighted, then I can check how scikit-learn vectorizer processes such words and see that it discards “t-“, so the model sees “shirt” - which may or may not be the problem, but it’s worth checking after looking at the model features, I see that words like “16gb” and “32gb” are really important - I would check, maybe people also write “16 gb” too, and it’s better to normalize such cases to give the model a better job I see “item_description__regimen” as a positive feature, this looks strange - is it a german word and so any german descriptions make the product more expensive? Or something else?]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ng'Cousera-Week 6-Bias vs Variance]]></title>
    <url>%2F2017-12-17%2FNg-Cousera-Week-6-Bias-VS-Variance%2F</url>
    <content type="text"><![CDATA[Diagnosing Bias vs. VarianceIn this section we examine the relationship between the degree of the polynomial d and the underfitting or overfitting of our hypothesis. We need to distinguish whether bias or variance is the problem contributing to bad predictions. High bias is underfitting and high variance is overfitting. Ideally, we need to find a golden mean between these two. The training error will tend to decrease as we increase the degree d of the polynomial. At the same time, the cross validation error will tend to decrease as we increase d up to a point, and then it will increase as d is increased, forming a convex curve. High bias (underfitting): both Jtrain(Θ) and JCV(Θ) will be high. Also, JCV(Θ)≈Jtrain(Θ). High variance (overfitting): Jtrain(Θ) will be low and JCV(Θ) will be much greater than Jtrain(Θ). The is summarized in the figure below: Regularization In the figure above, we see that as $\lambda$ increases, our fit becomes more rigid. On the other hand, as $\lambda$ approaches 0, we tend to over overfit the data. So how do we choose our parameter $\lambda$ to get it ‘just right’ ? In order to choose the model and the regularization term $\lambda$, we need to: Create a list of lambdas (i.e. $\lambda \in {0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24})$; Create a set of models with different degrees or any other variants. Iterate through the λs and for each λ go through all the models to learn some $\theta$ . Compute the cross validation error using the learned $\theta$ (computed with λ) on the $J_{CV}(\theta) $ without regularization or λ = 0. Select the best combo that produces the lowest error on the cross validation set. Using the best combo $\theta$ and λ, apply it on $J_{CV}(\theta) $ to see if it has a good generalization of the problem. DecisionOur decision process can be broken down as follows: Getting more training examples: Fixes high variance Trying smaller sets of features: Fixes high variance Adding features: Fixes high bias Adding polynomial features: Fixes high bias Decreasing λ: Fixes high bias Increasing λ: Fixes high variance. Diagnosing Neural Networks A neural network with fewer parameters is prone to underfitting. It is also computationally cheaper. A large neural network with more parameters is prone to overfitting. It is also computationally expensive. In this case you can use regularization (increase λ) to address the overfitting. Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using your cross validation set. You can then select the one that performs best. 网络越大越好，不要因噎废食怕过拟合就用小网络。上大网络加正则化肯定比小网络好，因为大网络加正则化后的损失函数更容易优化到一个更小的局部极值点，对随机初始化的依赖更小。 Model Complexity Effects: Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently. Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance. In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习-拉格朗日函数与KKT条件]]></title>
    <url>%2F2017-12-07%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%87%BD%E6%95%B0%E4%B8%8EKKT%E6%9D%A1%E4%BB%B6%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[天算计划-为什么使用Docker]]></title>
    <url>%2F2017-12-05%2F%E5%A4%A9%E7%AE%97%E8%AE%A1%E5%88%92-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8Docker%2F</url>
    <content type="text"><![CDATA[介绍Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。 常用命令Docker运行Linux上可以原生无压力，Windows上需要装插件虚化一个Linux环境，感觉很麻烦。 在 Docker 下安装镜像 1docker pull uhopper/hadoop-spark 在 Docker 中移除镜像 12docker rm [container_id]docker rmi [image_id] ​]]></content>
      <categories>
        <category>分布式计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[天算计划-虚拟化与云计算]]></title>
    <url>%2F2017-12-05%2F%E5%A4%A9%E7%AE%97%E8%AE%A1%E5%88%92-%E8%99%9A%E6%8B%9F%E5%8C%96%E4%B8%8E%E4%BA%91%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[摘录于知乎https://www.zhihu.com/question/22793847 组合了一些优秀答案 咱们需要实现的也是虚拟化（Docker）+分布式计算（Hadoop） 虚拟化对于主机上的虚拟化技术，其中一个可行的定义就是可以让IT系统的物理拓扑图与逻辑拓扑图无关，即解耦，我们暂时以商用虚拟化系统vmware举例为了实现拓扑解耦，它做的第一点就是让一台机器可以同时跑多个操作系统，即虚拟机，而且虚拟机还可以在物理机间来回转移，高可用，这样我们的操作系统就从物理机上彻底解放出来了，你可以把同一个虚拟机随时放到其他物理机上，实现了对硬件的高效资源利用，和系统的高度灵活，解除了大量人工劳动，便于实现大规模系统的方便管理，这种就是服务器虚拟化 通过虚拟化技术将一台计算机虚拟为多台逻辑计算机。在一台计算机上同时运行多个逻辑计算机，每个逻辑计算机可运行不同的操作系统，并且应用程序都可以在相互独立的空间内运行而互不影响，从而显著提高计算机的工作效率。 虚拟化使用软件的方法重新定义划分IT资源，可以实现IT资源的动态分配、灵活调度、跨域共享，提高IT资源利用率，使IT资源能够真正成为社会基础设施，服务于各行各业中灵活多变的应用需求。 虚拟化是一个广义的术语，是指计算元件在虚拟的基础上而不是真实的基础上运行，是一个为了简化管理，优化资源的解决方案。如同空旷、通透的写字楼，整个楼层没有固定的墙壁，用户可以用同样的成本构建出更加自主适用的办公空间，进而节省成本，发挥空间最大利用率。这种把有限的固定的资源根据不同需求进行重新规划以达到最大利用率的思路，在IT领域就叫做虚拟化技术。 虚拟化技术可以扩大硬件的容量，简化软件的重新配置过程。CPU的虚拟化技术可以单CPU模拟多CPU并行，允许一个平台同时运行多个操作系统，并且应用程序都可以在相互独立的空间内运行而互不影响，从而显著提高计算机的工作效率。 虚拟化技术与多任务以及超线程技术是完全不同的。多任务是指在一个操作系统中多个程序同时并行运行，而在虚拟化技术中，则可以同时运行多个操作系统，而且每一个操作系统中都有多个程序运行，每一个操作系统都运行在一个虚拟的CPU或者是虚拟主机上；而超线程技术只是单CPU模拟双CPU来平衡程序运行性能，这两个模拟出来的CPU是不能分离的，只能协同工作。 云计算云计算 （Cloud Computing）是基于互联网的相关服务的增加、使用和交付模式，通常涉及通过互联网来提供动态易扩展且经常是虚拟化的资源。云是网络、互联网的一种比喻说法。过去在图中往往用云来表示电信网，后来也用来表示互联网和底层基础设施的抽象。因此，云计算甚至可以让你体验每秒10万亿次的运算能力，拥有这么强大的计算能力可以模拟核爆炸、预测气候变化和市场发展趋势。用户通过电脑、笔记本、手机等方式接入数据中心，按自己的需求进行运算。 超大规模 “云”具有相当的规模，Google云计算已经拥有100多万台服务器， Amazon、IBM、微软、Yahoo等的“云”均拥有几十万台服务器。企业私有云一般拥有数百上千台服务器。“云”能赋予用户前所未有的计算能力。 虚拟化 云计算支持用户在任意位置、使用各种终端获取应用服务。所请求的资源来自“云”，而不是固定的有形的实体。应用在“云”中某处运行，但实际上用户无需了解、也不用担心应用运行的具体位置。只需要一台笔记本或者一个手机，就可以通过网络服务来实现我们需要的一切，甚至包括超级计算这样的任务。 高可靠性 “云”使用了数据多副本容错、计算节点同构可互换等措施来保障服务的高可靠性，使用云计算比使用本地计算机可靠。 通用性 云计算不针对特定的应用，在“云”的支撑下可以构造出千变万化的应用，同一个“云”可以同时支撑不同的应用运行。 高可扩展性 “云”的规模可以动态伸缩，满足应用和用户规模增长的需要。 按需服务 “云”是一个庞大的资源池，你按需购买，云可以像自来水，电，煤气那样计费。其还强调一个动态分配的特点，比如双11前后几天，电商部门需要比平时10倍的访问量，那就可以在那几天增加10倍的资源，过完又释放相关资源，而不是以前按最高性能要求来购买硬件，减少浪费，借助虚拟化（但不是必须）可以更加方便实现此目的； 其它特性，如服务自助，服务计量化，包括不同包装特性（IaaS,PaaS,Saas）只是在此基础上的增强派生而已。 极其廉价 由于“云”的特殊容错措施可以采用极其廉价的节点来构成云，“云”的自动化集中式管理使大量企业无需负担日益高昂的数据中心管理成本，“云”的通用性使资源的利用率较之传统系统大幅提升，因此用户可以充分享受“云”的低成本优势，经常只要花费几百美元、几天时间就能完成以前需要数万美元、数月时间才能完成的任务。 云计算可以彻底改变人们未来的生活，但同时也要重视环境问题，这样才能真正为人类进步做贡献,而不是简单的技术提升。 潜在的危险性 云计算服务除了提供计算服务外，还必然提供了存储服务。但是云计算服务当前垄断在私人机构（企业）手中，而他们仅仅能够提供商业信用。对于政府机构、商业机构（特别像银行这样持有敏感数据的商业机构）对于选择云计算服务应保持足够的警惕。一旦商业用户大规模使用私人机构提供的云计算服务，无论其技术优势有多强，都不可避免地让这些私人机构以“数据（信息）”的重要性挟制整个社会。对于信息社会而言，“信息”是至关重要的。另一方面，云计算中的数据对于数据所有者以外的其他用户云计算用户是保密的，但是对于提供云计算的商业机构而言确实毫无秘密可言。所有这些潜在的危险，是商业机构和政府机构选择云计算服务、特别是国外机构提供的云计算服务时，不得不考虑的一个重要的前提。]]></content>
      <categories>
        <category>分布式计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux：一些常用的Linux命令]]></title>
    <url>%2F2017-12-04%2FLinux%EF%BC%9A%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%20%2F</url>
    <content type="text"><![CDATA[CP操作格式：cp [选项]... 源… 目录 命令作用：将源文件复制至目标文件，或将多个源文件复制至目标目录。 命令参数： 1234567891011121314151617-a, --archive 等于-dR --preserve=all --backup[=CONTROL 为每个已存在的目标文件创建备份-b 类似--backup 但不接受参数 --copy-contents 在递归处理是复制特殊文件内容-d 等于--no-dereference --preserve=links-f, --force 如果目标文件无法打开则将其移除并重试(当 -n 选项 存在时则不需再选此项)-i, --interactive 覆盖前询问(使前面的 -n 选项失效)-H 跟随源文件中的命令行符号链接-l, --link 链接文件而不复制-L, --dereference 总是跟随符号链接-n, --no-clobber 不要覆盖已存在的文件(使前面的 -i 选项失效)-P, --no-dereference 不跟随源文件中的符号链接-p 等于--preserve=模式,所有权,时间戳 --preserve[=属性列表 保持指定的属性(默认：模式,所有权,时间戳)，如果 可能保持附加属性：环境、链接、xattr 等-R, -r, --recursive 复制目录及目录内的所有项目]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[天算计划-为什么Based on Spark?]]></title>
    <url>%2F2017-12-04%2F%E5%A4%A9%E7%AE%97%E8%AE%A1%E5%88%92-%E4%B8%BA%E4%BB%80%E4%B9%88Based-on-Spark%2F</url>
    <content type="text"><![CDATA[Spark简介Spark最初诞生于美国加州大学伯克利分校（UC Berkeley）的AMP实验室，是一个可应用于大规模数据处理的快速、通用引擎。2013年，Spark加入Apache孵化器项目后，开始获得迅猛的发展，如今已成为Apache软件基金会最重要的三大分布式计算系统开源项目之一（即Hadoop、Spark、Storm）。Spark最初的设计目标是使数据分析更快——不仅运行速度快，也要能快速、容易地编写程序。为了使程序运行更快，Spark提供了内存计算，减少了迭代计算时的IO开销；而为了使编写程序更为容易，Spark使用简练、优雅的Scala语言编写，基于Scala提供了交互式的编程体验。虽然，Hadoop已成为大数据的事实标准，但其MapReduce分布式计算模型仍存在诸多缺陷，而Spark不仅具备Hadoop MapReduce所具有的优点，且解决了Hadoop MapReduce的缺陷。Spark正以其结构一体化、功能多元化的优势逐渐成为当今大数据领域最热门的大数据计算平台。 Spark支持使用Scala、Java、Python和R语言进行编程。由于Spark采用Scala语言进行开发，因此，建议采用Scala语言进行Spark应用程序的编写。Scala是一门现代的多范式编程语言，平滑地集成了面向对象和函数式语言的特性，旨在以简练、优雅的方式来表达常用编程模式。Scala语言的名称来自于“可伸展的语言”，从写个小脚本到建立个大系统的编程任务均可胜任。Scala运行于Java平台（JVM，Java 虚拟机）上，并兼容现有的Java程序。 Spark模式区分在Spark中存在着多种运行模式，可使用本地模式运行、可使用伪分布式模式运行、使用分布式模式也存在多种模式如：Spark Mesos模式、Spark YARN模式。 Spark Mesos模式：官方推荐模式，通用集群管理，有两种调度模式：粗粒度模式（Coarse-grained Mode）与细粒度模式（Fine-grained Mode）。 Spark YARN模式：Hadoop YARN资源管理模式。 Standalone模式： 简单模式或称独立模式，可以单独部署到一个集群中，无依赖任何其他资源管理系统。不使用其他调度工具时会存在单点故障，使用Zookeeper等可以解决 （我们之前配置的是standalone模式） Local模式：本地模式，可以启动本地一个线程来运行job，可以启动N个线程或者使用系统所有核运行job。 RDD介绍Spark的核心是建立在统一的抽象RDD之上，使得Spark的各个组件可以无缝进行集成，在同一个应用程序中完成大数据计算任务。RDD的设计理念源自AMP实验室发表的论文《Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing》。 RDD设计背景在实际应用中，存在许多迭代式算法（比如机器学习、图算法等）和交互式数据挖掘工具，目前的MapReduce框架都是把中间结果写入到HDFS中，带来了大量的数据复制、磁盘IO和序列化开销。虽然，类似Pregel等图计算框架也是将结果保存在内存当中，但是，这些框架只能支持一些特定的计算模式，并没有提供一种通用的数据抽象。RDD就是为了满足这种需求而出现的，它提供了一个抽象的数据架构，我们不必担心底层数据的分布式特性，只需将具体的应用逻辑表达为一系列转换处理，不同RDD之间的转换操作形成依赖关系，可以实现管道化，从而避免了中间结果的存储，大大降低了数据复制、磁盘IO和序列化开销。 迭代式算法：下次计算依赖于上次结果，PageRank，单源最短路径是常常见的迭代式算法之一。 交互式数据挖掘：数据库+可视化+数据挖掘算法 RDD 概念一个RDD就是一个分布式对象集合，本质上是一个只读的分区记录集合，每个RDD可以分成多个分区，每个分区就是一个数据集片段，并且一个RDD的不同分区可以被保存到集群中不同的节点上，从而可以在集群中的不同节点上进行并行计算。RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，不能直接修改，只能基于稳定的物理存储中的数据集来创建RDD，或者通过在其他RDD上执行确定的转换操作（如map、join和groupBy）而创建得到新的RDD。RDD提供了一组丰富的操作以支持常见的数据运算，分为“行动”（Action）和“转换”（Transformation）两种类型，前者用于执行计算并指定输出的形式，后者指定RDD之间的相互依赖关系。两类操作的主要区别是，转换操作（比如map、filter、groupBy、join等）接受RDD并返回RDD，而行动操作（比如count、collect等）接受RDD但是返回非RDD（即输出一个值或结果）。RDD提供的转换接口都非常简单，都是类似map、filter、groupBy、join等粗粒度的数据转换操作，而不是针对某个数据项的细粒度修改。因此，RDD比较适合对于数据集中元素执行相同操作的批处理式应用，而不适合用于需要异步、细粒度状态的应用，比如Web应用系统、增量式的网页爬虫等。正因为这样，这种粗粒度转换接口设计，会使人直觉上认为RDD的功能很受限、不够强大。但是，实际上RDD已经被实践证明可以很好地应用于许多并行计算应用中，可以具备很多现有计算框架（比如MapReduce、SQL、Pregel等）的表达能力，并且可以应用于这些框架处理不了的交互式数据挖掘应用。Spark用Scala语言实现了RDD的API，程序员可以通过调用API实现对RDD的各种操作。RDD典型的执行过程如下： RDD读入外部数据源（或者内存中的集合）进行创建； RDD经过一系列的“转换”操作，每一次都会产生不同的RDD，供给下一个“转换”使用； 最后一个RDD经“行动”操作进行处理，并输出到外部数据源（或者变成Scala集合或标量）。需要说明的是，RDD采用了惰性调用，即在RDD的执行过程中（如图9-8所示），真正的计算发生在RDD的“行动”操作，对于“行动”之前的所有“转换”操作，Spark只是记录下“转换”操作应用的一些基础数据集以及RDD生成的轨迹，即相互之间的依赖关系，而不会触发真正的计算。 从输入中逻辑上生成A和C两个RDD，经过一系列“转换”操作，逻辑上生成了F（也是一个RDD），之所以说是逻辑上，是因为这时候计算并没有发生，Spark只是记录了RDD之间的生成和依赖关系。当F要进行输出时，也就是当F进行“行动”操作的时候，Spark才会根据RDD的依赖关系生成DAG，并从起点开始真正的计算。]]></content>
      <categories>
        <category>分布式计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[天算计划-Spark-计算任务分发流程]]></title>
    <url>%2F2017-12-04%2F%E5%A4%A9%E7%AE%97%E8%AE%A1%E5%88%92-Spark-%E8%AE%A1%E7%AE%97%E4%BB%BB%E5%8A%A1%E5%88%86%E5%8F%91%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Spark-Shell操作加载本地文件 在开始具体词频统计代码之前，需要解决一个问题，就是如何加载文件？ 要注意，文件可能位于本地文件系统中，也有可能存放在分布式文件系统HDFS中，所以，下面我们分别介绍如何加载本地文件，以及如何加载HDFS中的文件。 12val textFile = sc.textFile("file:///usr/local/spark/mycode/wordcount/word123.txt")//如果文件不存在会在执行诸如 textFile.first()指令时拒绝连接 加载HDFS中的文件 为了能够读取 HDFS 中的文件，我们需要先启动 Hadoop 中的 HDFS 组件。 1start-all 请使用下面命令创建用户名 hadoop 登录 Linux 系统。 1./bin/hdfs dfs -mkdir -p /user/hadoop 下面我们使用命令查看一下HDFS文件系统中的目录和文件： 123./bin/hdfs dfs -ls ../bin/hdfs dfs -ls /user/hadoop//这两个命令等价 上面命令中，最后一个点号“.”，表示要查看Linux当前登录用户hadoop在HDFS文件系统中与hadoop对应的目录下的文件，也就是查看HDFS文件系统中“/user/hadoop/”目录下的文件 如果要查看HDFS文件系统根目录下的内容，需要使用下面命令： 1./bin/hdfs dfs -ls / 下面我们使用dfs -put指令将文件上传到hdfs文件系统中 1./bin/hdfs dfs -put &lt;path&gt; . 使用-cat指令查看文件内容 1./bin/hdfs dfs -cat ./&lt;filename&gt; 现在，让我们切换回到spark-shell窗口，编写语句从HDFS中加载word.txt文件，并显示第一行文本内容： 12scala&gt; val textFile = sc.textFile("hdfs://localhost:9000/user/hadoop/&lt;filename&gt;")scala&gt; textFile.first() 如下三条语句也等价 123val textFile = sc.textFile("hdfs://localhost:9000/user/hadoop/word.txt")val textFile = sc.textFile("/user/hadoop/word.txt")val textFile = sc.textFile("word.txt")]]></content>
      <categories>
        <category>分布式计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kaggle实战-Mecari价格预测]]></title>
    <url>%2F2017-12-03%2FKaggle%E5%AE%9E%E6%88%98-Mecari%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[先从分析别人的visualization kernel入手。 OverviewThe training data has 1482535 observations with 7 features. The test data has 693359 rows that we need to predict. According to the competition description, the public leaderboard will be evaluated by ALL of the test data we have at the first stage. We can have a a rough look at the features (test data summary is hidden for simplicity) train_id / test_id: A unique key for each item. name: The item’s name as a string. item_condition_id: A factor with 5 levels. As the plot below shows, the mean prices for different conditions are really close and it’s hard to guess which whether higher / lower condition id is better so far. category_name: The category of the item. brand_name: The brand name of the item. Nearly half of the items do not have a brand. shipping: A binary indicator of the shipping information. (1 if shipping fee is paid by seller and 0 by buyer) item_description: A long string containing the raw text of the item description. ~5% of the items do not have a description. ShippingThe shipping cost burden is decently splitted between sellers and buyers with more than half of the items’ shipping fees are paid by the sellers (55%). In addition, the average price paid by users who have to pay for shipping fees is lower than those that don’t require additional shipping cost. This matches with our perception that the sellers need a lower price to compensate for the additional shipping. 普遍来说：价格低的不怎么需要付邮费，而价格高的需要支付更多的邮费。 1230 0.5527261 0.447274Name: shipping, dtype: float64 对于物品的状况item_condition来说，数值和价格关联不大，目前还不知道具体的作用是什么，处理的时候先扔掉（估计对于非常相似的物品，比较下它的状态或许能够给我们一些启发）。 ID = 5时看起来四分位数和中位数都比较高。 Tokenlize123456789101112131415161718192021stop = set(stopwords.words('english'))def tokenize(text): """ sent_tokenize(): segment text into sentences word_tokenize(): break sentences into words """ try: regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\r\\t\\n]') text = regex.sub(" ", text) # remove punctuation tokens_ = [word_tokenize(s) for s in sent_tokenize(text)] tokens = [] for token_by_sent in tokens_: tokens += token_by_sent tokens = list(filter(lambda t: t.lower() not in stop, tokens)) filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)] filtered_tokens = [w.lower() for w in filtered_tokens if len(w)&gt;=3] return filtered_tokens except TypeError as e: print(text,e) TF-IDFtf-idf（英语：term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。tf-idf加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了tf-idf以外，互联网上的搜索引擎还会使用基于链接分析的评级方法，以确定文件在搜索结果中出现的顺序。 有很多不同的数学公式可以用来计算tf-idf。这边的例子以上述的数学公式来计算。词频（tf）是一词语出现的次数除以该文件的总词语数。假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是3/100=0.03。而计算文件频率（DF）的方法是以文件集的文件总数，除以出现“母牛”一词的文件数。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是log（10,000,000 / 1,000）=4。最后的tf-idf的分数为0.03 * 4=0.12。 tf-idf算法是创建在这样一个假设之上的：对区别文档最有意义的词语应该是那些在文档中出现频率高，而在整个文档集合的其他文档中出现频率少的词语，所以如果特征空间坐标系取tf词频作为测度，就可以体现同类文本的特点。另外考虑到单词区别不同类别的能力，tf-idf法认为一个单词出现的文本频数越小，它区别不同类别文本的能力就越大。因此引入了逆文本频度idf的概念，以tf和idf的乘积作为特征空间坐标系的取值测度，并用它完成对权值tf的调整，调整权值的目的在于突出重要单词，抑制次要单词。但是在本质上idf是一种试图抑制噪声的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用，显然这并不是完全正确的。idf的简单结构并不能有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能，所以tf-idf法的精度并不是很高。 此外，在tf-idf算法中并没有体现出单词的位置信息，对于Web文档而言，权重的计算方法应该体现出HTML的结构特征。特征词在不同的标记符中对文章内容的反映程度不同，其权重的计算方法也应不同。因此应该对于处于网页不同位置的特征词分别赋予不同的系数，然后乘以特征词的词频，以提高文本表示的效果。 t-SNE因为原理不同，导致，tsne 保留下的属性信息，更具代表性，也即最能体现样本间的差异，且TSNE 运行极慢，PCA 则相对较快。 因此更为一般的处理，尤其在展示（可视化）高维数据时，常常先用 PCA 进行降维，再使用 tsne。 12data_pca = PCA(n_components=50).fit_transform(data)data_pca_tsne = TSNE(n_components=2).fit_transform(data_pca)]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Material主题中Mathjax的配置]]></title>
    <url>%2F2017-12-01%2FHexo-Material%E4%B8%BB%E9%A2%98%E4%B8%ADMathjax%E7%9A%84%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[最近Hexo无故始终出渲染的问题，删掉重建的过程中发现是因为操作系统-内存那节因为有些是从PPT里拷出来的，导致了无法识别符号，渲染失败。 在重建的过程中，最头疼的就是 Mathjax 的配置了，实现这个效果的插件似乎有很多个版本，有些也有时效性，而且挺多插件，针对于我当前使用的Material主题并无效果，如果使用最出名的Next主题似乎就要好很多。 下面简单记录下配置Mathjax的过程。 修改默认的renderhexo 默认的渲染引擎是 marked，但是 marked 不支持 mathjax。 kramed 是在 marked的基础上进行修改。我们在工程目录下执行以下命令来安装 kramed。 1234npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --savenpm uninstall hexo-math --savenpm install hexo-renderer-mathjax --save //这个使用来代替hexo-math的 执行了这两个操作这个，不用去_config.yml文件中改什么plugin之类的东西（这些是之前使用hexo-math才需要的操作）。 修改kramed的渲染细节这一步不做其实也可以，但是会出现一些转义的bug. 接下来在 /node_modules/hexo-renderer-kramed/lib/renderer.js，修改 12345678910// Change inline math rulefunction formatText(text) &#123; // Fit kramed's rule: $$ + \1 + $$ return text.replace(/`\$(.*?)\$`/g, '$$$$$1$$$$');&#125;// 将上面的修改为如下的// Change inline math rulefunction formatText(text) &#123; return text;&#125; 到&lt;path-to-your-project/node_modules/kramed/lib/rules/inline.js，将11行的 123escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/修改为escape: /^\\([`*\[\]()# +\-.!_&gt;])/, 将20行的： 123em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,修改为em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, ​ 配置Material主题的Mathjax加载的CDN链接： 在material/_config.yml下添加如下的链接 123456789vendors:# MaterialCDN# You can load theme unique files from your private cdn or oss.# The new src will have the base domain you configured below.# For example# materialcdn: https://cdn.jsdelivr.net/npm/hexo-material@1.4.0/source materialcdn: https://cdn.jsdelivr.net/npm/hexo-material@1.4.0/source # MathJax 2.7.0-2.7.1 mathjax: https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js ​]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[随想-同谋（Complicit）]]></title>
    <url>%2F2017-12-01%2F%E9%9A%8F%E6%83%B3-%E5%90%8C%E8%B0%8B%EF%BC%88Complicit%EF%BC%89%2F</url>
    <content type="text"><![CDATA[2017年度词汇 —— 同谋 “ 2017 年度词汇：同谋（Complicit）” —— dictionary.com 这个词的来源是： 4月5日，美国总统特朗普的女儿伊万卡· 特朗普接受《CBS 今晨秀》采访时，被问到她与丈夫是否是特朗普总统的同谋，她回答说 『我不知道同谋是什么意思』。 10月24日，参议员杰夫·弗莱克宣布辞职，并表示『我不会成为共谋』。 Dictionary.com 在公布年度词汇时表示 我们所选择的年度词汇既指那些看得见的行为，也指看些看不见的行为。这个词汇提醒我们，即使不作为也是一种行动。沉默地接受了错误的行为，是我们落到这般境地的原因。我们不能让这种情况继续变成一种常态。如果我们这样做，我们都是共谋。 用一种更通俗的话来讲，或许它在表达：人们越来越沉浸在社交网络的娱乐氛围当中，缺少对时事的严肃讨论和关切，人们倾向于看到事情更让滑稽，更戏剧性，或者说更愿意让自己相信的那一面，而沉默或者是不理智的发声，有时候也是错误的帮凶。]]></content>
      <categories>
        <category>随想</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[天算计划-Ubuntu-Spark组网计划配置操作]]></title>
    <url>%2F2017-11-30%2F%E5%A4%A9%E7%AE%97%E8%AE%A1%E5%88%92-Ubuntu-Spark%E7%BB%84%E7%BD%91%E8%AE%A1%E5%88%92%E9%85%8D%E7%BD%AE%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[记录下组内环境的配置流程，目前我们需要实现五个人在Ubuntu环境下的Hadoop+Spark配置，配置流程如下。 SSH无密码互联 使用ssh-keygen -t rsa -P &quot;&quot; 命令生成公钥，表示生成的公钥不含登录密码，只要A机器拿到B的公钥就可以实现 A 机器无密码登录 B 机器。 cd $HOME/.ssh目录下，使用cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys 命令或者直接将id_rsa.pub中的最后一行复制到authorized_keys当中。 最终保证五个人的电脑的中都有一份相同authorized_keys文件是最好的，这样就可以实现无密码登陆了（第一次可能需要输密码，这里建议咱们先都设置成12345678，方便实验）。 两个人之间测试能否通过当前的内网IP互相登录，如elrond@master这种格式，使用ssh &#39;inet IP&#39;@master登录 elrond 的机器。 错误收集 如果遇到sign_and_send_pubkey: signing failed: agent refused operation的错误，原因是因为ssh-agent并没有真正的工作，输入下列命令： 12eval `ssh-agent -s` ssh-add ​ 统一化HOSTNAME 对于不同的电脑有可能有不同的用户名，我们这里针对于hadoop统一设置一下我们每个人的用户名。 为了更好的在Shell中区分三台主机，修改其显示的主机名，执行如下命令 1sudo vim /etc/hostname master的/etc/hostname添加如下配置： 1master 同样slave01的/etc/hostname添加如下配置： 1slave01 同样slave02的/etc/hostname添加如下配置： 1slave02 重启三台电脑，重启后在终端Shell中才会看到机器名的变化,如下图： 到/etc/hosts路径下修改ip - 缩写，如当前阿臻是master用户，那么我在hosts下面加入了172.20.0.39 master这行语句之后，我再使用ping master实际上是ping 172.20.0.39 。 因此我们最好把hosts文件修改为如下的形式： 使用 1sudo vim /etc/hosts 配置如下： 1234127.0.0.1 localhost172.20.0.39 master172.20.0.38 slave01172.20.0.37 slave02 然后master需要修改~/.ssh/config文件，如果没有此文件，自己创建文件。 123456Host master user elrond // 阿臻的电脑Host slave01 user hadoop // 丹丹的电脑Host slave02 user yuhongzhong // 宇宏的电脑 注：这里的user和host对应的是命令行下的elrond@master，等于在master主机下的elrond用户。 这时我们尝试下面操作，应该就能登录到丹丹的电脑上了。 1ssh slave01 Hadoop配置 修改master主机修改Hadoop如下配置文件，这些配置文件都位于/usr/local/hadoop/etc/hadoop目录下。 slaves这里把DataNode的主机名写入该文件，每行一个。这里让master节点主机仅作为NameNode使用。 12slave01slave02 core-site.xml 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; //注：这里只能是奇数，代表当前允许的datanode个数，即slaver个数。 &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml(复制mapred-site.xml.template,再修改文件名) 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml 1234567891011&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 此时阿臻的电脑上 /usr/local 文件夹下面有一个hadoop.tar文件，我们使用scp指令将其传送到其他的slave结点上。 如传送到宇宏的电脑，我们使用：scp hadoop.tar slave02:/usr/local，传送之前要保证宇宏执行了了sudo chmod 777 /usr/local/。 传送完毕文件之后（丹丹和宇宏的电脑都已经有了），解压，在 slave01，slave02 节点上执行 sudo chown -R hadoop /usr/local/hadoop，以保证 master 有权限去修改 slaver 的这个目录。 在 master 主机上执行如下命令： 123cd /usr/local/hadoopbin/hdfs namenode -formatsbin/start-dfs.sh 运行过程中理论上不应该输入密码，如果要输入密码，重回 SSH 的步骤，检查 SSH 是否能够相互无密码登录。 理论情形如下： 我们通过浏览器来查看整个集群的HDFS状态，地址为：http://172.20.0.39（master的内网IP）:50070/dfshealth.html#tab-overview]]></content>
      <categories>
        <category>分布式计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[11-29-异常检测周报]]></title>
    <url>%2F2017-11-29%2F11-29-%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E5%91%A8%E6%8A%A5%2F</url>
    <content type="text"><![CDATA[这周因为机器学习的考试和软工课的一些其他事情。 我还没有进行我的计划：对于DBSCAN针对于时间序列数据的代码级别改进，但是从思路上考虑到了一些方向，下面主要结合这段时间对于这个问题的思考以及一切实验来进行汇报。 数据平滑之后的结果首先我考虑对数据集进行平滑处理，目前能够使用的是如窗口移动平均这类做法，这类做法的时间复杂度为 $O(N)$ ，常数较大，但是复杂度在可以接受的返回值内。 平滑的目的是将工业中因为传感器误差而引入的背景噪声消除，在平滑了曲线后我再使用差分的方法找出局部的最大最小值，效果更为理想（注意在演示中因为使用的数据太大，太密，导致了我们没有办法很好的细粒度的展示） 平滑的含义如下： 在平滑之后的数据，使用差分标记可能的异常值（红点里标记的为局部的最大最小值，我们通过让用户设置阈值，规定差距大于 $\epsilon$ 时，我们检测这个异常值，可以实现针对不同类型的传感器数据报出异常）。 对差分之后的结果进行 K-means和DBSCAN聚类的结果分别如下。 K-MEANS的效果： 但是很可惜的是，K-MEANS的效果是基于空间的聚类，经过更大规模数据的观察，显然它是更多的在根据Y轴坐标进行聚类的划分。 以下是DBSCAN的效果，DBSCAN能够识别出数据大的模式的转化，下一步我打算在红点处加重权值 minEps，希望能够成功的使用DBSCAN进行模式的划分。 MRDBSCAN最近一段时间，我接触到了分布式计算，并且在尝试使用Spark完成一些分布式计算的任务。我也了解到了一个比较前沿的分布式使用DBSCAN的算法：MRDBSCAN。 下面对这个算法做一些简单的记录以及报告： 第一步将平面划分为等点数的几个子平面。 将每个子平面交给一个Worknode去进行DBSCAN的划分。 收集每个子结点计算的结果，考虑子平面边界的情况，对于边界的点，能够向哪边传播，就属于哪边的簇，如果两边都能够同时传播，那么它就属于两边的簇，这时说明两边的簇能够连接到一起去，这时将两边修改为同样的颜色。 下一步方向规划目前我的方法是基于差分-&gt;DBSCAN-&gt;LSTM进行的组合方法的异常点+异常模式段的识别。 下一步我打算分为以下两个阶段进行： 去广泛的使用其他的算法进行异常点（和差分比较）和异常模式（和DBSCAN+LSTM比较）识别检测的效果比较。 将差分找异常点的算法打成包或者写成函数，向外提供调用的接口。 尝试针对于时序数据特性改进DBSCAN算法，并且尝试实现此函数。 一些疑惑 能够实现同样效果的有很多其余的算法，浩哥也在研究其他的一些算法，但是有一些算法时间空间效果不高，对于工业大数据要求的尽可能 $O(N)$ 级别的处理速度，这些算法还有其研究的意义吗？我们有没有必要去广泛的收集各类算法进行比较，测试？即使它看起来很慢而且效果不见得会很好。 目前我们的数据感觉存在低质量的情况，很多时候做不出和理论相近的效果，这方面可能会牺牲一些，需要用户自己多设置一些参数，去调整最后的效果。 最后用户希望从我们的系统检测出来的异常值获取什么信息，能够对工业上有什么帮助呢？]]></content>
      <categories>
        <category>异常检测</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习-第四章-SVM（Updating）]]></title>
    <url>%2F2017-11-26%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E5%9B%9B%E7%AB%A0-SVM%2F</url>
    <content type="text"><![CDATA[主要梳理以下三个方向：支持向量机，核函数，序列最小最优化算法 支持向量机支持向量学习方法包含构建线性可分支持向量机（Linear support vector machine in linearly separable case）、线性支持向量机（Linear support vector machine）及非线性支持向量机（non-linear support vector machine）。 在提到SVM以及逻辑回归，有如下几种情况。 当我们使用线性可分的数据集时，通过硬间隔最大化（hard margin maximization），学习一个线性的分类器，即线性可分支持向量机。 当训练数据近似可分，通过软间隔最大化（soft margin maximization），也学习一个线性分类器。 再则，当训练数据线性不可分时，通过引入核技巧（kernel trick）及软间隔最大化，学习非线性支持向量机。 定义假设在给定的特征空间训练集如下：$T = \{(x_1,y_1),…(x_n,y_n)\}$，其中，$x_i \in \chi= \Re^n, y_i \in \{1,-1\}$ 假设训练数据集是线性可分的，我们需要找到一个分离的超平面 $w \cdot x + b$ 能够将不同的实例分成两部分，我们利用间隔最大化求最优的分离超平面，这时，解是唯一的。 函数间隔和几何间隔函数间隔： 定义给定的数据集 $T$ 和超平面 $（w，b）$. 定义超平面 $(w,b)$ 和样本点 $(x_i,y_i)$ 的函数间隔为：$\hat{\gamma} = y_i(w \dot x_i + b)$. 定义超平面 $(w,b)$ 和数据集 $T$ 的函数间隔为：$\hat{\gamma} = min_{1…n}(\hat{\gamma}_i)$. 几何间隔 由于函数间隔在成倍数变化时，虽然超平面不变动，但是数值却在发生变化，因此我们继续定义几何间隔来对函数间隔进行规范化。 几何间隔：$\gamma_i = \frac{w}{\mid \mid w \mid \mid} \cdot x_i + \frac{b}{\mid \mid w \mid \mid}$ 为什么要间隔最大化 对于训练数据及找到几何间隔最大化的超平面意味着以充分大的确信度（我们可以定义离超平面越远的点对于分类面的确信度越大）对训练数据进行分类，这样的分类结果对于最难分类的点也有足够大的确信度将其分开，这样的超平面应该对未知的新实例有更好的泛化的效果。 支持向量和间隔边界 在线性可分的情况下，训练数据的样本点与分离超平面距离最近的样本点的实例称为支持向量（Support Vector）.支持向量是使约束条件式子等号成立的点，即 $y_i(w \cdot x_i + b) - 1=0$ 对于 $y_i = +1$ 的正例点，支持向量在超平面：$H_1:w \cdot x + b = 1$ 上。 对于 $y_i = -1$ 的负例点，支持向量在超平面：$H_1:w \cdot x + b = -1$ 上。 在决定分离平面时只有支持向量起作用，支持向量两侧的其余点并不起作用，所以我们将这种分类模型称为支持向量机，支持向量的个数一般很少，所以支持向量机由很少的『重要的』训练样本决定。 拉格朗日对偶 通过求解对偶问题的解来获取原问题的最优解，被称为Linear SVM的对偶算法，这样做的优点在于，一是，对偶问题往往更容易求解，二是，引入核函数，进而推广到非线性分类问题。 定义拉格朗日函数：$L(w,b,\alpha) = \frac{1}{2} \mid \mid w \mid \mid^2 - \Sigma^N_{i} \alpha_i y_i(w \cdot x_i + b) + \Sigma^N_{i} \alpha_i$. 对于支持向量需要尽量满足 $y_i(w \cdot x_i + b) - 1 \geq0$，根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：$\max \limits_{\alpha} \cdot \min \limits_{w,b} \{L(w,b,a)\}$. 核技巧当输入空间为欧式空间或者离散集合，特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积，通过核函数可以学习到非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机，这样的方法称为核技巧。 概述的说，线性SVM+核技巧 = 非线性SVM。 希尔伯特空间：抽象空间中的极限与实数上的极限有一个很大的不同就是，极限点可能不在原来给定的集合中，所以又引入了完备的概念，完备的内积空间就称为Hilbert空间。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习复习-第三章-逻辑回归（Updating）]]></title>
    <url>%2F2017-11-26%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%89%E7%AB%A0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习复习-第二章-概率]]></title>
    <url>%2F2017-11-25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%A6%82%E7%8E%87%2F</url>
    <content type="text"><![CDATA[几个公式 贝叶斯公式：$P(B \mid A) = \frac{P(B) \cdot P(A \mid B)}{P(A)}$ 全概率公式：$P(A) = \Sigma_{n} P(A \mid B_n) \cdot P(B_n)$. 生成模型与判别模型监督学习的任务就是学习一个模型，应用这一模型，对给定的输入预测相应的输出，这个模型的一般形式为决策函数：$Y = f(x)$，或者条件概率分布：$P(Y \mid X)$ 监督学习方法又可以分为生成方法（Generative Approach）和判别方法（Discriminative Approach），所学习到的模型分别称为生成模型和判别模型。 生成方法生成方法由数据学习联合概率分布 $P(X,Y)$，然后求出条件概率分布 $P(Y \mid X)$ 作为预测的模型. 即生成模型：$P(Y \mid X) = \frac{P(X,Y)}{P(X)}$ 这样的方法之所以称为生成方法，是因为模型表示了给定输入 $X$ 产生输出 $Y$ 的生成关系，典型的生成模型有：朴素贝叶斯方法和隐马尔科夫模型。 判别方法判别方法由数据直接学习决策函数 $f(X)$ 或者条件概率分布 $P(Y \mid X)$ 作为预测的模型，即判别模型。判别方法关心的是对给定的输入 $X$，应该预测什么样的输出 $Y$。典型的判别模型包括：$k$ 近邻法，感知机，决策树，逻辑斯蒂回归模型，最大熵模型，支持向量机，提升方法和条件随机场等。 评价分类指标准确率（accuracy） 精确率（precision） 召回率（recall） 朴素贝叶斯医生对病人进行诊断就是一个典型的分类过程，任何一个医生都无法直接看到病人的病情，只能观察病人表现出的症状和各种化验检测数据来推断病情，这时医生就好比一个分类器，而这个医生诊断的准确率，与他当初受到的教育方式（构造方法）、病人的症状是否突出（待分类数据的特性）以及医生的经验多少（训练样本数量）都有密切关系。 回忆朴素贝叶斯中的公式 $P(Y = y_k \mid X_1,X_2,\ldots X_n) = \frac{P(Y = y_k)\prod_i P(X_i \mid Y = y_k)}{\Sigma_j ( P(Y = y_j)\prod_i P(X_i \mid Y = y_j))}$ 对比与医生看病，就是医生利用他已知的发病的征兆，结合病人的征兆去获取这个 $\prod_i P(X_i \mid Y = y_k)$，然后再结合发病的概率，去推断病人现在身上的征兆，有多大的可能发病。 朴素贝叶斯学习和分类器与其他相比可以非常快。在条件独立的假设下，类条件特征分布的解耦意味着 每个分布可以独立估计为一个一维分布，这反过来又有助于缓解维灾难问题。 高斯朴素贝叶斯的训练过程 训练：对于每个 $y_k$： 估计 $\pi_k (即P(Y = y_k))$ 对于 $X$ 的每一个属性 $X_i$，估计类条件均值和方差 $\mu_{i,k},\sigma_{i,k}$. 分类： $Y_{new} \leftarrow argmax_{y_k} \prod_i {P(X^{new}_i \mid Y = y_k)}$ $Y_{new} \leftarrow argmax_{y_k} \pi_k \prod_i N(X^{new}_i,\mu_{i,k},\sigma_{i,k}) $ 对于均值和方差的计算如下： 一些问题 怎么对两组数据进行独立性检验？ 皮尔森卡方检验 标准的朴素贝叶斯分类器的决策面看起来是什么样的？ 对于高斯分布的话，如果两个类的协方差相同，决策面是线性的超平面]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习复习-第一章-决策树（Updating）]]></title>
    <url>%2F2017-11-24%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[在这里我记录下在学习决策树之后，我自己的思考以及重点，不会包含太多介绍性的内容。 新手入门的话推荐周志华的《机器学习》和李航的《统计学习方法》。 信息熵，基尼系数，分类误差定义关于过拟合决策树十分容易产生过拟合的情况，也就是说当前的决策树有很多的节点，整个决策树很大，此时会把样本中一些并不需要拿来作为分类的属性学习到，因此学习的决策树模型并不是最优的模型，这就是机器学习中的过拟合现象，与此相关的概念就是欠拟合，欠拟合主要是因为当前的样本不够，导致实际在学习的时候可以用来学习的信息很少，因此也会出现学习不好的现象。 预处理（Pre-Prunning） 在树已经成长成为完全树之后停止算法。 对于结点的剪枝规则 当所有类可以被分到一个类别时。 当某些类别的数量小于用户设定的阈值。 当某个结点并不能带来显著（高于阈值）的基尼系数（这里可以参考CART算法）或者信息熵的增长时。 后处理（Post-Prunning）使用MDL的思想，对于决策树自底向上逐层扫描剪枝。 注意预剪枝和后剪枝都可以使用验证集作为是否分裂的标准，与决策树的构建先后无关。但两者所获得的结果是有区别的，往往后剪枝获得的决策树要比预剪枝获得的大，且欠拟合风险很小，泛化性能往往更优，但毫无疑问的是，后剪枝的开销更大。 斜决策树单变量决策树的每个节点都是使用一个属性，这样生成的决策树如果用坐标空间来刻画（属性即坐标轴），划分的边界都是平行于坐标轴的。 但有时候单一的属性很难刻画分类的边缘，会造成抖动，而这个抖动只需要一条斜边就可以很好的解决了，其实所谓的斜边就是属性的线性组合，即节点使用多个属性的线性表达式来作为评判标准。 缺失值对于缺失值需要考虑两个问题： 样本集属性缺失，如何根据缺失的属性来选择划分节点？ 在不做缺失值填充的情况下，显然只能使用在该节点没有确实值的子样本集进行计算，然后信息增益的计算最后需要乘上一个不缺失系数（即该节点不缺失的样本集数与总样本的比） 。 假如你使用 ID3 算法，那么选择分类属性时，就要计算所有属性的熵增(信息增益，Gain)。假设10个样本，属性是 $a,b,c$。在计算 $a$ 属性熵时发现，第10个样本的 $a$ 属性缺失，那么就把第10个样本去掉，前9个样本组成新的样本集，在新样本集上按正常方法计算 $a$ 属性的熵增。然后结果乘 0.9（新样本占raw样本的比例），就是 $a$ 属性最终的熵。 这里给出一个课程 handouts 中的图示。 预测集属性缺失 若预测时，某个属性缺失，则以一定概率将该样本划分到该节点的各个取值中。至于这个概率如何选取，参考资料决策树是如何处理不完整数据的？]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[11-24-异常检测周报]]></title>
    <url>%2F2017-11-24%2F11-24-%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E5%91%A8%E6%8A%A5%2F</url>
    <content type="text"><![CDATA[这周的工作主要有了两个方向的进展，下面的报告分以下两个方面阐述。 一、异常点的识别首先要解释下，其实单一维度和多维度异常点识别可以看成是两类异常情况的识别，并不能简单的将单一维度看成是多维度的简化版本，对于我们手里的这组传感器数据，理性的思考，多维度的异常其实并不容易发生，很多时候其实就只是某个传感器的数据出了问题，但是我们利用多维的异常检测，比如iforest，DBSCAN聚类去扫描的方法，其实是不容易看出这点异常的，而且也有可能存在，本来就是单点异常，但是我们却把这个时段的所有点标记成异常，导致可能工人需要去检查所有的传感器，这样实际操作中费时费力且算法层面并不高效（iforest还好，但是DBSCAN最少都是nlogn的）。 因此目前我认为，不能先入为主的拿多维去做异常检测，而是首先对单一维度做异常的检测，也可以对多个强相关的维度进行协同检测。 下面首先讲述下我对于单一维度异常检测的一些思考和实验结果。 单一维度：差分方法 思路 利用多阶段的差分，我们可以识别出单点以及模式的异常。这个idea来自于梯度，当函数突然出现了一个异常的峰值点，或者是出现了突然地连续下降时，这个点的领域的梯度一定是有别于周围的其他点的，但是对于求区间的梯度（或者是斜率），我们首先要知道我们怎么去划分这个区间，但是这个区间的长度却会因数据的变化而动态变化（注意这里跟周期不是一个概念，虽然周期也是动态变化的），我们不容易确定这个值，因此我们选择利用差分，我们初始的差分可以从前后两个点开始，然后逐渐的调整差分的参数（实际上也就是去动态的找这个区间），显然单点异常一定会在差分之后的结果中突出出来，而模式的的异常，我们在增加差分的区间长度之后，也会显示出来。 下面的图中，第一行是原始数据，第二行是diff(1)的结果，第二行是diff(100)的结果。 从图中可以看出在diff（1）的时候，已经可以看出异常点了，而在diff（100）的时候，我们成功的检测到了梯度的突然下降。 再回到差分这个方法，虽然这个思想简单，比不得高大上的其他机器学习算法，但是目前这是我能够想到的效果不错，且效率最高的算法，因为这是一个在线的算法，它不需要累计一段时间的数据然后统一的进行处理（我使用过isolation forest和lof，效果不如差分好，且它们并不能实时的计算），我们可以部署到传感器上，自己就能轻松的实现报警。 还需改进的地方 目前差分对于简单异常点的识别非常暴力且轻松，但是对于比如某个区间梯度突然地变化这种异常情况，会比较依赖于差分参数的选择，而这个差分参数怎么自动化调整，是一个需要进一步解决的问题，我也在思考，目前有些思路比如『每个点统计前后n个点的均值方差，然后判断这个点究竟是需要进行一阶差分还是需要更高阶的差分』，这些思路其实也可以和聚类结合起来，因为最终我们的目的是去识别出函数断层的地方！，能够实现这个，我们的异常检测就能够粗略的达到了要求。 多维协同：孤立森林方法 这个方法对于单维数据，我并不推荐，我认为它的用处可以用作多维协同（多维一定要选取强相关的维度，否则一定会出错，而自动判定维度之间是否强相关是一个复杂度很高的问题！）的异常检测，这里我就不再赘述。 二、异常模式的识别首先也需要解释，我对于异常模式识别的理解，首先我们的目标是要识别出正常的模式，然后有别于这种模式的其他段就是异常。 ​ 而识别正常模式的第一个阶段，我认为需要去对总的数据进行分类（不能拿整体数据进行模式的识别），对于上面的数据，如果我们能够大致的判断出其中包括了三类区间，就能够用神经网络（LSTM）对小区间的模式进行识别，那么我们针对于这种情况，能够想到的只有聚类这个方式。目前常用的聚类方法我认为可行的只有K-Means和DBSCAN，下面分别对这两种方法进行说明。 K-Means 对于kmeans其实效果都还挺好，但是这个方法的局限性在于我们需要为我们的数据指定固定的cluster的个数。 下面是对15，16，17，18列的数据进行协同的模式分析（这里也要说明，如果要对多维数据协同分析，一定要利用先验知识或者是互相的拟合关系说明他们是强相关的，在下面的四组数据中实际上我为了观察聚类效果能否将其他维度的非模式段拉到本维度的正常模式，而放松了这个界限） DBSCAN DBSCAN相比如K-MEANS最具有优势的两点在于，其一，其不用指定数据的聚类，其二，其可以对于非凸的cluster进行较好的聚类，大致效果就是这个意思。 ![这里写图片描述](https://ws4.sinaimg.cn/large/006tKfTcly1flsjmxbcr4j309b0l1djr.jpg) 显然DBSCAN聚类的效果才是我们希望达到的目标，但是需要注意的是，这里的eps的选取对聚类效果影响很 大，同样这里也是影响到我对于windmachine数据测试的聚类效果一大关键的点。 在我调整参数的过程中出现了两个极端的情况。 第一种则是所有数据聚成一类 第二种情况则是算法自动将1000多个点分为了700多个类，效果五彩斑斓 目前这个问题还没有解决，但是我有两个还没有实验的想法。 我推测是没有正确的使用时间这个属性，权重？或者是对于已知异常点动态的加入参数。 对于1000个点分为了700类的情况，在图中实际可以看出类与类之间也有相似度的区别（从颜色就可以看出来），那么再对这个类的质心进行一次聚类，是否可以能够划分为更少的大类？ 后者方法其实我认为非常可行与可解，最近时间比较紧凑，但是我会尽快去做相应的尝试。 聚类之后 说完了聚类之后，那么对于较小区间的模式识别其实已经是有了现成的方法（但是时间复杂度很高，因此我们必须要尽可能多的去进行区间的识别，去划分有可能存在周期函数的区间出来再往上并行的套神经网络，不能对所有的层都做一遍线性的神经网络，复杂度太高了！） 如上图，我使用周期函数加了高斯噪声，并在中间这个地方加入了人工的异常点，进行了一组测试，LSTM是可以找出异常的模式并加以权重十分高的Error，我认为这个效果非常的好，所以我们只要能打通聚类的那一步，我认为对于区间函数下异常模式，其实是可以很好解决的。 小结 差分目前主要的问题在于需要去细化差分的参数，使其不仅能够找出异常点，而也能够对区间的异常有一定的检测作用，这方面得去看一些金融时间序列的东西，希望能够从统计这里得到一些信息。 对于聚类，目前认为DBSCAN以及其各种改进，比如OPTICS，SNN是一些较好的方法，但是聚类我认为应该结合具有相关性的几个维度去做，然后做模式的区分，目前使用K-Means已经有了一个较可观的实现，希望能够有一定的效果。 我之后的方向是希望去细化研究上述1，2点，能够通过对DBSCAN的细化研究，希望能够将DBSCAN在特定的数据集上做一些自己的优化，然后还能够研究下DBSCAN在时间序列下的更适配的应用（要注意时间序列的时间这一维特性，是否完全不用？我认为不行，但是要怎么用，我也不太清楚） LSTM是否是唯一最好的用于这种区间函数下异常模式识别的方法？我对神经网络了解的不多，我不太清楚，还需要请教一些高人。]]></content>
      <categories>
        <category>异常检测</category>
      </categories>
      <tags>
        <tag>大数据异常检测</tag>
      </tags>
  </entry>
</search>
